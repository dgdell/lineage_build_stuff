From 5f63e9cb5c048fdf4babe15d3595e72d446943de Mon Sep 17 00:00:00 2001
From: nx111 <gd.zhangdz@gmail.com>
Date: Sat, 14 Apr 2018 10:01:09 +0800
Subject: [PATCH 4/4] cpufreq: add some cpu freq governors.

governors:
abyssplugv2,dancedance,hyper,intellicative,intellidemand,interactiveX,lionheart,
ktoonservative,nightmare,ondemandplus,pegasusq,smartass2,smartassH3,smartmax.

Change-Id: I94e7d9990ea181a64a88e14599bc7ac3cfe29421
---
 arch/arm/configs/lineage_hlte_bcm2079x_defconfig   |   28 +
 arch/arm/configs/lineage_hlte_pn547_defconfig      |   28 +
 arch/arm/configs/lineage_hltechn_defconfig         |   28 +
 arch/arm/configs/lineage_hltekor_defconfig         |   28 +
 arch/arm/configs/lineage_klte_bcm2079x_defconfig   |   28 +
 arch/arm/configs/lineage_klte_pn547_defconfig      |   28 +
 arch/arm/configs/lineage_kltechn_defconfig         |   28 +
 arch/arm/configs/lineage_kltechnduo_defconfig      |   30 +
 arch/arm/configs/lineage_klteduos_defconfig        |   28 +
 arch/arm/configs/lineage_kltekdi_defconfig         |   28 +
 arch/arm/configs/lineage_kltekor_defconfig         |   28 +
 arch/arm/configs/lineage_kltesprsports_defconfig   |   28 +
 drivers/cpufreq/Kconfig                            |  268 +++
 drivers/cpufreq/Makefile                           |   16 +
 drivers/cpufreq/cpufreq_abyssplugv2.c              | 1060 +++++++++
 drivers/cpufreq/cpufreq_dancedance.c               |  651 ++++++
 drivers/cpufreq/cpufreq_hyper.c                    | 1077 +++++++++
 drivers/cpufreq/cpufreq_intelliactive.c            | 1552 +++++++++++++
 drivers/cpufreq/cpufreq_intellidemand.c            | 2442 ++++++++++++++++++++
 drivers/cpufreq/cpufreq_interactiveX.c             | 1290 +++++++++++
 drivers/cpufreq/cpufreq_ktoonservative.c           |  982 ++++++++
 drivers/cpufreq/cpufreq_lionheart.c                |  553 +++++
 drivers/cpufreq/cpufreq_nightmare.c                |  871 +++++++
 drivers/cpufreq/cpufreq_ondemandplus.c             |  982 ++++++++
 drivers/cpufreq/cpufreq_pegasusq.c                 |  719 ++++++
 drivers/cpufreq/cpufreq_smartass2.c                |  880 +++++++
 drivers/cpufreq/cpufreq_smartassH3.c               |  903 ++++++++
 drivers/cpufreq/cpufreq_smartmax.c                 | 1400 +++++++++++
 .../touchscreen/synaptics/synaptics_i2c_rmi.c      |    6 +
 include/asm-generic/cputime.h                      |    1 +
 include/linux/cpufreq.h                            |   39 +
 include/trace/events/cpufreq_interactivex.h        |  150 ++
 include/trace/events/cpufreq_ondemandplus.h        |   82 +
 kernel/power/Kconfig                               |    5 +
 kernel/power/Makefile                              |    1 +
 kernel/power/earlysuspend.c                        |    4 +
 36 files changed, 16272 insertions(+)
 create mode 100644 drivers/cpufreq/cpufreq_abyssplugv2.c
 create mode 100644 drivers/cpufreq/cpufreq_dancedance.c
 create mode 100644 drivers/cpufreq/cpufreq_hyper.c
 create mode 100644 drivers/cpufreq/cpufreq_intelliactive.c
 create mode 100644 drivers/cpufreq/cpufreq_intellidemand.c
 create mode 100644 drivers/cpufreq/cpufreq_interactiveX.c
 create mode 100644 drivers/cpufreq/cpufreq_ktoonservative.c
 create mode 100644 drivers/cpufreq/cpufreq_lionheart.c
 create mode 100644 drivers/cpufreq/cpufreq_nightmare.c
 create mode 100644 drivers/cpufreq/cpufreq_ondemandplus.c
 create mode 100644 drivers/cpufreq/cpufreq_pegasusq.c
 create mode 100644 drivers/cpufreq/cpufreq_smartass2.c
 create mode 100644 drivers/cpufreq/cpufreq_smartassH3.c
 create mode 100644 drivers/cpufreq/cpufreq_smartmax.c
 create mode 100644 include/trace/events/cpufreq_interactivex.h
 create mode 100644 include/trace/events/cpufreq_ondemandplus.h

diff --git a/arch/arm/configs/lineage_hlte_bcm2079x_defconfig b/arch/arm/configs/lineage_hlte_bcm2079x_defconfig
index 265b94fa268..568ee2f30f1 100644
--- a/arch/arm/configs/lineage_hlte_bcm2079x_defconfig
+++ b/arch/arm/configs/lineage_hlte_bcm2079x_defconfig
@@ -992,12 +992,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_hlte_pn547_defconfig b/arch/arm/configs/lineage_hlte_pn547_defconfig
index 2f7e51cd248..19bf2bb5d7f 100644
--- a/arch/arm/configs/lineage_hlte_pn547_defconfig
+++ b/arch/arm/configs/lineage_hlte_pn547_defconfig
@@ -992,12 +992,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_hltechn_defconfig b/arch/arm/configs/lineage_hltechn_defconfig
index 5263505ba1c..bcb58c98317 100644
--- a/arch/arm/configs/lineage_hltechn_defconfig
+++ b/arch/arm/configs/lineage_hltechn_defconfig
@@ -992,12 +992,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_hltekor_defconfig b/arch/arm/configs/lineage_hltekor_defconfig
index 26a11c557b3..21af3fa21cf 100644
--- a/arch/arm/configs/lineage_hltekor_defconfig
+++ b/arch/arm/configs/lineage_hltekor_defconfig
@@ -992,12 +992,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_klte_bcm2079x_defconfig b/arch/arm/configs/lineage_klte_bcm2079x_defconfig
index e9053366fc1..dde524462ea 100644
--- a/arch/arm/configs/lineage_klte_bcm2079x_defconfig
+++ b/arch/arm/configs/lineage_klte_bcm2079x_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_klte_pn547_defconfig b/arch/arm/configs/lineage_klte_pn547_defconfig
index b539597d367..e6f29607e60 100644
--- a/arch/arm/configs/lineage_klte_pn547_defconfig
+++ b/arch/arm/configs/lineage_klte_pn547_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_kltechn_defconfig b/arch/arm/configs/lineage_kltechn_defconfig
index 4d8dcbc9a26..01bab30a0cc 100644
--- a/arch/arm/configs/lineage_kltechn_defconfig
+++ b/arch/arm/configs/lineage_kltechn_defconfig
@@ -998,12 +998,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_kltechnduo_defconfig b/arch/arm/configs/lineage_kltechnduo_defconfig
index b2dead91140..59eeac72b0f 100644
--- a/arch/arm/configs/lineage_kltechnduo_defconfig
+++ b/arch/arm/configs/lineage_kltechnduo_defconfig
@@ -998,12 +998,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
@@ -1043,6 +1071,7 @@ CONFIG_ARCH_BINFMT_ELF_RANDOMIZE_PIE=y
 #
 CONFIG_SUSPEND=y
 CONFIG_SUSPEND_FREEZER=y
+CONFIG_HAS_EARLYSUSPEND=y
 CONFIG_CPU_FREQ_LIMIT_USERSPACE=y
 CONFIG_HAS_WAKELOCK=y
 CONFIG_WAKELOCK=y
@@ -4303,6 +4332,7 @@ CONFIG_MAGIC_SYSRQ=y
 CONFIG_DEBUG_FS=y
 # CONFIG_HEADERS_CHECK is not set
 # CONFIG_DEBUG_SECTION_MISMATCH is not set
+CONFIG_DEBUG_SECTION_MISMATCH=y
 CONFIG_DEBUG_KERNEL=y
 # CONFIG_DEBUG_SHIRQ is not set
 # CONFIG_LOCKUP_DETECTOR is not set
diff --git a/arch/arm/configs/lineage_klteduos_defconfig b/arch/arm/configs/lineage_klteduos_defconfig
index 7bf0cf8ee04..006cf90a4ac 100644
--- a/arch/arm/configs/lineage_klteduos_defconfig
+++ b/arch/arm/configs/lineage_klteduos_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_kltekdi_defconfig b/arch/arm/configs/lineage_kltekdi_defconfig
index 02973c4efc6..a9c81636ea2 100644
--- a/arch/arm/configs/lineage_kltekdi_defconfig
+++ b/arch/arm/configs/lineage_kltekdi_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_kltekor_defconfig b/arch/arm/configs/lineage_kltekor_defconfig
index cb82366fe39..37c997a7d41 100644
--- a/arch/arm/configs/lineage_kltekor_defconfig
+++ b/arch/arm/configs/lineage_kltekor_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/arch/arm/configs/lineage_kltesprsports_defconfig b/arch/arm/configs/lineage_kltesprsports_defconfig
index 380c959caae..c00ba229283 100644
--- a/arch/arm/configs/lineage_kltesprsports_defconfig
+++ b/arch/arm/configs/lineage_kltesprsports_defconfig
@@ -997,12 +997,40 @@ CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y
 # CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
 # CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3 is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ is not set
 CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
 CONFIG_CPU_FREQ_GOV_POWERSAVE=y
 CONFIG_CPU_FREQ_GOV_USERSPACE=y
 CONFIG_CPU_FREQ_GOV_ONDEMAND=y
 CONFIG_CPU_FREQ_GOV_INTERACTIVE=y
 CONFIG_CPU_FREQ_GOV_CONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_INTERACTIVEX=y
+CONFIG_CPU_FREQ_GOV_INTELLIACTIVE=y
+CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE=y
+CONFIG_CPU_FREQ_GOV_NIGHTMARE=y
+CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS=y
+CONFIG_CPU_FREQ_GOV_HYPER=y
+CONFIG_CPU_FREQ_GOV_SMARTASS2=y
+CONFIG_CPU_FREQ_GOV_SMARTASSH3=y
+CONFIG_CPU_FREQ_GOV_ABYSSPLUG=y
+CONFIG_CPU_FREQ_GOV_INTELLIDEMAND=y
+CONFIG_CPU_FREQ_GOV_DANCEDANCE=y
+CONFIG_CPU_FREQ_GOV_LIONHEART=y
+CONFIG_CPU_FREQ_GOV_SMARTMAX=y
+CONFIG_CPU_FREQ_GOV_PEGASUSQ=y
 
 #
 # ARM CPU frequency scaling drivers
diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index b2d230bdac1..bccb6d5e308 100644
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -59,6 +59,19 @@ choice
 	  This option sets which CPUFreq governor shall be loaded at
 	  startup. If in doubt, select 'performance'.
 
+config CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
+	bool "abyssplug"
+	select CPU_FREQ_GOV_ABYSSPLUG
+	---help---
+	  Use the CPUFreq governor 'abyssplug' as default. This allows you
+	  to get a full dynamic frequency capable system with CPU
+	  hotplug support by simply loading your cpufreq low-level
+	  hardware driver.  Be aware that not all cpufreq drivers
+	  support the hotplug governor. If unsure have a look at
+	  the help section of the driver. Fallback governor will be the
+	  performance governor.
+
+
 config CPU_FREQ_DEFAULT_GOV_PERFORMANCE
 	bool "performance"
 	select CPU_FREQ_GOV_PERFORMANCE
@@ -97,6 +110,19 @@ config CPU_FREQ_DEFAULT_GOV_ONDEMAND
 	  governor. If unsure have a look at the help section of the
 	  driver. Fallback governor will be the performance governor.
 
+config CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+	bool "ondemandplus"
+	select CPU_FREQ_GOV_ONDEMANDPLUS
+	help
+	  Use the CPUFreq governor 'ondemandplus' as default.
+
+config CPU_FREQ_DEFAULT_GOV_ONDEMANDX
+	bool "ondemandx"
+	select CPU_FREQ_GOV_ONDEMANDX
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'lionheart' as default.
+
 config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE
 	bool "conservative"
 	select CPU_FREQ_GOV_CONSERVATIVE
@@ -118,8 +144,152 @@ config CPU_FREQ_DEFAULT_GOV_INTERACTIVE
 	  loading your cpufreq low-level hardware driver, using the
 	  'interactive' governor for latency-sensitive workloads.
 
+config CPU_FREQ_DEFAULT_GOV_INTERACTIVEX
+	bool "interactiveX"
+	select CPU_FREQ_GOV_INTERACTIVEX
+	help
+	  Use the CPUFreq governor 'interactiveX' as default.
+
+config CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+	bool "intelliactive"
+	select CPU_FREQ_GOV_INTELLIACTIVE
+	help
+	  Use the CPUFreq governor 'intelliactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
+config CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE
+	bool "ktoonservative"
+	select CPU_FREQ_GOV_KTOONSERVATIVE
+	help
+	  Use the CPUFreq governor 'ktoonservative' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver.
+	  Be aware that not all cpufreq drivers support the ktoonservative
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.  This
+	  governor adds the capability of hotpluging.
+
+config CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVEQ
+	bool "ktoonservativeq"
+	select CPU_FREQ_GOV_KTOONSERVATIVEQ
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'ktoonservativeq' as default. This allows
+	  you to get a full dynamic frequency capable system by simply
+	  loading your cpufreq low-level hardware driver.
+	  Be aware that not all cpufreq drivers support the ktoonservativeq
+	  governor. If unsure have a look at the help section of the
+	  driver. Fallback governor will be the performance governor.  This
+	  governor adds the capability of hotpluging.
+
+config CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+	bool "nightmare"
+	select CPU_FREQ_GOV_NIGHTMARE
+	help
+	  Use the CPUFreq governor 'nightmare' as default. This is
+	  based on ltoonservative with browsing detection based on GPU loading
+
+config CPU_FREQ_DEFAULT_GOV_SMARTASS2
+	bool "smartass2"
+	select CPU_FREQ_GOV_SMARTASS2
+
+config CPU_FREQ_DEFAULT_GOV_SMARTASSH3
+	bool "smartassh3"
+	select CPU_FREQ_GOV_SMARTASSH3
+
+config CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+            bool "intellidemand"
+            select CPU_FREQ_GOV_INTELLIDEMAND
+            help
+              Use the CPUFreq governor 'intellidemand' as default.
+
+config CPU_FREQ_DEFAULT_GOV_DANCEDANCE
+            bool "dancedance"
+            select CPU_FREQ_GOV_DANCEDANCE
+            help
+              Use the CPUFreq governor 'dancedance' as default.
+
+config CPU_FREQ_DEFAULT_GOV_LIONHEART
+            bool "lionheart"
+            select CPU_FREQ_GOV_LIONHEART
+            help
+              Use the CPUFreq governor 'lionheart' as default.
+
+config CPU_FREQ_DEFAULT_GOV_SMARTMAX
+            bool "smartmax"
+            select CPU_FREQ_GOV_SMARTMAX
+            help
+              Use the CPUFreq governor 'smartmax' as default.
+
+config CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+	bool "pegasusq"
+	select CPU_FREQ_GOV_PEGASUSQ
+	help
+	  Use the CPUFreq governor 'pegasusq' as default.
+
+config CPU_FREQ_DEFAULT_GOV_HYPER
+	bool "hyper"
+	select CPU_FREQ_GOV_HYPER
+	help
+	  Use the CPUFreq governor 'hyper' as default.
+
 endchoice
 
+config CPU_FREQ_GOV_KTOONSERVATIVE
+	tristate "'ktoonservative' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'ktoonservative' - this driver is rather similar to the 'ondemand'
+	  governor both in its source code and its purpose, the difference is
+	  its optimisation for better suitability in a battery powered
+	  environment.  The frequency is gracefully increased and decreased
+	  rather than jumping to 100% when speed is required.
+
+	  If you have a desktop machine then you should really be considering
+	  the 'ondemand' governor instead, however if you are using a laptop,
+	  PDA or even an AMD64 based computer (due to the unacceptable
+	  step-by-step latency issues between the minimum and maximum frequency
+	  transitions in the CPU) you will probably want to use this governor.
+	  This governor adds the capability of hotpluging.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ktoonservative.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_KTOONSERVATIVEQ
+	tristate "'ktoonservativeq' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'ktoonservativeq' - this driver is rather similar to the 'ondemand'
+	  governor both in its source code and its purpose, the difference is
+	  its optimisation for better suitability in a battery powered
+	  environment.  The frequency is gracefully increased and decreased
+	  rather than jumping to 100% when speed is required.
+
+	  If you have a desktop machine then you should really be considering
+	  the 'ondemand' governor instead, however if you are using a laptop,
+	  PDA or even an AMD64 based computer (due to the unacceptable
+	  step-by-step latency issues between the minimum and maximum frequency
+	  transitions in the CPU) you will probably want to use this governor.
+	  This governor adds the capability of hotpluging.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_ktoonservativeq.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_NIGHTMARE
+        tristate "'nightmare' cpufreq governor"
+        depends on CPU_FREQ
+
+
 config CPU_FREQ_GOV_PERFORMANCE
 	tristate "'performance' governor"
 	help
@@ -142,6 +312,25 @@ config CPU_FREQ_GOV_POWERSAVE
 
 	  If in doubt, say Y.
 
+config CPU_FREQ_GOV_ABYSSPLUG
+	tristate "'abyssplug' cpufreq governor"
+	depends on CPU_FREQ && NO_HZ && HOTPLUG_CPU
+	---help---
+	  'abyssplug' - this driver mimics the frequency scaling behavior
+	  in 'ondemand', but with several key differences.  First is
+	  that frequency transitions use the CPUFreq table directly,
+	  instead of incrementing in a percentage of the maximum
+	  available frequency.  Second 'abyssplug' will offline auxillary
+	  CPUs when the system is idle, and online those CPUs once the
+	  system becomes busy again.  This last feature is needed for
+	  architectures which transition to low power states when only
+	  the "master" CPU is online, or for thermally constrained
+	  devices.
+	  If you don't have one of these architectures or devices, use
+	  'ondemand' instead.
+	  If in doubt, say N.
+
+
 config CPU_FREQ_GOV_USERSPACE
 	tristate "'userspace' governor for userspace frequency scaling"
 	help
@@ -175,6 +364,13 @@ config CPU_FREQ_GOV_ONDEMAND
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_ONDEMANDPLUS
+	tristate "'ondemandplus' cpufreq governor"
+
+config CPU_FREQ_GOV_ONDEMANDX
+	tristate "'ondemandx' cpufreq governor"
+	depends on CPU_FREQ
+
 config CPU_FREQ_GOV_INTERACTIVE
 	tristate "'interactive' cpufreq policy governor"
 	help
@@ -192,6 +388,14 @@ config CPU_FREQ_GOV_INTERACTIVE
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_INTERACTIVEX
+	tristate "'interactiveX' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'interactiveX' - interactive governor with early suspend support added.
+	  This governor uses hotplug to shutdown and start up cpu1 when suspended.
+	  If in doubt, say N.
+
 config CPU_FREQ_GOV_CONSERVATIVE
 	tristate "'conservative' cpufreq governor"
 	depends on CPU_FREQ
@@ -215,6 +419,70 @@ config CPU_FREQ_GOV_CONSERVATIVE
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_INTELLIACTIVE
+	tristate "'intelliactive' cpufreq policy governor"
+	help
+	  'intelliactive' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_interactive.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTASS2
+        tristate "'smartass2' cpufreq governor"
+        depends on CPU_FREQ
+
+config CPU_FREQ_GOV_SMARTASSH3
+        tristate "'smartassh3' cpufreq governor"
+        depends on CPU_FREQ
+
+config CPU_FREQ_GOV_INTELLIDEMAND
+           tristate "'intellidemand' cpufreq governor"
+           depends on CPU_FREQ
+           help
+             intellidemand' - a "smart" optimized governor!
+             If in doubt, say N.
+
+config CPU_FREQ_GOV_DANCEDANCE
+           tristate "'dancedance' cpufreq governor"
+           depends on CPU_FREQ
+           help
+             dancedance' - a "smart" optimized governor!
+             If in doubt, say N.
+
+config CPU_FREQ_GOV_LIONHEART
+           tristate "'lionheart' cpufreq governor"
+           depends on CPU_FREQ
+           help
+             lionheart' - a "smart" optimized governor!
+             If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTMAX
+           tristate "'smartmax' cpufreq governor"
+           depends on CPU_FREQ
+           help
+             smartmax' - a "smart" optimized governor!
+             If in doubt, say N.
+
+config CPU_FREQ_GOV_PEGASUSQ
+	tristate "'pegasusq' cpufreq policy governor"
+
+config CPU_FREQ_GOV_HYPER
+	tristate "'hyper' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'hyper' - a tweaked "ondemand" based smart and smooth optimized governor!
+
+	  If in doubt, say Y.
+
 menu "x86 CPU frequency scaling drivers"
 depends on X86
 source "drivers/cpufreq/Kconfig.x86"
diff --git a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
index 01337d77ddb..8c21ed14c1d 100644
--- a/drivers/cpufreq/Makefile
+++ b/drivers/cpufreq/Makefile
@@ -7,12 +7,28 @@ obj-$(CONFIG_CPU_FREQ_STAT)             += cpufreq_stats.o
 obj-$(CONFIG_CPU_FREQ_LIMIT)             += cpufreq_limit.o
 
 # CPUfreq governors 
+obj-$(CONFIG_CPU_FREQ_GOV_ABYSSPLUG)		+= cpufreq_abyssplugv2.o
 obj-$(CONFIG_CPU_FREQ_GOV_PERFORMANCE)	+= cpufreq_performance.o
 obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
 obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS) += cpufreq_ondemandplus.o
+obj-$(CONFIG_CPU_FREQ_GOV_ONDEMANDPLUS) += cpufreq_hyper.o
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
 obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)	+= cpufreq_interactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVEX)	+= cpufreq_interactiveX.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTELLIACTIVE)+= cpufreq_intelliactive.o
+obj-$(CONFIG_CPU_FREQ_GOV_KTOONSERVATIVE) += cpufreq_ktoonservative.o
+obj-$(CONFIG_CPU_FREQ_GOV_KTOONSERVATIVEQ) += cpufreq_ktoonservativeq.o
+obj-$(CONFIG_CPU_FREQ_GOV_NIGHTMARE)	+= cpufreq_nightmare.o
+obj-$(CONFIG_CPU_FREQ_GOV_HOTPLUG)	+= cpufreq_hotplug.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTASS2)	+= cpufreq_smartass2.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTASSH3)	+= cpufreq_smartassH3.o
+obj-$(CONFIG_CPU_FREQ_GOV_INTELLIDEMAND) += cpufreq_intellidemand.o
+obj-$(CONFIG_CPU_FREQ_GOV_DANCEDANCE)   += cpufreq_dancedance.o
+obj-$(CONFIG_CPU_FREQ_GOV_LIONHEART)    += cpufreq_lionheart.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTMAX)     += cpufreq_smartmax.o
+obj-$(CONFIG_CPU_FREQ_GOV_PEGASUSQ)	+= cpufreq_pegasusq.o
 
 # CPUfreq cross-arch helpers
 obj-$(CONFIG_CPU_FREQ_TABLE)		+= freq_table.o
diff --git a/drivers/cpufreq/cpufreq_abyssplugv2.c b/drivers/cpufreq/cpufreq_abyssplugv2.c
new file mode 100644
index 00000000000..e29bdf7fd2a
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_abyssplugv2.c
@@ -0,0 +1,1060 @@
+/*
+ *  drivers/cpufreq/cpufreq_abyssplugv2.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2012 Dennis Rassmann <showp1984@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+/*
+ * bds is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define DEF_FREQUENCY_UP_THRESHOLD		(80)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(3)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+#define POWERSAVE_BIAS_MAXLEVEL			(1000)
+#define POWERSAVE_BIAS_MINLEVEL			(-1000)
+
+static void do_bds_timer(struct work_struct *work);
+static int cpufreq_governor_bds(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
+static
+#endif
+struct cpufreq_governor cpufreq_gov_abyssplug = {
+       .name                   = "abyssplugv2",
+       .governor               = cpufreq_governor_bds,
+       .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+       .owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {BDS_NORMAL_SAMPLE, BDS_SUB_SAMPLE};
+
+struct cpu_bds_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	int cpu;
+	unsigned int sample_type:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_bds_timer invocation. We do not want do_bds_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_bds_info_s, od_cpu_bds_info);
+
+static inline void bds_timer_init(struct cpu_bds_info_s *bds_info);
+static inline void bds_timer_exit(struct cpu_bds_info_s *bds_info);
+
+static unsigned int bds_enable;	/* number of CPUs using this policy */
+
+/*
+ * bds_mutex protects bds_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(bds_mutex);
+
+static struct workqueue_struct *input_wq;
+
+static DEFINE_PER_CPU(struct work_struct, bds_refresh_work);
+
+static struct bds_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int down_differential;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	int          powersave_bias;
+	unsigned int io_is_busy;
+} bds_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+};
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu,
+							u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+
+	return idle_time;
+}
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/*
+ * Find right freq to be set now with powersave_bias on.
+ * Returns the freq_hi to be used right now and will set freq_hi_jiffies,
+ * freq_lo, and freq_lo_jiffies in percpu area for averaging freqs.
+ */
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	int freq_reduc;
+	struct cpu_bds_info_s *bds_info = &per_cpu(od_cpu_bds_info,
+						   policy->cpu);
+
+	if (!bds_info->freq_table) {
+		bds_info->freq_lo = 0;
+		bds_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, bds_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = bds_info->freq_table[index].frequency;
+	freq_reduc = freq_req * bds_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+	/* Find freq bounds for freq_avg in freq_table */
+	index = 0;
+	cpufreq_frequency_table_target(policy, bds_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = bds_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, bds_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = bds_info->freq_table[index].frequency;
+
+	/* Find out how long we have to be in hi and lo freqs */
+	if (freq_hi == freq_lo) {
+		bds_info->freq_lo = 0;
+		bds_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(bds_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	bds_info->freq_lo = freq_lo;
+	bds_info->freq_lo_jiffies = jiffies_lo;
+	bds_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static int abyssplug_powersave_bias_setspeed(struct cpufreq_policy *policy,
+					    struct cpufreq_policy *altpolicy,
+					    int level)
+{
+	if (level == POWERSAVE_BIAS_MAXLEVEL) {
+		/* maximum powersave; set to lowest frequency */
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->min : policy->min,
+			CPUFREQ_RELATION_L);
+		return 1;
+	} else if (level == POWERSAVE_BIAS_MINLEVEL) {
+		/* minimum powersave; set to highest frequency */
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->max : policy->max,
+			CPUFREQ_RELATION_H);
+		return 1;
+	}
+	return 0;
+}
+
+static void abyssplug_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_bds_info_s *bds_info = &per_cpu(od_cpu_bds_info, cpu);
+	bds_info->freq_table = cpufreq_frequency_get_table(cpu);
+	bds_info->freq_lo = 0;
+}
+
+static void abyssplug_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		abyssplug_powersave_bias_init_cpu(i);
+	}
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_abyssplug Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", bds_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(up_threshold, up_threshold);
+show_one(down_differential, down_differential);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+
+static ssize_t show_powersave_bias
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", bds_tuners_ins.powersave_bias);
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	bds_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	bds_tuners_ins.io_is_busy = !!input;
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	bds_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= bds_tuners_ins.up_threshold ||
+			input < MIN_FREQUENCY_DOWN_DIFFERENTIAL) {
+		return -EINVAL;
+	}
+
+	bds_tuners_ins.down_differential = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	bds_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_bds_info_s *bds_info;
+		bds_info = &per_cpu(od_cpu_bds_info, j);
+		bds_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == bds_tuners_ins.ignore_nice) { /* nothing to do */
+		return count;
+	}
+	bds_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_bds_info_s *bds_info;
+		bds_info = &per_cpu(od_cpu_bds_info, j);
+		bds_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&bds_info->prev_cpu_wall);
+		if (bds_tuners_ins.ignore_nice)
+			bds_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	int input  = 0;
+	int bypass = 0;
+	int ret, cpu, reenable_timer, j;
+	struct cpu_bds_info_s *bds_info;
+
+	struct cpumask cpus_timer_done;
+	cpumask_clear(&cpus_timer_done);
+
+	ret = sscanf(buf, "%d", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input >= POWERSAVE_BIAS_MAXLEVEL) {
+		input  = POWERSAVE_BIAS_MAXLEVEL;
+		bypass = 1;
+	} else if (input <= POWERSAVE_BIAS_MINLEVEL) {
+		input  = POWERSAVE_BIAS_MINLEVEL;
+		bypass = 1;
+	}
+
+	if (input == bds_tuners_ins.powersave_bias) {
+		/* no change */
+		return count;
+	}
+
+	reenable_timer = ((bds_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MAXLEVEL) ||
+				(bds_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MINLEVEL));
+
+	bds_tuners_ins.powersave_bias = input;
+	if (!bypass) {
+		if (reenable_timer) {
+			/* reinstate bds timer */
+			for_each_online_cpu(cpu) {
+				if (lock_policy_rwsem_write(cpu) < 0)
+					continue;
+
+				bds_info = &per_cpu(od_cpu_bds_info, cpu);
+
+				for_each_cpu(j, &cpus_timer_done) {
+					if (!bds_info->cur_policy) {
+						printk(KERN_ERR
+						"%s Dbs policy is NULL\n",
+						 __func__);
+						goto skip_this_cpu;
+					}
+					if (cpumask_test_cpu(j, bds_info->
+							cur_policy->cpus))
+						goto skip_this_cpu;
+				}
+
+				cpumask_set_cpu(cpu, &cpus_timer_done);
+				if (bds_info->cur_policy) {
+					/* restart bds timer */
+					bds_timer_init(bds_info);
+				}
+skip_this_cpu:
+				unlock_policy_rwsem_write(cpu);
+			}
+		}
+		abyssplug_powersave_bias_init();
+	} else {
+		/* running at maximum or minimum frequencies; cancel
+		   bds timer as periodic load sampling is not necessary */
+		for_each_online_cpu(cpu) {
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			bds_info = &per_cpu(od_cpu_bds_info, cpu);
+
+			for_each_cpu(j, &cpus_timer_done) {
+				if (!bds_info->cur_policy) {
+					printk(KERN_ERR
+					"%s Dbs policy is NULL\n",
+					 __func__);
+					goto skip_this_cpu_bypass;
+				}
+				if (cpumask_test_cpu(j, bds_info->
+							cur_policy->cpus))
+					goto skip_this_cpu_bypass;
+			}
+
+			cpumask_set_cpu(cpu, &cpus_timer_done);
+
+			if (bds_info->cur_policy) {
+				/* cpu using abyssplug, cancel bds timer */
+				mutex_lock(&bds_info->timer_mutex);
+				bds_timer_exit(bds_info);
+
+				abyssplug_powersave_bias_setspeed(
+					bds_info->cur_policy,
+					NULL,
+					input);
+
+				mutex_unlock(&bds_info->timer_mutex);
+			}
+skip_this_cpu_bypass:
+			unlock_policy_rwsem_write(cpu);
+		}
+	}
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+
+
+static struct attribute *bds_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&down_differential.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&io_is_busy.attr,
+	NULL
+};
+
+static struct attribute_group bds_attr_group = {
+	.attrs = bds_attributes,
+	.name = "abyssplugv2",
+};
+
+/************************** sysfs end ************************/
+
+static void bds_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (bds_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, bds_tuners_ins.powersave_bias ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+}
+
+static void bds_check_cpu(struct cpu_bds_info_s *this_bds_info)
+{
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	this_bds_info->freq_lo = 0;
+	policy = this_bds_info->cur_policy;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_bds_info_s *j_bds_info;
+		cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+
+		j_bds_info = &per_cpu(od_cpu_bds_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int) (cur_wall_time - j_bds_info->prev_cpu_wall);
+		j_bds_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int) (cur_idle_time - j_bds_info->prev_cpu_idle);
+		j_bds_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int) (cur_iowait_time - j_bds_info->prev_cpu_iowait);
+		j_bds_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (bds_tuners_ins.ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_bds_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_bds_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		/*
+		 * For the purpose of abyssplug, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (bds_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+	}
+	/* calculate the scaled load across CPU */
+	load_at_max_freq = (cur_load * policy->cur)/policy->cpuinfo.max_freq;
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	/* Check for frequency increase */
+	if (max_load_freq > bds_tuners_ins.up_threshold * policy->cur) {
+		/* If switching to max speed, apply sampling_down_factor */
+		
+			/* busy phase */
+			if (policy->cur < policy->max)
+				this_bds_info->rate_mult =
+					bds_tuners_ins.sampling_down_factor;
+			bds_freq_increase(policy, policy->max);
+		return;
+	}
+
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (bds_tuners_ins.up_threshold - bds_tuners_ins.down_differential) *
+	     policy->cur) {
+		unsigned int freq_next;
+		freq_next = max_load_freq /
+				(bds_tuners_ins.up_threshold -
+				 bds_tuners_ins.down_differential);
+
+		/* No longer fully busy, reset rate_mult */
+		this_bds_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if (!bds_tuners_ins.powersave_bias) {
+			__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		} else {
+			int freq = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+		}
+	}
+}
+
+static void do_bds_timer(struct work_struct *work)
+{
+	struct cpu_bds_info_s *bds_info =
+		container_of(work, struct cpu_bds_info_s, work.work);
+	unsigned int cpu = bds_info->cpu;
+	int sample_type = bds_info->sample_type;
+
+	int delay;
+
+	mutex_lock(&bds_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	bds_info->sample_type = BDS_NORMAL_SAMPLE;
+	if (!bds_tuners_ins.powersave_bias ||
+	    sample_type == BDS_NORMAL_SAMPLE) {
+		bds_check_cpu(bds_info);
+		if (bds_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			bds_info->sample_type = BDS_SUB_SAMPLE;
+			delay = bds_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(bds_tuners_ins.sampling_rate
+				* bds_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		__cpufreq_driver_target(bds_info->cur_policy,
+			bds_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = bds_info->freq_lo_jiffies;
+	}
+	schedule_delayed_work_on(cpu, &bds_info->work, delay);
+	mutex_unlock(&bds_info->timer_mutex);
+}
+
+static inline void bds_timer_init(struct cpu_bds_info_s *bds_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(bds_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	bds_info->sample_type = BDS_NORMAL_SAMPLE;
+	INIT_DELAYED_WORK_DEFERRABLE(&bds_info->work, do_bds_timer);
+	schedule_delayed_work_on(bds_info->cpu, &bds_info->work, delay);
+}
+
+static inline void bds_timer_exit(struct cpu_bds_info_s *bds_info)
+{
+	cancel_delayed_work_sync(&bds_info->work);
+}
+
+/*
+ * Not all CPUs want IO time to be accounted as busy; this dependson how
+ * efficient idling at a higher frequency/voltage is.
+ * Pavel Machek says this is not so for various generations of AMD and old
+ * Intel systems.
+ * Mike Chan (androidlcom) calis this is also not true for ARM.
+ * Because of this, whitelist specific known (series) of CPUs by default, and
+ * leave all others up to the user.
+ */
+static int should_io_be_busy(void)
+{
+#if defined(CONFIG_X86)
+	/*
+	 * For Intel, Core 2 (model 15) andl later have an efficient idle.
+	 */
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+	    boot_cpu_data.x86 == 6 &&
+	    boot_cpu_data.x86_model >= 15)
+		return 1;
+#endif
+	return 0;
+}
+
+static void bds_refresh_callback(struct work_struct *unused)
+{
+	struct cpufreq_policy *policy;
+	struct cpu_bds_info_s *this_bds_info;
+	unsigned int cpu = smp_processor_id();
+
+	if (lock_policy_rwsem_write(cpu) < 0)
+		return;
+
+	this_bds_info = &per_cpu(od_cpu_bds_info, cpu);
+	policy = this_bds_info->cur_policy;
+	if (!policy) {
+		/* CPU not using abyssplug governor */
+		unlock_policy_rwsem_write(cpu);
+		return;
+	}
+
+	if (policy->cur < policy->max) {
+		policy->cur = policy->max;
+
+		__cpufreq_driver_target(policy, policy->max,
+					CPUFREQ_RELATION_L);
+		this_bds_info->prev_cpu_idle = get_cpu_idle_time(cpu,
+				&this_bds_info->prev_cpu_wall);
+	}
+	unlock_policy_rwsem_write(cpu);
+}
+
+static unsigned int enable_bds_input_event;
+static void bds_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value)
+{
+	int i;
+
+	if (enable_bds_input_event) {
+
+		if ((bds_tuners_ins.powersave_bias == POWERSAVE_BIAS_MAXLEVEL) ||
+			(bds_tuners_ins.powersave_bias == POWERSAVE_BIAS_MINLEVEL)) {
+			/* nothing to do */
+			return;
+		}
+
+		for_each_online_cpu(i) {
+			queue_work_on(i, input_wq, &per_cpu(bds_refresh_work, i));
+		}
+	}
+}
+
+static int bds_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void bds_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id bds_ids[] = {
+	{ .driver_info = 1 },
+	{ },
+};
+
+static struct input_handler bds_input_handler = {
+	.event		= bds_input_event,
+	.connect	= bds_input_connect,
+	.disconnect	= bds_input_disconnect,
+	.name		= "cpufreq_abyss",
+	.id_table	= bds_ids,
+};
+
+static int cpufreq_governor_bds(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_bds_info_s *this_bds_info;
+	unsigned int j;
+	int rc;
+
+	this_bds_info = &per_cpu(od_cpu_bds_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&bds_mutex);
+
+		bds_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_bds_info_s *j_bds_info;
+			j_bds_info = &per_cpu(od_cpu_bds_info, j);
+			j_bds_info->cur_policy = policy;
+
+			j_bds_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_bds_info->prev_cpu_wall);
+			if (bds_tuners_ins.ignore_nice) {
+				j_bds_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_bds_info->cpu = cpu;
+		this_bds_info->rate_mult = 1;
+		abyssplug_powersave_bias_init_cpu(cpu);
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (bds_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&bds_attr_group);
+			if (rc) {
+				mutex_unlock(&bds_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			bds_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+			bds_tuners_ins.io_is_busy = should_io_be_busy();
+		}
+		if (!cpu)
+			rc = input_register_handler(&bds_input_handler);
+		mutex_unlock(&bds_mutex);
+
+		mutex_init(&this_bds_info->timer_mutex);
+
+		if (!abyssplug_powersave_bias_setspeed(
+					this_bds_info->cur_policy,
+					NULL,
+					bds_tuners_ins.powersave_bias))
+			bds_timer_init(this_bds_info);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		bds_timer_exit(this_bds_info);
+
+		mutex_lock(&bds_mutex);
+		mutex_destroy(&this_bds_info->timer_mutex);
+		bds_enable--;
+		/* If device is being removed, policy is no longer
+		 * valid. */
+		this_bds_info->cur_policy = NULL;
+		if (!cpu)
+			input_unregister_handler(&bds_input_handler);
+		mutex_unlock(&bds_mutex);
+		if (!bds_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &bds_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_bds_info->timer_mutex);
+		if (policy->max < this_bds_info->cur_policy->cur)
+			__cpufreq_driver_target(this_bds_info->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_bds_info->cur_policy->cur)
+			__cpufreq_driver_target(this_bds_info->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		else if (bds_tuners_ins.powersave_bias != 0)
+			abyssplug_powersave_bias_setspeed(
+				this_bds_info->cur_policy,
+				policy,
+				bds_tuners_ins.powersave_bias);
+		mutex_unlock(&this_bds_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_bds_init(void)
+{
+	cputime64_t wall;
+	u64 idle_time;
+	unsigned int i;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		bds_tuners_ins.up_threshold = MICRO_FREQUENCY_UP_THRESHOLD;
+		bds_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	input_wq = create_workqueue("iewq");
+	if (!input_wq) {
+		printk(KERN_ERR "Failed to create iewq workqueue\n");
+		return -EFAULT;
+	}
+	for_each_possible_cpu(i) {
+		INIT_WORK(&per_cpu(bds_refresh_work, i), bds_refresh_callback);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_abyssplug);
+}
+
+static void __exit cpufreq_gov_bds_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_abyssplug);
+	destroy_workqueue(input_wq);
+}
+
+static int set_enable_bds_input_event_param(const char *val, struct kernel_param *kp)
+{
+	int ret = 0;
+
+	ret = param_set_uint(val, kp);
+	if (ret)
+		pr_err("%s: error setting value %d\n", __func__, ret);
+
+	return ret;
+}
+module_param_call(enable_bds_input_event, set_enable_bds_input_event_param, param_get_uint,
+		&enable_bds_input_event, S_IWUSR | S_IRUGO);
+
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_AUTHOR("Dennis Rassmann <showp1984@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_abyssplugv2' - An abyssplug cpufreq governor based on ondemand");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG
+fs_initcall(cpufreq_gov_bds_init);
+#else
+module_init(cpufreq_gov_bds_init);
+#endif
+module_exit(cpufreq_gov_bds_exit);
+
diff --git a/drivers/cpufreq/cpufreq_dancedance.c b/drivers/cpufreq/cpufreq_dancedance.c
new file mode 100644
index 00000000000..2a444c486fc
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_dancedance.c
@@ -0,0 +1,651 @@
+/*
+ *  drivers/cpufreq/cpufreq_dancedance.c
+ *
+ *  Copyright (C)  2012 Shaun Nuzzo <jrracinfan@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/cpuidle.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(90)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(30)
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_dancedance = {
+    .name                   = "dancedance",
+    .governor               = cpufreq_governor_dbs,
+    .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+    .owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+    cputime64_t prev_cpu_idle;
+    cputime64_t prev_cpu_iowait;
+    cputime64_t prev_cpu_wall;
+    cputime64_t prev_cpu_nice;
+    struct cpufreq_policy *cur_policy;
+    struct delayed_work work;
+    struct cpufreq_frequency_table *freq_table;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+    unsigned int freq_lo;
+    unsigned int freq_lo_jiffies;
+    unsigned int freq_hi_jiffies;
+    unsigned int rate_mult;
+    int cpu;
+    unsigned int sample_type:1;
+    unsigned long long prev_idletime;
+    unsigned long long prev_idleusage;
+	unsigned int enable:1;
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+    unsigned int sampling_rate;
+    unsigned int up_threshold;
+    unsigned int down_differential;
+    unsigned int ignore_nice;
+    unsigned int sampling_down_factor;
+    unsigned int powersave_bias;
+    unsigned int io_is_busy;
+    unsigned int target_residency;
+    unsigned int allowed_misses;
+	unsigned int freq_step;
+	unsigned int down_threshold;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.ignore_nice = 0,
+	.freq_step = 5,
+};
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu,
+						  u64 *wall)
+{
+    u64 idle_time;
+    u64 cur_wall_time;
+    u64 busy_time;
+
+    cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+    busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+    busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+    busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+    busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+    busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+    busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+    idle_time = cur_wall_time - busy_time;
+    if (wall)
+	*wall = jiffies_to_usecs(cur_wall_time);
+
+    return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
+{
+    u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+    if (idle_time == -1ULL)
+	return get_cpu_idle_time_jiffy(cpu, wall);
+
+    return idle_time;
+}
+
+/* keep track of frequency transitions */
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside
+	 * the 'valid' ranges of freqency available to us otherwise
+	 * we do not change it
+	*/
+	if (this_dbs_info->requested_freq > policy->max
+			|| this_dbs_info->requested_freq < policy->min)
+		this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+/************************** sysfs interface ************************/
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_dancedance Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(down_threshold, down_threshold);
+show_one(ignore_nice_load, ignore_nice);
+show_one(freq_step, freq_step);
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) /* nothing to do */
+		return count;
+
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall);
+		if (dbs_tuners_ins.ignore_nice)
+		      dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	/* no need to test here if freq_step is zero as the user might actually
+	 * want this, they would be crazy though :) */
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_threshold);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+	&ignore_nice_load.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "dancedance",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load = 0;
+	unsigned int max_load = 0;
+	unsigned int freq_target;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate*sampling_down_factor, we check, if current
+	 * idle time is more than 80%, then we try to decrease frequency
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of maximum frequency
+	 */
+
+	/* Get Absolute Load */
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int) (cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int) (cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+		        cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+             			j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+         		j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (load > max_load)
+			max_load = load;
+	}
+
+	/*
+	 * break out if we 'cannot' reduce the speed as the user might
+	 * want freq_step to be zero
+	 */
+	if (dbs_tuners_ins.freq_step == 0)
+		return;
+
+	/* Check for frequency increase */
+	if (max_load > dbs_tuners_ins.up_threshold) {
+		this_dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		/* max freq cannot be less than 100. But who knows.... */
+		if (unlikely(freq_target == 0))
+			freq_target = 5;
+
+		this_dbs_info->requested_freq += freq_target;
+		if (this_dbs_info->requested_freq > policy->max)
+			this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load < (dbs_tuners_ins.down_threshold - 10)) {
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		this_dbs_info->requested_freq -= freq_target;
+		if (this_dbs_info->requested_freq < policy->min)
+			this_dbs_info->requested_freq = policy->min;
+
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+    struct cpu_dbs_info_s *dbs_info =
+	container_of(work, struct cpu_dbs_info_s, work.work);
+    unsigned int cpu = dbs_info->cpu;
+    int sample_type = dbs_info->sample_type;
+
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+    /* Common NORMAL_SAMPLE setup */
+    dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+    if (!dbs_tuners_ins.powersave_bias ||
+	sample_type == DBS_NORMAL_SAMPLE) {
+	dbs_check_cpu(dbs_info);
+	if (dbs_info->freq_lo) {
+	    /* Setup timer for SUB_SAMPLE */
+	    dbs_info->sample_type = DBS_SUB_SAMPLE;
+	    delay = dbs_info->freq_hi_jiffies;
+	} else {
+	    /* We want all CPUs to do sampling nearly on
+	     * same jiffy
+	     */
+	    delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				     * dbs_info->rate_mult);
+	    if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+	}
+    } else {
+	__cpufreq_driver_target(dbs_info->cur_policy,
+				dbs_info->freq_lo, CPUFREQ_RELATION_H);
+	delay = dbs_info->freq_lo_jiffies;
+    }
+
+    schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+    mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+    /* We want all CPUs to do sampling nearly on same jiffy */
+    int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+    if (num_online_cpus() > 1)
+	delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/*
+			 * conservative does not implement micro like ondemand
+			 * governor, thus we are bound to jiffes/HZ
+			 */
+			min_sampling_rate =
+				MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		/*
+		 * Stop the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 0)
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_dancedance);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_dancedance);
+}
+
+MODULE_AUTHOR("Shaun Nuzzo <jrracinfan@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_dancedance' - A dynamic cpufreq governor for "
+		"Low Latency Frequency Transition capable processors "
+		"optimised for use in a battery environment"
+		"Modified code based off conservative with a faster"
+		"deep sleep rate");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
+
diff --git a/drivers/cpufreq/cpufreq_hyper.c b/drivers/cpufreq/cpufreq_hyper.c
new file mode 100644
index 00000000000..0988f8dbfff
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_hyper.c
@@ -0,0 +1,1077 @@
+/*
+ *  drivers/cpufreq/cpufreq_HYPER.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ *                 2012 Minor Edits by Sar Castillo <sar.castillo@gmail.com>
+ *                 2012 MAR heavy addons by DORIMANX <yuri@bynet.co.il>
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/slab.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define DEF_FREQUENCY_UP_THRESHOLD		(70)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define BOOSTED_SAMPLING_DOWN_FACTOR		(10)
+#define MAX_SAMPLING_DOWN_FACTOR		(3)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(5)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(70)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(5000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define FREQ_STEP				(50)
+#define DEFAULT_FREQ_BOOST_TIME			(500000)
+#define MAX_FREQ_BOOST_TIME			(5000000)
+#define UP_THRESHOLD_AT_MIN_FREQ		(40)
+#define FREQ_FOR_RESPONSIVENESS			(2265600)
+
+static u64 hyper_freq_boosted_time;
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+#define DEFAULT_SAMPLING_RATE			(80000)
+#define BOOSTED_SAMPLING_RATE			(40000)
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+/* have the timer rate booted for this much time 4s*/
+#define TIMER_RATE_BOOST_TIME 4000000
+static int hyper_sampling_rate_boosted;
+static u64 hyper_sampling_rate_boosted_time;
+unsigned int hyper_current_sampling_rate;
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER
+static
+#endif
+struct cpufreq_governor cpufreq_gov_HYPER = {
+       .name                   = "HYPER",
+       .governor               = cpufreq_governor_dbs,
+       .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+       .owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	unsigned int prev_cpu_wall_delta;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int load_at_prev_sample;
+	int cpu;
+	unsigned int sample_type:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+	bool activated; /* dbs_timer_init is in effect */
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_min_freq;
+	unsigned int down_differential;
+	unsigned int micro_freq_up_threshold;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	unsigned int powersave_bias;
+	unsigned int io_is_busy;
+	unsigned int boosted;
+	unsigned int freq_boost_time;
+	unsigned int boostfreq;
+	/*struct notifier_block dvfs_lat_qos_db;
+	unsigned int dvfs_lat_qos_wants;*/
+	unsigned int freq_step;
+	unsigned int freq_responsiveness;
+
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold_min_freq = UP_THRESHOLD_AT_MIN_FREQ,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.micro_freq_up_threshold = MICRO_FREQUENCY_UP_THRESHOLD,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+	.freq_boost_time = DEFAULT_FREQ_BOOST_TIME,
+	.boostfreq = 2265600,
+	.freq_step = FREQ_STEP,
+	.freq_responsiveness = FREQ_FOR_RESPONSIVENESS,
+	.sampling_rate = 60000,
+};
+
+static unsigned int dbs_enable = 0;	/* number of CPUs using this policy */
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, bool io_is_busy)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+/*
+ * Find right freq to be set now with powersave_bias on.
+ * Returns the freq_hi to be used right now and will set freq_hi_jiffies,
+ * freq_lo, and freq_lo_jiffies in percpu area for averaging freqs.
+ */
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_reduc, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info,
+						   policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+	/* Find freq bounds for freq_avg in freq_table */
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+	/* Find out how long we have to be in hi and lo freqs */
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static void hyper_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+	dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
+	dbs_info->freq_lo = 0;
+}
+
+static void hyper_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		hyper_powersave_bias_init_cpu(i);
+	}
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_hyper_power Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_min_freq, up_threshold_min_freq);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(powersave_bias, powersave_bias);
+show_one(micro_freq_up_threshold, micro_freq_up_threshold);
+show_one(down_differential, down_differential);
+show_one(boostpulse, boosted);
+show_one(boostfreq, boostfreq);
+show_one(freq_step, freq_step);
+show_one(freq_responsiveness, freq_responsiveness);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned int input;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret < 0)
+		return ret;
+
+	if (input > 1 && input <= MAX_FREQ_BOOST_TIME)
+		dbs_tuners_ins.freq_boost_time = input;
+	else
+		dbs_tuners_ins.freq_boost_time = DEFAULT_FREQ_BOOST_TIME;
+
+	dbs_tuners_ins.boosted = 1;
+	hyper_freq_boosted_time = ktime_to_us(ktime_get());
+
+	if (hyper_sampling_rate_boosted) {
+		hyper_sampling_rate_boosted = 0;
+		dbs_tuners_ins.sampling_rate = hyper_current_sampling_rate;
+	}
+	return count;
+}
+
+static ssize_t store_boostfreq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.boostfreq = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	hyper_current_sampling_rate = dbs_tuners_ins.sampling_rate;
+
+	return count;
+}
+
+/* io_is_busy */
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.io_is_busy = !!input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_micro_freq_up_threshold(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.micro_freq_up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_differential = min(input, 100u);
+
+	return count;
+}
+
+static ssize_t store_up_threshold_min_freq(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_min_freq = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+/* ignore_nice_load */
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) {/* nothing to do */
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall,
+						dbs_tuners_ins.io_is_busy);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1000)
+		input = 1000;
+
+	dbs_tuners_ins.powersave_bias = input;
+	hyper_powersave_bias_init();
+
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_step = min(input, 100u);
+	return count;
+}
+
+static ssize_t store_freq_responsiveness(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 2496000)
+		input = 2496000;
+
+	if (input < 300000)
+		input = 300000;
+
+	dbs_tuners_ins.freq_responsiveness = input;
+
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(up_threshold);
+define_one_global_rw(up_threshold_min_freq);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+define_one_global_rw(down_differential);
+define_one_global_rw(micro_freq_up_threshold);
+define_one_global_rw(boostpulse);
+define_one_global_rw(boostfreq);
+define_one_global_rw(freq_step);
+define_one_global_rw(freq_responsiveness);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&up_threshold_min_freq.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&io_is_busy.attr,
+	&down_differential.attr,
+	&micro_freq_up_threshold.attr,
+	&boostpulse.attr,
+	&boostfreq.attr,
+	&freq_step.attr,
+	&freq_responsiveness.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "HYPER",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (dbs_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, dbs_tuners_ins.powersave_bias ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int avg_load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+	unsigned int max_load = 0;
+
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *j_dbs_info;
+	unsigned int j = 0;
+	int up_threshold = dbs_tuners_ins.up_threshold;
+
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+	j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+	/* Only core0 controls the boost */
+	if (dbs_tuners_ins.boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - hyper_freq_boosted_time >=
+					dbs_tuners_ins.freq_boost_time) {
+			dbs_tuners_ins.boosted = 0;
+		}
+	}
+
+	/* Only core0 controls the timer_rate */
+	if (hyper_sampling_rate_boosted && policy->cpu == 0) {
+		if (ktime_to_us(ktime_get()) - hyper_sampling_rate_boosted_time >=
+					TIMER_RATE_BOOST_TIME) {
+
+			dbs_tuners_ins.sampling_rate = hyper_current_sampling_rate;
+			hyper_sampling_rate_boosted = 0;
+		}
+	}
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+		bool deep_sleep_detected = false;
+		/* the evil magic numbers, only 2 at least */
+		const unsigned int deep_sleep_backoff = 10;
+		const unsigned int deep_sleep_factor = 5;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time,
+					dbs_tuners_ins.io_is_busy);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		/*
+		 * Ignore wall delta jitters in both directions.  An
+		 * exceptionally long wall_time will likely result
+		 * idle but it was waken up to do work so the next
+		 * slice is less likely to want to run at low
+		 * frequency. Let's evaluate the next slice instead of
+		 * the idle long one that passed already and it's too
+		 * late to reduce in frequency.  As opposed an
+		 * exceptionally short slice that just run at low
+		 * frequency is unlikely to be idle, but we may go
+		 * back to idle pretty soon and that not idle slice
+		 * already passed. If short slices will keep coming
+		 * after a series of long slices the exponential
+		 * backoff will converge faster and we'll react faster
+		 * to high load. As opposed we'll decay slower
+		 * towards low load and long idle times.
+		 */
+		if (j_dbs_info->prev_cpu_wall_delta >
+		    wall_time * deep_sleep_factor ||
+		    j_dbs_info->prev_cpu_wall_delta * deep_sleep_factor <
+		    wall_time)
+			deep_sleep_detected = true;
+		j_dbs_info->prev_cpu_wall_delta =
+			(j_dbs_info->prev_cpu_wall_delta * deep_sleep_backoff
+			 + wall_time) / (deep_sleep_backoff+1);
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+			(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (deep_sleep_detected)
+			continue;
+
+		/*
+		 * For the purpose of HYPER, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (cur_load > max_load)
+			max_load = cur_load;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+		/* calculate the scaled load across CPU */
+		load_at_max_freq += (cur_load * policy->cur) /
+					policy->max;
+
+		avg_load_at_max_freq += ((load_at_max_freq +
+				j_dbs_info->load_at_prev_sample) / 2);
+
+		j_dbs_info->load_at_prev_sample = load_at_max_freq;
+	}
+
+	cpufreq_notify_utilization(policy, max_load);
+
+	/* Check for frequency increase */
+	if (policy->cur < dbs_tuners_ins.freq_responsiveness)
+			up_threshold = dbs_tuners_ins.up_threshold_min_freq;
+
+	if (max_load_freq > up_threshold * policy->cur) {
+		int inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+		int target = min(policy->max, policy->cur + inc);
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max && target == policy->max) {
+			if (hyper_sampling_rate_boosted &&
+				(dbs_tuners_ins.sampling_down_factor <
+					BOOSTED_SAMPLING_DOWN_FACTOR)) {
+				this_dbs_info->rate_mult =
+					BOOSTED_SAMPLING_DOWN_FACTOR;
+			} else {
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+			}
+		}
+		dbs_freq_increase(policy, target);
+		return;
+	}
+
+	/* check for frequency boost */
+	if (dbs_tuners_ins.boosted && policy->cur < dbs_tuners_ins.boostfreq) {
+		dbs_freq_increase(policy, dbs_tuners_ins.boostfreq);
+		dbs_tuners_ins.boostfreq = policy->cur;
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+			policy->cur) {
+		unsigned int freq_next;
+		unsigned int down_thres;
+
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+					dbs_tuners_ins.down_differential);
+
+		if (dbs_tuners_ins.boosted &&
+				freq_next < dbs_tuners_ins.boostfreq) {
+			freq_next = dbs_tuners_ins.boostfreq;
+		}
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		down_thres = dbs_tuners_ins.up_threshold_min_freq
+			- dbs_tuners_ins.down_differential;
+
+		if (freq_next < dbs_tuners_ins.freq_responsiveness
+			&& (max_load_freq / freq_next) > down_thres)
+			freq_next = dbs_tuners_ins.freq_responsiveness;
+
+		if (!dbs_tuners_ins.powersave_bias) {
+			__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		} else {
+			int freq = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+		}
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+
+	int delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, 10 * delay);
+	dbs_info->activated = true;
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->activated = false;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+/*
+ * Not all CPUs want IO time to be accounted as busy; this dependson how
+ * efficient idling at a higher frequency/voltage is.
+ * Pavel Machek says this is not so for various generations of AMD and old
+ * Intel systems.
+ * Mike Chan (androidlcom) calis this is also not true for ARM.
+ * Because of this, whitelist specific known (series) of CPUs by default, and
+ * leave all others up to the user.
+ */
+static int should_io_be_busy(void)
+{
+	return 0;
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall,
+						dbs_tuners_ins.io_is_busy);
+			if (dbs_tuners_ins.ignore_nice)
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		hyper_powersave_bias_init_cpu(cpu);
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			if (latency != 1)
+				dbs_tuners_ins.sampling_rate =
+					max(dbs_tuners_ins.sampling_rate,
+						latency * LATENCY_MULTIPLIER);
+
+			dbs_tuners_ins.io_is_busy = should_io_be_busy();
+		}
+		mutex_unlock(&dbs_mutex);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		dbs_enable--;
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		/* If device is being removed, skip set limits */
+		if (!this_dbs_info->cur_policy)
+			break;
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+	int err = 0;
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = dbs_tuners_ins.micro_freq_up_threshold;
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	err = cpufreq_register_governor(&cpufreq_gov_HYPER);
+	if (err)
+		goto error_reg;
+
+	return err;
+error_reg:
+	kfree(&dbs_tuners_ins);
+	return err;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	/*pm_qos_remove_notifier(PM_QOS_DVFS_RESPONSE_LATENCY,
+			       &hyper_qos_dvfs_lat_nb);*/
+
+	cpufreq_unregister_governor(&cpufreq_gov_HYPER);
+	kfree(&dbs_tuners_ins);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_DESCRIPTION("'cpufreq_HYPER' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
+
diff --git a/drivers/cpufreq/cpufreq_intelliactive.c b/drivers/cpufreq/cpufreq_intelliactive.c
new file mode 100644
index 00000000000..151ac726118
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_intelliactive.c
@@ -0,0 +1,1552 @@
+/*
+ * drivers/cpufreq/cpufreq_intelliactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ * Copyright (C) 2014 Paul Reioux
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ * Author: Paul Reioux (reioux@gmail.com) Modified for intelliactive
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+#include <linux/input.h>
+
+static int active_count;
+
+struct cpufreq_interactive_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+	int prev_load;
+	unsigned int two_phase_freq;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Sampling down factor to be applied to min_sample_time at max freq */
+static unsigned int sampling_down_factor;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/* Busy SDF parameters*/
+#define MIN_BUSY_TIME (100 * USEC_PER_MSEC)
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+static bool io_is_busy = 1;
+
+/*
+ * If the max load among other CPUs is higher than up_threshold_any_cpu_load
+ * and if the highest frequency among the other CPUs is higher than
+ * up_threshold_any_cpu_freq then do not let the frequency to drop below
+ * sync_freq
+ */
+static unsigned int up_threshold_any_cpu_load = 95;
+static unsigned int sync_freq = 700000;
+static unsigned int up_threshold_any_cpu_freq = 920000;
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 1060000} ;
+
+static int cpufreq_governor_intelliactive(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_intelliactive = {
+	.name = "intelliactive",
+	.governor = cpufreq_governor_intelliactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+							cputime64_t *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu,
+					    cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+	if (idle_time == -1ULL)
+		idle_time = get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+static void cpufreq_interactive_timer_resched(
+	struct cpufreq_interactive_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				     &pcpu->time_in_idle_timestamp);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactive_timer_start(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long expires = jiffies + usecs_to_jiffies(timer_rate);
+	unsigned long flags;
+
+	pcpu->cpu_timer.expires = expires;
+	if (cpu_online(cpu)) {
+		add_timer_on(&pcpu->cpu_timer, cpu);
+		if (timer_slack_val >= 0 && pcpu->target_freq >
+		     pcpu->policy->min) {
+			expires += usecs_to_jiffies(timer_slack_val);
+			pcpu->cpu_slack_timer.expires = expires;
+			add_timer_on(&pcpu->cpu_slack_timer, cpu);
+		}
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	ret = (ret > (1 * USEC_PER_MSEC)) ? (ret - (1 * USEC_PER_MSEC)) : ret;
+
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactive_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now);
+	delta_idle = (unsigned int)(now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int)(now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactive_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+	unsigned long mod_min_sample_time;
+	int i, max_load;
+	unsigned int max_freq;
+	struct cpufreq_interactive_cpuinfo *picpu;
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	if (cpu_is_offline(data))
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	pcpu->prev_load = cpu_load;
+	boosted = boost_val || now < boostpulse_endtime;
+
+	// HACK HACK HACK BEGIN
+	if (counter < 5) {
+		counter++;
+		if (counter > 2) {
+			phase = 1;
+		}
+	}
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < hispeed_freq) {
+			nr_cpus = num_online_cpus();
+
+			pcpu->two_phase_freq = two_phase_freq_array[nr_cpus-1];
+			if (pcpu->two_phase_freq < pcpu->policy->cur)
+				phase = 1;
+			if (pcpu->two_phase_freq != 0 && phase == 0) {
+				new_freq = pcpu->two_phase_freq;
+			} else
+				new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+
+		if (sync_freq && new_freq < sync_freq) {
+
+			max_load = 0;
+			max_freq = 0;
+
+			for_each_online_cpu(i) {
+				picpu = &per_cpu(cpuinfo, i);
+
+				if (i == data || picpu->prev_load <
+						up_threshold_any_cpu_load)
+					continue;
+
+				max_load = max(max_load, picpu->prev_load);
+				max_freq = max(max_freq, picpu->policy->cur);
+			}
+
+			if (max_freq > up_threshold_any_cpu_freq &&
+				max_load >= up_threshold_any_cpu_load)
+				new_freq = sync_freq;
+		}
+	}
+
+	if (counter > 0) {
+		counter--;
+		if (counter == 0) {
+			phase = 0;
+		}
+	}
+
+	if (pcpu->target_freq >= hispeed_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index))
+		goto rearm;
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (sampling_down_factor && pcpu->policy->cur == pcpu->policy->max)
+		mod_min_sample_time = sampling_down_factor;
+	else
+		mod_min_sample_time = min_sample_time;
+
+	if (new_freq < pcpu->floor_freq) {
+		if (now - pcpu->floor_validate_time < mod_min_sample_time) {
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq) {
+		goto rearm_if_notmax;
+	}
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactive_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactive_idle_start(void)
+{
+	int cpu = smp_processor_id();
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+	u64 now;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	/* Cancel the timer if cpu is offline */
+	if (cpu_is_offline(cpu)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		goto exit;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			cpufreq_interactive_timer_resched(pcpu);
+
+			now = ktime_to_us(ktime_get());
+			if ((pcpu->policy->cur == pcpu->policy->max) &&
+				(now - pcpu->hispeed_validate_time) >
+							MIN_BUSY_TIME) {
+				pcpu->floor_validate_time = now;
+			}
+
+		}
+	}
+exit:
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_interactive_idle_end(void)
+{
+	struct cpufreq_interactive_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactive_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactive_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactive_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_interactive_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactive_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static int cpufreq_interactive_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactive_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactive_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static struct global_attr two_phase_freq_attr =
+	__ATTR(two_phase_freq, S_IRUGO | S_IWUSR,
+		show_two_phase_freq, store_two_phase_freq);
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret = ret - 1;
+	ret += sprintf(buf + ret, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret = ret - 1;
+	ret += sprintf(buf + ret, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+static ssize_t show_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sampling_down_factor);
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *kobj,
+				struct attribute *attr, const char *buf,
+				size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sampling_down_factor = val;
+	return count;
+}
+
+static struct global_attr sampling_down_factor_attr =
+				__ATTR(sampling_down_factor, 0644,
+		show_sampling_down_factor, store_sampling_down_factor);
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	timer_rate = val;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		cpufreq_interactive_boost();
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	cpufreq_interactive_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static ssize_t show_sync_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sync_freq);
+}
+
+static ssize_t store_sync_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	sync_freq = val;
+	return count;
+}
+
+static struct global_attr sync_freq_attr = __ATTR(sync_freq, 0644,
+		show_sync_freq, store_sync_freq);
+
+static ssize_t show_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_load);
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_load = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_load_attr =
+		__ATTR(up_threshold_any_cpu_load, 0644,
+		show_up_threshold_any_cpu_load,
+				store_up_threshold_any_cpu_load);
+
+static ssize_t show_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", up_threshold_any_cpu_freq);
+}
+
+static ssize_t store_up_threshold_any_cpu_freq(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	up_threshold_any_cpu_freq = val;
+	return count;
+}
+
+static struct global_attr up_threshold_any_cpu_freq_attr =
+		__ATTR(up_threshold_any_cpu_freq, 0644,
+		show_up_threshold_any_cpu_freq,
+				store_up_threshold_any_cpu_freq);
+
+static struct attribute *interactive_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	&sampling_down_factor_attr.attr,
+	&sync_freq_attr.attr,
+	&up_threshold_any_cpu_load_attr.attr,
+	&up_threshold_any_cpu_freq_attr.attr,
+	&two_phase_freq_attr.attr,
+	NULL,
+};
+
+static void interactive_input_event(struct input_handle *handle,
+		unsigned int type,
+		unsigned int code, int value)
+{
+	if (type == EV_SYN && code == SYN_REPORT) {
+		boostpulse_endtime = ktime_to_us(ktime_get()) +
+			boostpulse_duration_val;
+		cpufreq_interactive_boost();
+	}
+}
+
+static int input_dev_filter(const char *input_dev_name)
+{
+	if (strstr(input_dev_name, "touchscreen") ||
+	    strstr(input_dev_name, "Atmel maXTouch Touchscreen") ||
+	    strstr(input_dev_name, "touch_dev") ||
+	    strstr(input_dev_name, "sec-touchscreen") ||
+	    strstr(input_dev_name, "keypad")) {
+		return 0;
+	} else {
+		return 1;
+	}
+}
+
+
+static int interactive_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	if (input_dev_filter(dev->name))
+		return -ENODEV;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void interactive_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id interactive_ids[] = {
+	{ .driver_info = 1 },
+	{ },
+};
+
+static struct input_handler interactive_input_handler = {
+	.event		= interactive_input_event,
+	.connect	= interactive_input_connect,
+	.disconnect	= interactive_input_disconnect,
+	.name		= "intelliactive",
+	.id_table	= interactive_ids,
+};
+
+static struct attribute_group interactive_attr_group = {
+	.attrs = interactive_attributes,
+	.name = "intelliactive",
+};
+
+static int cpufreq_interactive_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_interactive_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_interactive_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactive_idle_nb = {
+	.notifier_call = cpufreq_interactive_idle_notifier,
+};
+
+static int cpufreq_governor_intelliactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			cpufreq_interactive_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		if (!policy->cpu)
+			rc = input_register_handler
+				(&interactive_input_handler);
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		if (rc) {
+			mutex_unlock(&gov_lock);
+			return rc;
+		}
+
+		idle_notifier_register(&cpufreq_interactive_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		if (!policy->cpu)
+			input_unregister_handler(&interactive_input_handler);
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactive_idle_nb);
+		sysfs_remove_group(cpufreq_global_kobject,
+				&interactive_attr_group);
+		mutex_unlock(&gov_lock);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			else if (policy->min > pcpu->target_freq)
+				pcpu->target_freq = policy->min;
+
+			/* Reschedule timer.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_interactive_timer_start(j);
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+static void cpufreq_interactive_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_intelliactive_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactive_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactive_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactive_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactive_speedchange_task, NULL,
+			       "cfintelliactive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_intelliactive);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE
+fs_initcall(cpufreq_intelliactive_init);
+#else
+module_init(cpufreq_intelliactive_init);
+#endif
+
+static void __exit cpufreq_interactive_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_intelliactive);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactive_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_intelliactive' - A cpufreq governor for "
+	"Latency sensitive workloads based on Google's Interactive");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_intellidemand.c b/drivers/cpufreq/cpufreq_intellidemand.c
new file mode 100644
index 00000000000..101192c7d86
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_intellidemand.c
@@ -0,0 +1,2442 @@
+/*
+ *  drivers/cpufreq/cpufreq_intellidemand.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2013 The Linux Foundation. All rights reserved.
+ *            (C)  2013 Paul Reioux
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/kthread.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+
+#ifdef CONFIG_POWERSUSPEND
+#include <linux/powersuspend.h>
+#endif
+
+#define INTELLIDEMAND_MAJOR_VERSION    5
+#define INTELLIDEMAND_MINOR_VERSION    0
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_SAMPLING_RATE			(50000)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(10)
+#define DEF_FREQUENCY_UP_THRESHOLD		(80)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define MICRO_FREQUENCY_DOWN_DIFFERENTIAL	(3)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define MIN_FREQUENCY_DOWN_DIFFERENTIAL		(1)
+#define DBS_INPUT_EVENT_MIN_FREQ		(1190400)
+#define DEF_UI_DYNAMIC_SAMPLING_RATE		(30000)
+#define DBS_UI_SAMPLING_MIN_TIMEOUT		(30)
+#define DBS_UI_SAMPLING_MAX_TIMEOUT		(1000)
+#define DBS_UI_SAMPLING_TIMEOUT			(80)
+
+#define DEF_FREQ_STEP				(25)
+#define DEF_STEP_UP_EARLY_HISPEED		(1190400)
+#define DEF_STEP_UP_INTERIM_HISPEED		(1728000)
+#define DEF_SAMPLING_EARLY_HISPEED_FACTOR	(2)
+#define DEF_SAMPLING_INTERIM_HISPEED_FACTOR	(3)
+
+/* PATCH : SMART_UP */
+#define MIN(X, Y) ((X) < (Y) ? (X) : (Y))
+
+#define SMART_UP_PLUS (0)
+#define SMART_UP_SLOW_UP_AT_HIGH_FREQ (1)
+#define SUP_MAX_STEP (3)
+#define SUP_CORE_NUM (4)
+#define SUP_SLOW_UP_DUR (5)
+#define SUP_SLOW_UP_DUR_DEFAULT (2)
+
+#define SUP_HIGH_SLOW_UP_DUR (5)
+#define SUP_FREQ_LEVEL (14)
+
+#ifdef CONFIG_POWERSUSPEND
+static unsigned long stored_sampling_rate;
+#endif
+
+#if defined(SMART_UP_PLUS)
+static unsigned int SUP_THRESHOLD_STEPS[SUP_MAX_STEP] = {75, 85, 90};
+static unsigned int SUP_FREQ_STEPS[SUP_MAX_STEP] = {4, 3, 2};
+//static unsigned int min_range = 108000;
+typedef struct{
+	unsigned int freq_idx;
+	unsigned int freq_value;
+} freq_table_idx;
+static freq_table_idx pre_freq_idx[SUP_CORE_NUM] = {};
+
+#endif
+
+
+#if defined(SMART_UP_SLOW_UP_AT_HIGH_FREQ)
+
+#define SUP_SLOW_UP_FREQUENCY 		(1574400)
+#define SUP_HIGH_SLOW_UP_FREQUENCY 	(1728000)
+#define SUP_SLOW_UP_LOAD 		(90)
+
+typedef struct {
+	unsigned int hist_max_load[SUP_SLOW_UP_DUR];
+	unsigned int hist_load_cnt;
+} history_load;
+static void reset_hist(history_load *hist_load);
+static history_load hist_load[SUP_CORE_NUM] = {};
+
+typedef struct {
+	unsigned int hist_max_load[SUP_HIGH_SLOW_UP_DUR];
+	unsigned int hist_load_cnt;
+} history_load_high;
+static void reset_hist_high(history_load_high *hist_load);
+static history_load_high hist_load_high[SUP_CORE_NUM] = {};
+
+#endif
+
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+static unsigned int skip_intellidemand = 0;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(20)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+#define POWERSAVE_BIAS_MAXLEVEL			(1000)
+#define POWERSAVE_BIAS_MINLEVEL			(-1000)
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+static
+#endif
+struct cpufreq_governor cpufreq_gov_intellidemand = {
+       .name                   = "intellidemand",
+       .governor               = cpufreq_governor_dbs,
+       .max_transition_latency = TRANSITION_LATENCY_LIMIT,
+       .owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_iowait;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int freq_lo;
+	unsigned int freq_lo_jiffies;
+	unsigned int freq_hi_jiffies;
+	unsigned int rate_mult;
+	unsigned int prev_load;
+	unsigned int max_load;
+	int cpu;
+	unsigned int sample_type:1;
+	unsigned int freq_stay_count;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+
+	struct task_struct *sync_thread;
+	wait_queue_head_t sync_wq;
+	atomic_t src_sync_cpu;
+	atomic_t sync_enabled;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info);
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+static DEFINE_PER_CPU(struct task_struct *, up_task);
+static spinlock_t input_boost_lock;
+static bool input_event_boost = false;
+static unsigned long ui_sampling_expired = 0;
+
+/*
+ * dbs_mutex protects dbs_enable and dbs_info during start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct workqueue_struct *dbs_wq;
+
+struct dbs_work_struct {
+	struct work_struct work;
+	unsigned int cpu;
+};
+
+static DEFINE_PER_CPU(struct dbs_work_struct, dbs_refresh_work);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int up_threshold_multi_core;
+	unsigned int down_differential;
+	unsigned int down_differential_multi_core;
+	unsigned int optimal_freq;
+	unsigned int up_threshold_any_cpu_load;
+	unsigned int sync_freq;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	int          powersave_bias;
+	unsigned int io_is_busy;
+	//20130711 smart_up
+	unsigned int smart_up;
+	unsigned int smart_slow_up_load;
+	unsigned int smart_slow_up_freq;
+	unsigned int smart_slow_up_dur;
+	unsigned int smart_high_slow_up_freq;
+	unsigned int smart_high_slow_up_dur;
+	unsigned int smart_each_off;
+	// end smart_up
+	unsigned int freq_step;
+	unsigned int step_up_early_hispeed;
+	unsigned int step_up_interim_hispeed;
+	unsigned int sampling_early_factor;
+	unsigned int sampling_interim_factor;
+	unsigned int two_phase_freq;
+	unsigned int origin_sampling_rate;
+	unsigned int ui_sampling_rate;
+	unsigned int ui_timeout;
+	unsigned int enable_boost_cpu;
+
+} dbs_tuners_ins = {
+	.up_threshold_multi_core = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.down_differential_multi_core = MICRO_FREQUENCY_DOWN_DIFFERENTIAL,
+	.up_threshold_any_cpu_load = DEF_FREQUENCY_UP_THRESHOLD,
+	.ignore_nice = 0,
+	.powersave_bias = 0,
+	.sync_freq = 0,
+	.optimal_freq = 0,
+	//20130711 smart_up 
+	.smart_up = SMART_UP_PLUS,
+	.smart_slow_up_load = SUP_SLOW_UP_LOAD,
+	.smart_slow_up_freq = SUP_SLOW_UP_FREQUENCY,
+	.smart_slow_up_dur = SUP_SLOW_UP_DUR_DEFAULT,
+	.smart_high_slow_up_freq = SUP_HIGH_SLOW_UP_FREQUENCY,
+	.smart_high_slow_up_dur = SUP_HIGH_SLOW_UP_DUR,
+	.smart_each_off = 0,	
+	// end smart_up
+	.freq_step = DEF_FREQ_STEP,
+	.step_up_early_hispeed = DEF_STEP_UP_EARLY_HISPEED,
+	.step_up_interim_hispeed = DEF_STEP_UP_INTERIM_HISPEED,
+	.sampling_early_factor = DEF_SAMPLING_EARLY_HISPEED_FACTOR,
+	.sampling_interim_factor = DEF_SAMPLING_INTERIM_HISPEED_FACTOR,
+	.two_phase_freq = 0,
+	.ui_sampling_rate = DEF_UI_DYNAMIC_SAMPLING_RATE,
+	.ui_timeout = DBS_UI_SAMPLING_TIMEOUT,
+	.enable_boost_cpu = 1,
+
+};
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+static inline cputime64_t get_cpu_iowait_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/*
+ * Find right freq to be set now with powersave_bias on.
+ * Returns the freq_hi to be used right now and will set freq_hi_jiffies,
+ * freq_lo, and freq_lo_jiffies in percpu area for averaging freqs.
+ */
+static unsigned int powersave_bias_target(struct cpufreq_policy *policy,
+					  unsigned int freq_next,
+					  unsigned int relation)
+{
+	unsigned int freq_req, freq_avg;
+	unsigned int freq_hi, freq_lo;
+	unsigned int index = 0;
+	unsigned int jiffies_total, jiffies_hi, jiffies_lo;
+	int freq_reduc;
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info,
+						   policy->cpu);
+
+	if (!dbs_info->freq_table) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_next;
+	}
+
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_next,
+			relation, &index);
+	freq_req = dbs_info->freq_table[index].frequency;
+	freq_reduc = freq_req * dbs_tuners_ins.powersave_bias / 1000;
+	freq_avg = freq_req - freq_reduc;
+
+	/* Find freq bounds for freq_avg in freq_table */
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_H, &index);
+	freq_lo = dbs_info->freq_table[index].frequency;
+	index = 0;
+	cpufreq_frequency_table_target(policy, dbs_info->freq_table, freq_avg,
+			CPUFREQ_RELATION_L, &index);
+	freq_hi = dbs_info->freq_table[index].frequency;
+
+	/* Find out how long we have to be in hi and lo freqs */
+	if (freq_hi == freq_lo) {
+		dbs_info->freq_lo = 0;
+		dbs_info->freq_lo_jiffies = 0;
+		return freq_lo;
+	}
+	jiffies_total = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	jiffies_hi = (freq_avg - freq_lo) * jiffies_total;
+	jiffies_hi += ((freq_hi - freq_lo) / 2);
+	jiffies_hi /= (freq_hi - freq_lo);
+	jiffies_lo = jiffies_total - jiffies_hi;
+	dbs_info->freq_lo = freq_lo;
+	dbs_info->freq_lo_jiffies = jiffies_lo;
+	dbs_info->freq_hi_jiffies = jiffies_hi;
+	return freq_hi;
+}
+
+static int intellidemand_powersave_bias_setspeed(struct cpufreq_policy *policy,
+					    struct cpufreq_policy *altpolicy,
+					    int level)
+{
+	if (level == POWERSAVE_BIAS_MAXLEVEL) {
+		/* maximum powersave; set to lowest frequency */
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->min : policy->min,
+			CPUFREQ_RELATION_L);
+		return 1;
+	} else if (level == POWERSAVE_BIAS_MINLEVEL) {
+		/* minimum powersave; set to highest frequency */
+		__cpufreq_driver_target(policy,
+			(altpolicy) ? altpolicy->max : policy->max,
+			CPUFREQ_RELATION_H);
+		return 1;
+	}
+	return 0;
+}
+
+static void intellidemand_powersave_bias_init_cpu(int cpu)
+{
+	struct cpu_dbs_info_s *dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+	dbs_info->freq_table = cpufreq_frequency_get_table(cpu);
+	dbs_info->freq_lo = 0;
+}
+
+static void intellidemand_powersave_bias_init(void)
+{
+	int i;
+	for_each_online_cpu(i) {
+		intellidemand_powersave_bias_init_cpu(i);
+	}
+}
+
+void intellidemand_boost_cpu(int boost)
+{
+	int cpu;
+
+	if (!dbs_tuners_ins.enable_boost_cpu)
+		return;
+
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpu_dbs_info_s *dbs_info;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy)
+			continue;
+		dbs_info = &per_cpu(od_cpu_dbs_info, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		mutex_lock(&dbs_info->timer_mutex);
+		if (boost) {
+			skip_intellidemand = 1;
+			__cpufreq_driver_target(policy, policy->max, CPUFREQ_RELATION_H);
+		} else {
+			skip_intellidemand = 0;
+		}
+		mutex_unlock(&dbs_info->timer_mutex);
+	}
+}
+EXPORT_SYMBOL(intellidemand_boost_cpu);
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_intellidemand Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_multi_core, up_threshold_multi_core);
+show_one(down_differential, down_differential);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(optimal_freq, optimal_freq);
+show_one(up_threshold_any_cpu_load, up_threshold_any_cpu_load);
+show_one(sync_freq, sync_freq);
+//20130711 smart_up 
+show_one(smart_up, smart_up);
+show_one(smart_slow_up_load, smart_slow_up_load);
+show_one(smart_slow_up_freq, smart_slow_up_freq);
+show_one(smart_slow_up_dur, smart_slow_up_dur);
+show_one(smart_high_slow_up_freq, smart_high_slow_up_freq);
+show_one(smart_high_slow_up_dur, smart_high_slow_up_dur);
+show_one(smart_each_off, smart_each_off);
+// end smart_up
+show_one(freq_step, freq_step);
+show_one(step_up_early_hispeed, step_up_early_hispeed);
+show_one(step_up_interim_hispeed, step_up_interim_hispeed);
+show_one(sampling_early_factor, sampling_early_factor);
+show_one(sampling_interim_factor, sampling_interim_factor);
+show_one(enable_boost_cpu, enable_boost_cpu)
+
+static ssize_t show_powersave_bias
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", dbs_tuners_ins.powersave_bias);
+}
+
+/**
+ * update_sampling_rate - update sampling rate effective immediately if needed.
+ * @new_rate: new sampling rate
+ *
+ * If new rate is smaller than the old, simply updaing
+ * dbs_tuners_int.sampling_rate might not be appropriate. For example,
+ * if the original sampling_rate was 1 second and the requested new sampling
+ * rate is 10 ms because the user needs immediate reaction from intellidemand
+ * governor, but not sure if higher frequency will be required or not,
+ * then, the governor may change the sampling rate too late; up to 1 second
+ * later. Thus, if we are reducing the sampling rate, we need to make the
+ * new value effective immediately.
+ */
+static void update_sampling_rate(unsigned int new_rate)
+{
+	int cpu;
+
+	dbs_tuners_ins.sampling_rate = new_rate
+				     = max(new_rate, min_sampling_rate);
+
+	get_online_cpus();
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpu_dbs_info_s *dbs_info;
+		unsigned long next_sampling, appointed_at;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy)
+			continue;
+		dbs_info = &per_cpu(od_cpu_dbs_info, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		mutex_lock(&dbs_info->timer_mutex);
+
+		if (!delayed_work_pending(&dbs_info->work)) {
+			mutex_unlock(&dbs_info->timer_mutex);
+			continue;
+		}
+
+		next_sampling  = jiffies + usecs_to_jiffies(new_rate);
+		appointed_at = dbs_info->work.timer.expires;
+
+
+		if (time_before(next_sampling, appointed_at)) {
+
+			mutex_unlock(&dbs_info->timer_mutex);
+			cancel_delayed_work_sync(&dbs_info->work);
+			mutex_lock(&dbs_info->timer_mutex);
+
+			queue_delayed_work_on(dbs_info->cpu, dbs_wq,
+				&dbs_info->work, usecs_to_jiffies(new_rate));
+
+		}
+		mutex_unlock(&dbs_info->timer_mutex);
+	}
+	put_online_cpus();
+}
+
+show_one(ui_timeout, ui_timeout);
+
+static ssize_t store_ui_timeout(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input, (unsigned int)DBS_UI_SAMPLING_MIN_TIMEOUT);
+	dbs_tuners_ins.ui_timeout = min(input, (unsigned int)DBS_UI_SAMPLING_MAX_TIMEOUT);
+
+	return count;
+}
+
+static int two_phase_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = 0} ;
+
+static ssize_t show_two_phase_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",two_phase_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_two_phase_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&two_phase_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&two_phase_freq_array[0],
+				&two_phase_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &two_phase_freq_array[0],
+				&two_phase_freq_array[1],
+				&two_phase_freq_array[2],
+				&two_phase_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+static int input_event_min_freq_array[NR_CPUS] = {[0 ... NR_CPUS-1] = DBS_INPUT_EVENT_MIN_FREQ} ;
+
+static ssize_t show_input_event_min_freq
+(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i = 0 ;
+	int shift = 0 ;
+	char *buf_pos = buf;
+	for ( i = 0 ; i < NR_CPUS; i++) {
+		shift = sprintf(buf_pos,"%d,",input_event_min_freq_array[i]);
+		buf_pos += shift;
+	}
+	*(buf_pos-1) = '\0';
+	return strlen(buf);
+}
+
+static ssize_t store_input_event_min_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+
+	int ret = 0;
+	if (NR_CPUS == 1)
+		ret = sscanf(buf,"%u",&input_event_min_freq_array[0]);
+	else if (NR_CPUS == 2)
+		ret = sscanf(buf,"%u,%u",&input_event_min_freq_array[0],
+				&input_event_min_freq_array[1]);
+	else if (NR_CPUS == 4)
+		ret = sscanf(buf, "%u,%u,%u,%u", &input_event_min_freq_array[0],
+				&input_event_min_freq_array[1],
+				&input_event_min_freq_array[2],
+				&input_event_min_freq_array[3]);
+	if (ret < NR_CPUS)
+		return -EINVAL;
+
+	return count;
+}
+
+show_one(ui_sampling_rate, ui_sampling_rate);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input == dbs_tuners_ins.origin_sampling_rate)
+		return count;
+	update_sampling_rate(input);
+	dbs_tuners_ins.origin_sampling_rate = dbs_tuners_ins.sampling_rate;
+	return count;
+}
+
+static ssize_t store_ui_sampling_rate(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.ui_sampling_rate = max(input, min_sampling_rate);
+
+	return count;
+}
+
+static ssize_t store_sync_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.sync_freq = input;
+	return count;
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.io_is_busy = !!input;
+	return count;
+}
+
+static ssize_t store_optimal_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.optimal_freq = input;
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_multi_core(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_multi_core = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_any_cpu_load(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_any_cpu_load = input;
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input >= dbs_tuners_ins.up_threshold ||
+			input < MIN_FREQUENCY_DOWN_DIFFERENTIAL) {
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.down_differential = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) { /* nothing to do */
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+	}
+	return count;
+}
+
+static ssize_t store_powersave_bias(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	int input  = 0;
+	int bypass = 0;
+	int ret, cpu, reenable_timer, j;
+	struct cpu_dbs_info_s *dbs_info;
+
+	struct cpumask cpus_timer_done;
+	cpumask_clear(&cpus_timer_done);
+
+	ret = sscanf(buf, "%d", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input >= POWERSAVE_BIAS_MAXLEVEL) {
+		input  = POWERSAVE_BIAS_MAXLEVEL;
+		bypass = 1;
+	} else if (input <= POWERSAVE_BIAS_MINLEVEL) {
+		input  = POWERSAVE_BIAS_MINLEVEL;
+		bypass = 1;
+	}
+
+	if (input == dbs_tuners_ins.powersave_bias) {
+		/* no change */
+		return count;
+	}
+
+	reenable_timer = ((dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MAXLEVEL) ||
+				(dbs_tuners_ins.powersave_bias ==
+				POWERSAVE_BIAS_MINLEVEL));
+
+	dbs_tuners_ins.powersave_bias = input;
+
+	get_online_cpus();
+	mutex_lock(&dbs_mutex);
+
+	if (!bypass) {
+		if (reenable_timer) {
+			/* reinstate dbs timer */
+			for_each_online_cpu(cpu) {
+				if (lock_policy_rwsem_write(cpu) < 0)
+					continue;
+
+				dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+				for_each_cpu(j, &cpus_timer_done) {
+					if (!dbs_info->cur_policy) {
+						pr_err("Dbs policy is NULL\n");
+						goto skip_this_cpu;
+					}
+					if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+						goto skip_this_cpu;
+				}
+
+				cpumask_set_cpu(cpu, &cpus_timer_done);
+				if (dbs_info->cur_policy) {
+					/* restart dbs timer */
+					dbs_timer_init(dbs_info);
+					/* Enable frequency synchronization
+					 * of CPUs */
+					atomic_set(&dbs_info->sync_enabled, 1);
+				}
+skip_this_cpu:
+				unlock_policy_rwsem_write(cpu);
+			}
+		}
+		intellidemand_powersave_bias_init();
+	} else {
+		/* running at maximum or minimum frequencies; cancel
+		   dbs timer as periodic load sampling is not necessary */
+		for_each_online_cpu(cpu) {
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+			for_each_cpu(j, &cpus_timer_done) {
+				if (!dbs_info->cur_policy) {
+					pr_err("Dbs policy is NULL\n");
+					goto skip_this_cpu_bypass;
+				}
+				if (cpumask_test_cpu(j, dbs_info->
+							cur_policy->cpus))
+					goto skip_this_cpu_bypass;
+			}
+
+			cpumask_set_cpu(cpu, &cpus_timer_done);
+
+			if (dbs_info->cur_policy) {
+				/* cpu using intellidemand, cancel dbs timer */
+				dbs_timer_exit(dbs_info);
+				/* Disable frequency synchronization of
+				 * CPUs to avoid re-queueing of work from
+				 * sync_thread */
+				atomic_set(&dbs_info->sync_enabled, 0);
+
+				mutex_lock(&dbs_info->timer_mutex);
+				intellidemand_powersave_bias_setspeed(
+					dbs_info->cur_policy,
+					NULL,
+					input);
+				mutex_unlock(&dbs_info->timer_mutex);
+
+			}
+skip_this_cpu_bypass:
+			unlock_policy_rwsem_write(cpu);
+		}
+	}
+
+	mutex_unlock(&dbs_mutex);
+	put_online_cpus();
+
+	return count;
+}
+
+static ssize_t store_enable_boost_cpu(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if(ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.enable_boost_cpu = (input > 0 ? input : 0);
+	return count;
+}
+
+/* PATCH : SMART_UP */
+#if defined(SMART_UP_SLOW_UP_AT_HIGH_FREQ)
+static void reset_hist(history_load *hist_load)
+{	int i;
+
+	for (i = 0; i < SUP_SLOW_UP_DUR ; i++)
+		hist_load->hist_max_load[i] = 0;
+
+	hist_load->hist_load_cnt = 0;
+}
+
+
+static void reset_hist_high(history_load_high *hist_load)
+{	int i;
+
+	for (i = 0; i < SUP_HIGH_SLOW_UP_DUR ; i++)
+		hist_load->hist_max_load[i] = 0;
+
+	hist_load->hist_load_cnt = 0;
+}
+
+#endif
+
+//20130711 smart_up
+static ssize_t store_smart_up(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > 1 ){
+		input = 1;
+	}else if (input < 0 ){
+		input = 0;
+	}
+	
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_up = input;
+	return count;
+}
+
+static ssize_t store_smart_slow_up_load(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > 100 ){
+		input = 100;
+	}else if (input < 0){
+		input = 0;
+	}
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_load = input;
+	return count;
+}
+static ssize_t store_smart_slow_up_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input < 0)
+		input = 0;
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_freq = input;
+	return count;
+}
+static ssize_t store_smart_slow_up_dur(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > SUP_SLOW_UP_DUR ){
+		input = SUP_SLOW_UP_DUR;
+	}else if (input < 1 ){
+		input = 1;
+	}
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_slow_up_dur = input;
+	return count;
+}
+static ssize_t store_smart_high_slow_up_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input < 0)
+		input = 0;
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_high_slow_up_freq = input;
+	return count;
+}
+static ssize_t store_smart_high_slow_up_dur(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input > SUP_HIGH_SLOW_UP_DUR ){
+		input = SUP_HIGH_SLOW_UP_DUR;
+	}else if (input < 1 ){
+		input = 1;
+	}
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_high_slow_up_dur = input;
+	return count;
+}
+static ssize_t store_smart_each_off(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int i;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	if (input >  SUP_CORE_NUM){
+		input = SUP_CORE_NUM;
+	}else if ( input < 0){
+		input = 0;
+	}
+	// buffer reset
+	for_each_online_cpu(i){
+		reset_hist(&hist_load[i]);
+		reset_hist_high(&hist_load_high[i]);
+	}
+	dbs_tuners_ins.smart_each_off = input;
+
+	return count;
+}
+//end  smart_up
+
+static ssize_t store_freq_step(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+static ssize_t store_step_up_early_hispeed(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 2265600 ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.step_up_early_hispeed = input;
+	return count;
+}
+
+static ssize_t store_step_up_interim_hispeed(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 2265600 ||
+			input < 0) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.step_up_interim_hispeed = input;
+	return count;
+}
+
+static ssize_t store_sampling_early_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_early_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_interim_factor(struct kobject *a,
+			struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 1)
+		return -EINVAL;
+	dbs_tuners_ins.sampling_interim_factor = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_differential);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(powersave_bias);
+define_one_global_rw(up_threshold_multi_core);
+define_one_global_rw(optimal_freq);
+define_one_global_rw(up_threshold_any_cpu_load);
+define_one_global_rw(sync_freq);
+//20130711 smart_up
+define_one_global_rw(smart_up);
+define_one_global_rw(smart_slow_up_load);
+define_one_global_rw(smart_slow_up_freq);
+define_one_global_rw(smart_slow_up_dur);
+define_one_global_rw(smart_high_slow_up_freq);
+define_one_global_rw(smart_high_slow_up_dur);
+define_one_global_rw(smart_each_off);
+// end smart_up
+define_one_global_rw(freq_step);
+define_one_global_rw(step_up_early_hispeed);
+define_one_global_rw(step_up_interim_hispeed);
+define_one_global_rw(sampling_early_factor);
+define_one_global_rw(sampling_interim_factor);
+define_one_global_rw(two_phase_freq);
+define_one_global_rw(input_event_min_freq);
+define_one_global_rw(ui_sampling_rate);
+define_one_global_rw(ui_timeout);
+define_one_global_rw(enable_boost_cpu);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&down_differential.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&powersave_bias.attr,
+	&io_is_busy.attr,
+	&up_threshold_multi_core.attr,
+	&optimal_freq.attr,
+	&up_threshold_any_cpu_load.attr,
+	&sync_freq.attr,
+	//20130711 smart_up
+	&smart_up.attr,
+	&smart_slow_up_load.attr,
+	&smart_slow_up_freq.attr,
+	&smart_slow_up_dur.attr,
+	&smart_high_slow_up_freq.attr,
+	&smart_high_slow_up_dur.attr,
+	&smart_each_off.attr,
+	// end smart_up
+	&freq_step.attr,
+	&step_up_early_hispeed.attr,
+	&step_up_interim_hispeed.attr,
+	&sampling_early_factor.attr,
+	&sampling_interim_factor.attr,
+	&two_phase_freq.attr,
+	&input_event_min_freq.attr,
+	&ui_sampling_rate.attr,
+	&ui_timeout.attr,
+	&enable_boost_cpu.attr,
+
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "intellidemand",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+	if (dbs_tuners_ins.powersave_bias)
+		freq = powersave_bias_target(p, freq, CPUFREQ_RELATION_H);
+	else if (p->cur == p->max)
+		return;
+
+	__cpufreq_driver_target(p, freq, dbs_tuners_ins.powersave_bias ?
+			CPUFREQ_RELATION_L : CPUFREQ_RELATION_H);
+}
+
+int set_two_phase_freq(int cpufreq)
+{
+	int i  = 0;
+	for ( i = 0 ; i < NR_CPUS; i++)
+		two_phase_freq_array[i] = cpufreq;
+	return 0;
+}
+
+void set_two_phase_freq_by_cpu ( int cpu_nr, int cpufreq){
+	two_phase_freq_array[cpu_nr-1] = cpufreq;
+}
+
+int input_event_boosted(void)
+{
+	unsigned long flags;
+
+	
+	spin_lock_irqsave(&input_boost_lock, flags);
+	if (input_event_boost) {
+		if (time_before(jiffies, ui_sampling_expired)) {
+			spin_unlock_irqrestore(&input_boost_lock, flags);
+			return 1;
+		}
+		input_event_boost = false;
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.origin_sampling_rate;
+	}
+	spin_unlock_irqrestore(&input_boost_lock, flags);
+
+	return 0;
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+
+#if defined(SMART_UP_PLUS)
+	unsigned int max_load = 0;
+	unsigned int core_j = 0;
+#endif
+
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int max_load_freq;
+	/* Current load across this CPU */
+	unsigned int cur_load = 0;
+	unsigned int max_load_other_cpu = 0;
+	struct cpufreq_policy *policy;
+	unsigned int j;
+	static unsigned int phase = 0;
+	static unsigned int counter = 0;
+	unsigned int nr_cpus;
+
+	this_dbs_info->freq_lo = 0;
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate, we look for a the lowest
+	 * frequency which can sustain the load while keeping idle time over
+	 * 30%. If such a frequency exist, we try to decrease to this frequency.
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of current frequency
+	 */
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load_freq;
+		int freq_avg;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+			(cur_iowait_time - j_dbs_info->prev_cpu_iowait);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		/*
+		 * For the purpose of intellidemand, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+
+		if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+		j_dbs_info->max_load  = max(cur_load, j_dbs_info->prev_load);
+		j_dbs_info->prev_load = cur_load;
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = cur_load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+
+#if defined(SMART_UP_PLUS)
+		max_load = cur_load;
+		core_j = j;
+#endif
+
+	}
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+
+		if (j == policy->cpu)
+			continue;
+
+		if (max_load_other_cpu < j_dbs_info->max_load)
+			max_load_other_cpu = j_dbs_info->max_load;
+		/*
+		 * The other cpu could be running at higher frequency
+		 * but may not have completed it's sampling_down_factor.
+		 * For that case consider other cpu is loaded so that
+		 * frequency imbalance does not occur.
+		 */
+
+		if ((j_dbs_info->cur_policy != NULL)
+			&& (j_dbs_info->cur_policy->cur ==
+					j_dbs_info->cur_policy->max)) {
+
+			if (policy->cur >= dbs_tuners_ins.optimal_freq)
+				max_load_other_cpu =
+				dbs_tuners_ins.up_threshold_any_cpu_load;
+		}
+	}
+
+	/* calculate the scaled load across CPU */
+	load_at_max_freq = (cur_load * policy->cur)/policy->cpuinfo.max_freq;
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+/* PATCH : SMART_UP */
+	if (dbs_tuners_ins.smart_up && ( core_j + 1 ) > dbs_tuners_ins.smart_each_off ){
+
+	if (max_load_freq > SUP_THRESHOLD_STEPS[0] * policy->cur) {
+		int smart_up_inc =
+			(policy->max - policy->cur) / SUP_FREQ_STEPS[0];
+		int freq_next = 0;
+		int i = 0;
+		
+		//20130429 UPDATE
+		int check_idx =  0;
+		int check_freq = 0; 
+		int temp_up_inc =0;
+
+		if (counter < 5) {
+			counter++;
+			if (counter > 2) {				
+				phase = 1;
+			}
+		}
+		
+		nr_cpus = num_online_cpus();
+		dbs_tuners_ins.two_phase_freq = two_phase_freq_array[nr_cpus-1];
+		if (dbs_tuners_ins.two_phase_freq < policy->cur)
+			phase = 1;
+		if (dbs_tuners_ins.two_phase_freq != 0 && phase == 0) {			
+			dbs_freq_increase(policy, dbs_tuners_ins.two_phase_freq);
+		} else {			
+			if (policy->cur < policy->max)
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+			dbs_freq_increase(policy, policy->max);
+		}
+
+		for (i = (SUP_MAX_STEP - 1); i > 0; i--) {
+			if (max_load_freq > SUP_THRESHOLD_STEPS[i]
+							* policy->cur) {
+				smart_up_inc = (policy->max - policy->cur)
+						/ SUP_FREQ_STEPS[i];
+			
+				break;
+			}
+		}
+		
+		//20130429 UPDATE
+		check_idx =  pre_freq_idx[core_j].freq_idx;
+		check_freq = pre_freq_idx[core_j].freq_value; 
+		if ( ( check_idx == 0) 
+		|| (this_dbs_info->freq_table[check_idx].frequency 
+			!=  policy->cur) )
+		{
+			int i = 0;
+			for( i =0; i < SUP_FREQ_LEVEL; i ++)
+			{
+				if (this_dbs_info->freq_table[i].frequency == policy->cur)
+				{
+					
+					pre_freq_idx[core_j].freq_idx = i;
+					pre_freq_idx[core_j].freq_value = policy->cur;
+					check_idx =  i;
+					check_freq = policy->cur; 
+					break;
+				}
+			}
+			
+		}
+		if( check_idx < SUP_FREQ_LEVEL-1 ){ 
+		temp_up_inc =  
+			this_dbs_info->freq_table[check_idx + 1].frequency 
+			- check_freq;
+		}
+			
+		if (smart_up_inc < temp_up_inc )
+			smart_up_inc = temp_up_inc;
+
+		freq_next = MIN((policy->cur + smart_up_inc), policy->max);
+
+			
+			if (policy->cur >= dbs_tuners_ins.smart_high_slow_up_freq){
+			int idx = hist_load_high[core_j].hist_load_cnt;
+			int avg_hist_load = 0;
+			
+				if (idx >= dbs_tuners_ins.smart_high_slow_up_dur)
+				idx = 0;
+				
+			hist_load_high[core_j].hist_max_load[idx] = max_load;
+			hist_load_high[core_j].hist_load_cnt = idx + 1;
+
+			/* note : check history_load and get_sum_hist_load */
+			if (hist_load_high[core_j].
+					hist_max_load[dbs_tuners_ins.smart_high_slow_up_dur - 1] > 0) {
+				int sum_hist_load_freq = 0;
+				int i = 0;
+					for (i = 0; i < dbs_tuners_ins.smart_high_slow_up_dur; i++)
+					sum_hist_load_freq +=
+					hist_load_high[core_j].hist_max_load[i];
+
+				avg_hist_load = sum_hist_load_freq
+							/dbs_tuners_ins.smart_high_slow_up_dur;
+						
+					if (avg_hist_load > dbs_tuners_ins.smart_slow_up_load){
+					reset_hist_high(&hist_load_high[core_j]);
+					freq_next = MIN((policy->cur + temp_up_inc), policy->max);
+				} else
+					freq_next = policy->cur;
+			} else {
+				freq_next = policy->cur;
+			}
+			
+			} else if (policy->cur >= dbs_tuners_ins.smart_slow_up_freq ) {
+			int idx = hist_load[core_j].hist_load_cnt;
+			int avg_hist_load = 0;
+
+				if (idx >= dbs_tuners_ins.smart_slow_up_dur)
+				idx = 0;
+
+			hist_load[core_j].hist_max_load[idx] = max_load;
+			hist_load[core_j].hist_load_cnt = idx + 1;
+
+			/* note : check history_load and get_sum_hist_load */
+			if (hist_load[core_j].
+					hist_max_load[dbs_tuners_ins.smart_slow_up_dur - 1] > 0) {
+				int sum_hist_load_freq = 0;
+				int i = 0;
+					for (i = 0; i < dbs_tuners_ins.smart_slow_up_dur; i++)
+					sum_hist_load_freq +=
+					hist_load[core_j].hist_max_load[i];
+
+				avg_hist_load = sum_hist_load_freq
+							/ dbs_tuners_ins.smart_slow_up_dur ;
+
+					if (avg_hist_load > dbs_tuners_ins.smart_slow_up_load){
+					reset_hist(&hist_load[core_j]);
+					freq_next = MIN((policy->cur + temp_up_inc), policy->max);
+				} else
+					freq_next = policy->cur;
+			} else {
+				freq_next = policy->cur;
+			}
+		} else {
+			reset_hist(&hist_load[core_j]);
+		}
+		if (freq_next == policy->max)
+			this_dbs_info->rate_mult =
+				dbs_tuners_ins.sampling_down_factor;
+
+		dbs_freq_increase(policy, freq_next);
+
+		return;
+	}
+	}else{
+	/* Check for frequency increase */
+	if (max_load_freq > dbs_tuners_ins.up_threshold * policy->cur) {
+			int target;
+			int inc;
+
+			if (policy->cur < dbs_tuners_ins.step_up_early_hispeed) {
+				target = dbs_tuners_ins.step_up_early_hispeed;
+			}else if (policy->cur < dbs_tuners_ins.step_up_interim_hispeed) {
+				if(policy->cur == dbs_tuners_ins.step_up_early_hispeed) {
+					if(this_dbs_info->freq_stay_count <
+						dbs_tuners_ins.sampling_early_factor) {
+						this_dbs_info->freq_stay_count++;
+						return;
+					}
+				}
+				this_dbs_info->freq_stay_count = 1;
+				inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+				target = min(dbs_tuners_ins.step_up_interim_hispeed,
+					policy->cur + inc);
+			}else {
+				if(policy->cur == dbs_tuners_ins.step_up_interim_hispeed) {
+					if(this_dbs_info->freq_stay_count <
+						dbs_tuners_ins.sampling_interim_factor) {
+						this_dbs_info->freq_stay_count++;
+						return;
+					}
+				}
+				this_dbs_info->freq_stay_count = 1;
+				target = policy->max;
+
+				//int inc = (policy->max * dbs_tuners_ins.freq_step) / 100;
+				//target = min(policy->max, policy->cur + inc);
+			}
+
+			pr_debug("%s: cpu=%d, cur=%d, target=%d\n",
+				__func__, policy->cpu, policy->cur, target);
+
+			/* If switching to max speed, apply sampling_down_factor */
+			if (target == policy->max)
+				this_dbs_info->rate_mult =
+					dbs_tuners_ins.sampling_down_factor;
+
+			dbs_freq_increase(policy, target);
+		return;
+	}
+	}
+	if (counter > 0) {
+		counter--;
+		if (counter == 0) {						
+			phase = 0;
+		}
+	}
+
+	if (num_online_cpus() > 1) {
+
+		if (max_load_other_cpu >
+				dbs_tuners_ins.up_threshold_any_cpu_load) {
+			if (policy->cur < dbs_tuners_ins.sync_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.sync_freq);
+			return;
+		}
+
+		if (max_load_freq > dbs_tuners_ins.up_threshold_multi_core *
+								policy->cur) {
+			if (policy->cur < dbs_tuners_ins.optimal_freq)
+				dbs_freq_increase(policy,
+						dbs_tuners_ins.optimal_freq);
+			return;
+		}
+	}
+
+	if (input_event_boosted()) {
+		return;
+	}
+
+	/* Check for frequency decrease */
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+	     policy->cur) {
+		unsigned int freq_next;
+		freq_next = max_load_freq /
+				(dbs_tuners_ins.up_threshold -
+				 dbs_tuners_ins.down_differential);
+
+		pr_debug("%s: cpu=%d, cur=%d, target=%d (down)\n",
+			__func__, policy->cpu, policy->cur, freq_next);
+
+/* PATCH : SMART_UP */
+		if (dbs_tuners_ins.smart_up && ( core_j + 1 ) > dbs_tuners_ins.smart_each_off ){
+
+			if (freq_next >= dbs_tuners_ins.smart_high_slow_up_freq){
+			int idx = hist_load_high[core_j].hist_load_cnt;
+
+				if (idx >= dbs_tuners_ins.smart_high_slow_up_dur )
+				idx = 0;
+
+			hist_load_high[core_j].hist_max_load[idx] = max_load;
+			hist_load_high[core_j].hist_load_cnt = idx + 1;
+
+		
+			}else if (freq_next >= dbs_tuners_ins.smart_slow_up_freq) {
+			int idx = hist_load[core_j].hist_load_cnt;
+
+				if (idx >= dbs_tuners_ins.smart_slow_up_dur)
+				idx = 0;
+
+			hist_load[core_j].hist_max_load[idx] = max_load;
+			hist_load[core_j].hist_load_cnt = idx + 1;
+
+			reset_hist_high(&hist_load_high[core_j]);
+
+		
+			} else if (policy->cur >= dbs_tuners_ins.smart_slow_up_freq) {
+			reset_hist(&hist_load[core_j]);
+			reset_hist_high(&hist_load_high[core_j]);
+
+				
+			}
+		}
+//#endif
+
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+		this_dbs_info->freq_stay_count = 1;
+
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		if (num_online_cpus() > 1) {
+			if (max_load_other_cpu >
+			(dbs_tuners_ins.up_threshold_multi_core -
+			dbs_tuners_ins.down_differential) &&
+			freq_next < dbs_tuners_ins.sync_freq)
+				freq_next = dbs_tuners_ins.sync_freq;
+
+			if (max_load_freq >
+				 ((dbs_tuners_ins.up_threshold_multi_core -
+				  dbs_tuners_ins.down_differential_multi_core) *
+				  policy->cur) &&
+				freq_next < dbs_tuners_ins.optimal_freq)
+				freq_next = dbs_tuners_ins.optimal_freq;
+
+		}
+		if (!dbs_tuners_ins.powersave_bias) {
+			__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+		} else {
+			int freq = powersave_bias_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+			__cpufreq_driver_target(policy, freq,
+				CPUFREQ_RELATION_L);
+		}
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int sample_type = dbs_info->sample_type;
+	int delay = msecs_to_jiffies(50);
+
+	if (skip_intellidemand)
+		goto sched_wait;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	/* Common NORMAL_SAMPLE setup */
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	if (!dbs_tuners_ins.powersave_bias ||
+	    sample_type == DBS_NORMAL_SAMPLE) {
+		dbs_check_cpu(dbs_info);
+		if (dbs_info->freq_lo) {
+			/* Setup timer for SUB_SAMPLE */
+			dbs_info->sample_type = DBS_SUB_SAMPLE;
+			delay = dbs_info->freq_hi_jiffies;
+		} else {
+			/* We want all CPUs to do sampling nearly on
+			 * same jiffy
+			 */
+			delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				* dbs_info->rate_mult);
+
+			if (num_online_cpus() > 1)
+				delay -= jiffies % delay;
+		}
+	} else {
+		if (input_event_boosted())
+			goto sched_wait;
+
+		__cpufreq_driver_target(dbs_info->cur_policy,
+			dbs_info->freq_lo, CPUFREQ_RELATION_H);
+		delay = dbs_info->freq_lo_jiffies;
+	}
+sched_wait:
+	queue_delayed_work_on(cpu, dbs_wq, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	dbs_info->sample_type = DBS_NORMAL_SAMPLE;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	queue_delayed_work_on(dbs_info->cpu, dbs_wq, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+/*
+ * Not all CPUs want IO time to be accounted as busy; this dependson how
+ * efficient idling at a higher frequency/voltage is.
+ * Pavel Machek says this is not so for various generations of AMD and old
+ * Intel systems.
+ * Mike Chan (androidlcom) calis this is also not true for ARM.
+ * Because of this, whitelist specific known (series) of CPUs by default, and
+ * leave all others up to the user.
+ */
+static int should_io_be_busy(void)
+{
+#if defined(CONFIG_X86)
+	/*
+	 * For Intel, Core 2 (model 15) andl later have an efficient idle.
+	 */
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
+	    boot_cpu_data.x86 == 6 &&
+	    boot_cpu_data.x86_model >= 15)
+		return 1;
+#endif
+	return 0;
+}
+
+static void dbs_refresh_callback(struct work_struct *work)
+{
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *this_dbs_info;
+	struct dbs_work_struct *dbs_work;
+	unsigned int cpu;
+
+	dbs_work = container_of(work, struct dbs_work_struct, work);
+	cpu = dbs_work->cpu;
+
+	get_online_cpus();
+
+	if (lock_policy_rwsem_write(cpu) < 0)
+		goto bail_acq_sema_failed;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+	policy = this_dbs_info->cur_policy;
+	if (!policy) {
+		/* CPU not using intellidemand governor */
+		goto bail_incorrect_governor;
+	}
+
+	if (policy->cur < policy->max) {
+		/*
+		 * Arch specific cpufreq driver may fail.
+		 * Don't update governor frequency upon failure.
+		 */
+		if (__cpufreq_driver_target(policy, policy->max,
+					CPUFREQ_RELATION_L) >= 0)
+			policy->cur = policy->max;
+
+		this_dbs_info->prev_cpu_idle = get_cpu_idle_time(cpu,
+				&this_dbs_info->prev_cpu_wall);
+	}
+
+bail_incorrect_governor:
+	unlock_policy_rwsem_write(cpu);
+
+bail_acq_sema_failed:
+	put_online_cpus();
+	return;
+}
+
+static int dbs_migration_notify(struct notifier_block *nb,
+				unsigned long target_cpu, void *arg)
+{
+	struct cpu_dbs_info_s *target_dbs_info =
+		&per_cpu(od_cpu_dbs_info, target_cpu);
+
+	atomic_set(&target_dbs_info->src_sync_cpu, (int)arg);
+	wake_up(&target_dbs_info->sync_wq);
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block dbs_migration_nb = {
+	.notifier_call = dbs_migration_notify,
+};
+
+static int sync_pending(struct cpu_dbs_info_s *this_dbs_info)
+{
+	return atomic_read(&this_dbs_info->src_sync_cpu) >= 0;
+}
+
+static int dbs_sync_thread(void *data)
+{
+	int src_cpu, cpu = (int)data;
+	unsigned int src_freq, src_max_load;
+	struct cpu_dbs_info_s *this_dbs_info, *src_dbs_info;
+	struct cpufreq_policy *policy;
+	int delay;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	while (1) {
+		wait_event(this_dbs_info->sync_wq,
+			   sync_pending(this_dbs_info) ||
+			   kthread_should_stop());
+
+		if (kthread_should_stop())
+			break;
+
+		get_online_cpus();
+
+		src_cpu = atomic_read(&this_dbs_info->src_sync_cpu);
+		src_dbs_info = &per_cpu(od_cpu_dbs_info, src_cpu);
+		if (src_dbs_info != NULL &&
+		    src_dbs_info->cur_policy != NULL) {
+			src_freq = src_dbs_info->cur_policy->cur;
+			src_max_load = src_dbs_info->max_load;
+		} else {
+			src_freq = dbs_tuners_ins.sync_freq;
+			src_max_load = 0;
+		}
+
+		if (lock_policy_rwsem_write(cpu) < 0)
+			goto bail_acq_sema_failed;
+
+		if (!atomic_read(&this_dbs_info->sync_enabled)) {
+			atomic_set(&this_dbs_info->src_sync_cpu, -1);
+			put_online_cpus();
+			unlock_policy_rwsem_write(cpu);
+			continue;
+		}
+
+		policy = this_dbs_info->cur_policy;
+		if (!policy) {
+			/* CPU not using intellidemand governor */
+			goto bail_incorrect_governor;
+		}
+		delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+
+		if (policy->cur < src_freq) {
+			/* cancel the next intellidemand sample */
+			cancel_delayed_work_sync(&this_dbs_info->work);
+
+			/*
+			 * Arch specific cpufreq driver may fail.
+			 * Don't update governor frequency upon failure.
+			 */
+			if (__cpufreq_driver_target(policy, src_freq,
+						    CPUFREQ_RELATION_L) >= 0) {
+				policy->cur = src_freq;
+				if (src_max_load > this_dbs_info->max_load) {
+					this_dbs_info->max_load = src_max_load;
+					this_dbs_info->prev_load = src_max_load;
+				}
+			}
+
+			/* reschedule the next intellidemand sample */
+			mutex_lock(&this_dbs_info->timer_mutex);
+			queue_delayed_work_on(cpu, dbs_wq,
+					      &this_dbs_info->work, delay);
+			mutex_unlock(&this_dbs_info->timer_mutex);
+		}
+
+bail_incorrect_governor:
+		unlock_policy_rwsem_write(cpu);
+bail_acq_sema_failed:
+		put_online_cpus();
+		atomic_set(&this_dbs_info->src_sync_cpu, -1);
+	}
+
+	return 0;
+}
+
+static void dbs_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value)
+{
+	int i;
+	struct cpu_dbs_info_s *dbs_info;
+	unsigned long flags;
+	int input_event_min_freq;
+
+	if ((dbs_tuners_ins.powersave_bias == POWERSAVE_BIAS_MAXLEVEL) ||
+		(dbs_tuners_ins.powersave_bias == POWERSAVE_BIAS_MINLEVEL)) {
+		/* nothing to do */
+		return;
+	}
+
+	if (type == EV_SYN && code == SYN_REPORT) {		
+		spin_lock_irqsave(&input_boost_lock, flags);
+		input_event_boost = true;
+		ui_sampling_expired = jiffies + msecs_to_jiffies(dbs_tuners_ins.ui_timeout);
+		spin_unlock_irqrestore(&input_boost_lock, flags);
+
+		input_event_min_freq = input_event_min_freq_array[num_online_cpus() - 1];
+		for_each_online_cpu(i) {
+			dbs_info = &per_cpu(od_cpu_dbs_info, i);
+			if (dbs_info->cur_policy &&		
+				dbs_info->cur_policy->cur < input_event_min_freq) {
+				wake_up_process(per_cpu(up_task, i));
+			}
+		}
+	}
+}
+
+static int input_dev_filter(const char *input_dev_name)
+{
+	if (strstr(input_dev_name, "touchscreen") ||
+ 	    strstr(input_dev_name, "touch_dev") ||
+ 	    strstr(input_dev_name, "sec-touchscreen") ||
+	    strstr(input_dev_name, "keypad")) {
+		return 0; 
+	} else {
+		return 1;
+	}
+}
+
+
+static int dbs_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+	
+	if (input_dev_filter(dev->name))
+		return -ENODEV;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void dbs_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id dbs_ids[] = {
+	{ .driver_info = 1 },
+	{ },
+};
+
+static struct input_handler dbs_input_handler = {
+	.event		= dbs_input_event,
+	.connect	= dbs_input_connect,
+	.disconnect	= dbs_input_disconnect,
+	.name		= "cpufreq_ond",
+	.id_table	= dbs_ids,
+};
+
+int set_input_event_min_freq(int cpufreq)
+{
+	int i  = 0;
+	for ( i = 0 ; i < NR_CPUS; i++)
+		input_event_min_freq_array[i] = cpufreq;
+	return 0;
+}
+
+void set_input_event_min_freq_by_cpu ( int cpu_nr, int cpufreq){
+	input_event_min_freq_array[cpu_nr-1] = cpufreq;
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall);
+			if (dbs_tuners_ins.ignore_nice)
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			set_cpus_allowed(j_dbs_info->sync_thread,
+					 *cpumask_of(j));
+			if (!dbs_tuners_ins.powersave_bias)
+				atomic_set(&j_dbs_info->sync_enabled, 1);
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		this_dbs_info->freq_stay_count = 1;
+		intellidemand_powersave_bias_init_cpu(cpu);
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate =
+				max(min_sampling_rate,
+				    latency * LATENCY_MULTIPLIER);
+			if (dbs_tuners_ins.sampling_rate < DEF_SAMPLING_RATE)
+				dbs_tuners_ins.sampling_rate = DEF_SAMPLING_RATE;
+			dbs_tuners_ins.origin_sampling_rate = dbs_tuners_ins.sampling_rate;
+			dbs_tuners_ins.io_is_busy = should_io_be_busy();
+
+			if (dbs_tuners_ins.optimal_freq == 0)
+				dbs_tuners_ins.optimal_freq = policy->min;
+
+			if (dbs_tuners_ins.sync_freq == 0)
+				dbs_tuners_ins.sync_freq = policy->min;
+
+			atomic_notifier_chain_register(&migration_notifier_head,
+					&dbs_migration_nb);
+		}
+		if (!cpu)
+			rc = input_register_handler(&dbs_input_handler);
+		mutex_unlock(&dbs_mutex);
+
+
+		if (!intellidemand_powersave_bias_setspeed(
+					this_dbs_info->cur_policy,
+					NULL,
+					dbs_tuners_ins.powersave_bias))
+			dbs_timer_init(this_dbs_info);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			atomic_set(&j_dbs_info->sync_enabled, 0);
+		}
+
+		/* If device is being removed, policy is no longer
+		 * valid. */
+		this_dbs_info->cur_policy = NULL;
+		if (!cpu)
+			input_unregister_handler(&dbs_input_handler);
+		if (!dbs_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+			atomic_notifier_chain_unregister(
+				&migration_notifier_head,
+				&dbs_migration_nb);
+		}
+
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (this_dbs_info->cur_policy == NULL) {
+			pr_debug("Unable to limit cpu freq due to cur_policy == NULL\n");
+			return -EPERM;
+		}
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		else if (dbs_tuners_ins.powersave_bias != 0)
+			intellidemand_powersave_bias_setspeed(
+				this_dbs_info->cur_policy,
+				policy,
+				dbs_tuners_ins.powersave_bias);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+		break;
+	}
+	return 0;
+}
+
+static int cpufreq_gov_dbs_up_task(void *data)
+{
+	struct cpufreq_policy *policy;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int cpu = smp_processor_id();
+	int input_event_min_freq;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		get_online_cpus();
+
+		if (lock_policy_rwsem_write(cpu) < 0)
+			goto bail_acq_sema_failed;
+
+		this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+		policy = this_dbs_info->cur_policy;
+		if (!policy) {
+			
+			goto bail_incorrect_governor;
+		}
+
+		mutex_lock(&this_dbs_info->timer_mutex);
+
+		input_event_min_freq = input_event_min_freq_array[num_online_cpus() - 1];
+		if (policy->cur < input_event_min_freq) {
+			
+			dbs_tuners_ins.powersave_bias = 0;
+			dbs_freq_increase(policy, input_event_min_freq);
+			this_dbs_info->prev_cpu_idle = get_cpu_idle_time(cpu, &this_dbs_info->prev_cpu_wall);
+		}
+
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+bail_incorrect_governor:
+		unlock_policy_rwsem_write(cpu);
+
+bail_acq_sema_failed:
+		put_online_cpus();
+
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.ui_sampling_rate;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_POWERSUSPEND
+static void cpufreq_intellidemand_power_suspend(struct power_suspend *h)
+{
+	mutex_lock(&dbs_mutex);
+	stored_sampling_rate = dbs_tuners_ins.sampling_rate;
+	dbs_tuners_ins.sampling_rate = DEF_SAMPLING_RATE * 6;
+	update_sampling_rate(dbs_tuners_ins.sampling_rate);
+	mutex_unlock(&dbs_mutex);
+}
+
+static void cpufreq_intellidemand_power_resume(struct power_suspend *h)
+{
+	mutex_lock(&dbs_mutex);
+	dbs_tuners_ins.sampling_rate = stored_sampling_rate;
+	update_sampling_rate(dbs_tuners_ins.sampling_rate);
+	mutex_unlock(&dbs_mutex);
+}
+
+static struct power_suspend cpufreq_intellidemand_power_suspend_info = {
+	.suspend = cpufreq_intellidemand_power_suspend,
+	.resume = cpufreq_intellidemand_power_resume,
+};
+#endif
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	u64 idle_time;
+	unsigned int i;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+	struct task_struct *pthread;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, NULL);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/* Idle micro accounting is supported. Use finer thresholds */
+		dbs_tuners_ins.up_threshold = MICRO_FREQUENCY_UP_THRESHOLD;
+		dbs_tuners_ins.down_differential =
+					MICRO_FREQUENCY_DOWN_DIFFERENTIAL;
+		/*
+		 * In nohz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate =
+			MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	dbs_wq = alloc_workqueue("intellidemand_dbs_wq", WQ_HIGHPRI, 0);
+	if (!dbs_wq) {
+		printk(KERN_ERR "Failed to create intellidemand_dbs_wq workqueue\n");
+		return -EFAULT;
+	}
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(od_cpu_dbs_info, i);
+		struct dbs_work_struct *dbs_work =
+			&per_cpu(dbs_refresh_work, i);
+
+		pthread = kthread_create_on_node(cpufreq_gov_dbs_up_task,
+								NULL, cpu_to_node(i),
+								"kdbs_up/%d", i);
+		if (likely(!IS_ERR(pthread))) {
+			kthread_bind(pthread, i);
+			sched_setscheduler_nocheck(pthread, SCHED_FIFO, &param);
+			get_task_struct(pthread);
+			per_cpu(up_task, i) = pthread;
+		}
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		INIT_WORK(&dbs_work->work, dbs_refresh_callback);
+		dbs_work->cpu = i;
+
+		atomic_set(&this_dbs_info->src_sync_cpu, -1);
+		init_waitqueue_head(&this_dbs_info->sync_wq);
+
+		this_dbs_info->sync_thread = kthread_run(dbs_sync_thread,
+							 (void *)i,
+							 "dbs_sync/%d", i);
+	}
+
+#ifdef CONFIG_POWERSUSPEND
+	register_power_suspend(&cpufreq_intellidemand_power_suspend_info);
+#endif
+	return cpufreq_register_governor(&cpufreq_gov_intellidemand);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	unsigned int i;
+
+	cpufreq_unregister_governor(&cpufreq_gov_intellidemand);
+	for_each_possible_cpu(i) {
+		struct cpu_dbs_info_s *this_dbs_info =
+			&per_cpu(od_cpu_dbs_info, i);
+		mutex_destroy(&this_dbs_info->timer_mutex);
+		kthread_stop(this_dbs_info->sync_thread);
+		if (per_cpu(up_task, i)) {
+			kthread_stop(per_cpu(up_task, i));
+			put_task_struct(per_cpu(up_task, i));
+		}
+	}
+	destroy_workqueue(dbs_wq);
+}
+
+MODULE_AUTHOR("Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>");
+MODULE_AUTHOR("Alexey Starikovskiy <alexey.y.starikovskiy@intel.com>");
+MODULE_AUTHOR("Paul Reioux <reioux@gmail.com>");
+MODULE_DESCRIPTION("'cpufreq_intellidemand' - A dynamic cpufreq governor for "
+	"Low Latency Frequency Transition capable processors");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
+
diff --git a/drivers/cpufreq/cpufreq_interactiveX.c b/drivers/cpufreq/cpufreq_interactiveX.c
new file mode 100644
index 00000000000..b7bbfca3b0a
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_interactiveX.c
@@ -0,0 +1,1290 @@
+/*
+ * drivers/cpufreq/cpufreq_interactive.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Mike Chan (mike@android.com)
+ * Modified for early suspend support and hotplugging by imoseyon (imoseyon@gmail.com)
+ *   interactiveX V2
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/earlysuspend.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_interactivex.h>
+
+static int active_count;
+
+struct cpufreq_interactivex_cpuinfo {
+	struct timer_list cpu_timer;
+	struct timer_list cpu_slack_timer;
+	spinlock_t load_lock; /* protects the next 4 fields */
+	u64 time_in_idle;
+	u64 time_in_idle_timestamp;
+	u64 cputime_speedadj;
+	u64 cputime_speedadj_timestamp;
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int target_freq;
+	unsigned int floor_freq;
+	u64 floor_validate_time;
+	u64 hispeed_validate_time;
+	struct rw_semaphore enable_sem;
+	int governor_enabled;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_interactivex_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+// used for suspend code
+static unsigned int registration = 0;
+static unsigned int enabled = 0;
+
+static spinlock_t speedchange_cpumask_lock;
+static struct mutex gov_lock;
+
+/* Hi speed to bump to from lo speed when load burst (default max) */
+static unsigned int hispeed_freq;
+
+/* Go to hi speed when CPU load at or above this value. */
+#define DEFAULT_GO_HISPEED_LOAD 99
+static unsigned long go_hispeed_load = DEFAULT_GO_HISPEED_LOAD;
+
+/* Target load.  Lower values result in higher CPU speeds. */
+#define DEFAULT_TARGET_LOAD 90
+static unsigned int default_target_loads[] = {DEFAULT_TARGET_LOAD};
+static spinlock_t target_loads_lock;
+static unsigned int *target_loads = default_target_loads;
+static int ntarget_loads = ARRAY_SIZE(default_target_loads);
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ */
+#define DEFAULT_MIN_SAMPLE_TIME (80 * USEC_PER_MSEC)
+static unsigned long min_sample_time = DEFAULT_MIN_SAMPLE_TIME;
+
+/*
+ * The sample rate of the timer used to increase frequency
+ */
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate = DEFAULT_TIMER_RATE;
+
+/*
+ * Wait this long before raising speed above hispeed, by default a single
+ * timer interval.
+ */
+#define DEFAULT_ABOVE_HISPEED_DELAY DEFAULT_TIMER_RATE
+static unsigned int default_above_hispeed_delay[] = {
+	DEFAULT_ABOVE_HISPEED_DELAY };
+static spinlock_t above_hispeed_delay_lock;
+static unsigned int *above_hispeed_delay = default_above_hispeed_delay;
+static int nabove_hispeed_delay = ARRAY_SIZE(default_above_hispeed_delay);
+
+/* Non-zero means indefinite speed boost active */
+static int boost_val;
+/* Duration of a boot pulse in usecs */
+static int boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
+/* End time of boost pulse in ktime converted to usecs */
+static u64 boostpulse_endtime;
+
+/*
+ * Max additional time to wait in idle, beyond timer_rate, at speeds above
+ * minimum before wakeup to reduce speed, or -1 if unnecessary.
+ */
+#define DEFAULT_TIMER_SLACK (4 * DEFAULT_TIMER_RATE)
+static int timer_slack_val = DEFAULT_TIMER_SLACK;
+
+static bool io_is_busy;
+
+static int cpufreq_governor_interactive(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_interactivex = {
+	.name = "interactivex",
+	.governor = cpufreq_governor_interactive,
+	.max_transition_latency = 10000000,
+	.owner = THIS_MODULE,
+};
+
+static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+						  cputime64_t *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu,
+					    cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+	if (idle_time == -1ULL)
+		idle_time = get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+static void cpufreq_interactivex_timer_resched(
+	struct cpufreq_interactivex_cpuinfo *pcpu)
+{
+	unsigned long expires;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(smp_processor_id(),
+				     &pcpu->time_in_idle_timestamp);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	expires = jiffies + usecs_to_jiffies(timer_rate);
+	mod_timer_pinned(&pcpu->cpu_timer, expires);
+
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		mod_timer_pinned(&pcpu->cpu_slack_timer, expires);
+	}
+
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+/* The caller shall take enable_sem write semaphore to avoid any timer race.
+ * The cpu_timer and cpu_slack_timer must be deactivated when calling this
+ * function.
+ */
+static void cpufreq_interactivex_timer_start(int cpu)
+{
+	struct cpufreq_interactivex_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	unsigned long expires = jiffies + usecs_to_jiffies(timer_rate);
+	unsigned long flags;
+
+	pcpu->cpu_timer.expires = expires;
+	add_timer_on(&pcpu->cpu_timer, cpu);
+	if (timer_slack_val >= 0 && pcpu->target_freq > pcpu->policy->min) {
+		expires += usecs_to_jiffies(timer_slack_val);
+		pcpu->cpu_slack_timer.expires = expires;
+		add_timer_on(&pcpu->cpu_slack_timer, cpu);
+	}
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	pcpu->time_in_idle =
+		get_cpu_idle_time(cpu, &pcpu->time_in_idle_timestamp);
+	pcpu->cputime_speedadj = 0;
+	pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+}
+
+static unsigned int freq_to_above_hispeed_delay(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay - 1 &&
+			freq >= above_hispeed_delay[i+1]; i += 2)
+		;
+
+	ret = above_hispeed_delay[i];
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static unsigned int freq_to_targetload(unsigned int freq)
+{
+	int i;
+	unsigned int ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads - 1 && freq >= target_loads[i+1]; i += 2)
+		;
+
+	ret = target_loads[i];
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+/*
+ * If increasing frequencies never map to a lower target load then
+ * choose_freq() will find the minimum frequency that does not exceed its
+ * target load given the current load.
+ */
+
+static unsigned int choose_freq(
+	struct cpufreq_interactivex_cpuinfo *pcpu, unsigned int loadadjfreq)
+{
+	unsigned int freq = pcpu->policy->cur;
+	unsigned int prevfreq, freqmin, freqmax;
+	unsigned int tl;
+	int index;
+
+	freqmin = 0;
+	freqmax = UINT_MAX;
+
+	do {
+		prevfreq = freq;
+		tl = freq_to_targetload(freq);
+
+		/*
+		 * Find the lowest frequency where the computed load is less
+		 * than or equal to the target load.
+		 */
+
+		if (cpufreq_frequency_table_target(
+			    pcpu->policy, pcpu->freq_table, loadadjfreq / tl,
+			    CPUFREQ_RELATION_L, &index))
+			break;
+		freq = pcpu->freq_table[index].frequency;
+
+		if (freq > prevfreq) {
+			/* The previous frequency is too low. */
+			freqmin = prevfreq;
+
+			if (freq >= freqmax) {
+				/*
+				 * Find the highest frequency that is less
+				 * than freqmax.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmax - 1, CPUFREQ_RELATION_H,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				if (freq == freqmin) {
+					/*
+					 * The first frequency below freqmax
+					 * has already been found to be too
+					 * low.  freqmax is the lowest speed
+					 * we found that is fast enough.
+					 */
+					freq = freqmax;
+					break;
+				}
+			}
+		} else if (freq < prevfreq) {
+			/* The previous frequency is high enough. */
+			freqmax = prevfreq;
+
+			if (freq <= freqmin) {
+				/*
+				 * Find the lowest frequency that is higher
+				 * than freqmin.
+				 */
+				if (cpufreq_frequency_table_target(
+					    pcpu->policy, pcpu->freq_table,
+					    freqmin + 1, CPUFREQ_RELATION_L,
+					    &index))
+					break;
+				freq = pcpu->freq_table[index].frequency;
+
+				/*
+				 * If freqmax is the first frequency above
+				 * freqmin then we have already found that
+				 * this speed is fast enough.
+				 */
+				if (freq == freqmax)
+					break;
+			}
+		}
+
+		/* If same frequency chosen as previous then done. */
+	} while (freq != prevfreq);
+
+	return freq;
+}
+
+static u64 update_load(int cpu)
+{
+	struct cpufreq_interactivex_cpuinfo *pcpu = &per_cpu(cpuinfo, cpu);
+	u64 now;
+	u64 now_idle;
+	unsigned int delta_idle;
+	unsigned int delta_time;
+	u64 active_time;
+
+	now_idle = get_cpu_idle_time(cpu, &now);
+	delta_idle = (unsigned int) (now_idle - pcpu->time_in_idle);
+	delta_time = (unsigned int) (now - pcpu->time_in_idle_timestamp);
+
+	if (delta_time <= delta_idle)
+		active_time = 0;
+	else
+		active_time = delta_time - delta_idle;
+
+	pcpu->cputime_speedadj += active_time * pcpu->policy->cur;
+
+	pcpu->time_in_idle = now_idle;
+	pcpu->time_in_idle_timestamp = now;
+	return now;
+}
+
+static void cpufreq_interactivex_timer(unsigned long data)
+{
+	u64 now;
+	unsigned int delta_time;
+	u64 cputime_speedadj;
+	int cpu_load;
+	struct cpufreq_interactivex_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, data);
+	unsigned int new_freq;
+	unsigned int loadadjfreq;
+	unsigned int index;
+	unsigned long flags;
+	bool boosted;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled)
+		goto exit;
+
+	spin_lock_irqsave(&pcpu->load_lock, flags);
+	now = update_load(data);
+	delta_time = (unsigned int)(now - pcpu->cputime_speedadj_timestamp);
+	cputime_speedadj = pcpu->cputime_speedadj;
+	spin_unlock_irqrestore(&pcpu->load_lock, flags);
+
+	if (WARN_ON_ONCE(!delta_time))
+		goto rearm;
+
+	do_div(cputime_speedadj, delta_time);
+	loadadjfreq = (unsigned int)cputime_speedadj * 100;
+	cpu_load = loadadjfreq / pcpu->target_freq;
+	boosted = boost_val || now < boostpulse_endtime;
+
+	if (cpu_load >= go_hispeed_load || boosted) {
+		if (pcpu->target_freq < hispeed_freq) {
+			new_freq = hispeed_freq;
+		} else {
+			new_freq = choose_freq(pcpu, loadadjfreq);
+
+			if (new_freq < hispeed_freq)
+				new_freq = hispeed_freq;
+		}
+	} else {
+		new_freq = choose_freq(pcpu, loadadjfreq);
+	}
+
+	if (pcpu->target_freq >= hispeed_freq &&
+	    new_freq > pcpu->target_freq &&
+	    now - pcpu->hispeed_validate_time <
+	    freq_to_above_hispeed_delay(pcpu->target_freq)) {
+		trace_cpufreq_interactivex_notyet(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm;
+	}
+
+	pcpu->hispeed_validate_time = now;
+
+	if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+					   new_freq, CPUFREQ_RELATION_L,
+					   &index))
+		goto rearm;
+
+	new_freq = pcpu->freq_table[index].frequency;
+
+	/*
+	 * Do not scale below floor_freq unless we have been at or above the
+	 * floor frequency for the minimum sample time since last validated.
+	 */
+	if (new_freq < pcpu->floor_freq) {
+		if ((now - pcpu->floor_validate_time) < min_sample_time) {
+			trace_cpufreq_interactivex_notyet(
+				data, cpu_load, pcpu->target_freq,
+				pcpu->policy->cur, new_freq);
+			goto rearm;
+		}
+	}
+
+	/*
+	 * Update the timestamp for checking whether speed has been held at
+	 * or above the selected frequency for a minimum of min_sample_time,
+	 * if not boosted to hispeed_freq.  If boosted to hispeed_freq then we
+	 * allow the speed to drop as soon as the boostpulse duration expires
+	 * (or the indefinite boost is turned off).
+	 */
+
+	if (!boosted || new_freq > hispeed_freq) {
+		pcpu->floor_freq = new_freq;
+		pcpu->floor_validate_time = now;
+	}
+
+	if (pcpu->target_freq == new_freq) {
+		trace_cpufreq_interactivex_already(
+			data, cpu_load, pcpu->target_freq,
+			pcpu->policy->cur, new_freq);
+		goto rearm_if_notmax;
+	}
+
+	trace_cpufreq_interactivex_target(data, cpu_load, pcpu->target_freq,
+					 pcpu->policy->cur, new_freq);
+
+	pcpu->target_freq = new_freq;
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	cpumask_set_cpu(data, &speedchange_cpumask);
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+	/*
+	 * Already set max speed and don't see a need to change that,
+	 * wait until next idle to re-evaluate, don't need timer.
+	 */
+	if (pcpu->target_freq == pcpu->policy->max)
+		goto exit;
+
+rearm:
+	if (!timer_pending(&pcpu->cpu_timer))
+		cpufreq_interactivex_timer_resched(pcpu);
+
+exit:
+	up_read(&pcpu->enable_sem);
+	return;
+}
+
+static void cpufreq_interactivex_idle_start(void)
+{
+	struct cpufreq_interactivex_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+	int pending;
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	pending = timer_pending(&pcpu->cpu_timer);
+
+	if (pcpu->target_freq != pcpu->policy->min) {
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending)
+			cpufreq_interactivex_timer_resched(pcpu);
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static void cpufreq_interactivex_idle_end(void)
+{
+	struct cpufreq_interactivex_cpuinfo *pcpu =
+		&per_cpu(cpuinfo, smp_processor_id());
+
+	if (!down_read_trylock(&pcpu->enable_sem))
+		return;
+	if (!pcpu->governor_enabled) {
+		up_read(&pcpu->enable_sem);
+		return;
+	}
+
+	/* Arm the timer for 1-2 ticks later if not already. */
+	if (!timer_pending(&pcpu->cpu_timer)) {
+		cpufreq_interactivex_timer_resched(pcpu);
+	} else if (time_after_eq(jiffies, pcpu->cpu_timer.expires)) {
+		del_timer(&pcpu->cpu_timer);
+		del_timer(&pcpu->cpu_slack_timer);
+		cpufreq_interactivex_timer(smp_processor_id());
+	}
+
+	up_read(&pcpu->enable_sem);
+}
+
+static int cpufreq_interactivex_speedchange_task(void *data)
+{
+	unsigned int cpu;
+	cpumask_t tmp_mask;
+	unsigned long flags;
+	struct cpufreq_interactivex_cpuinfo *pcpu;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+		if (cpumask_empty(&speedchange_cpumask)) {
+			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+					       flags);
+			schedule();
+
+			if (kthread_should_stop())
+				break;
+
+			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		}
+
+		set_current_state(TASK_RUNNING);
+		tmp_mask = speedchange_cpumask;
+		cpumask_clear(&speedchange_cpumask);
+		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+		for_each_cpu(cpu, &tmp_mask) {
+			unsigned int j;
+			unsigned int max_freq = 0;
+
+			pcpu = &per_cpu(cpuinfo, cpu);
+			if (!down_read_trylock(&pcpu->enable_sem))
+				continue;
+			if (!pcpu->governor_enabled) {
+				up_read(&pcpu->enable_sem);
+				continue;
+			}
+
+			for_each_cpu(j, pcpu->policy->cpus) {
+				struct cpufreq_interactivex_cpuinfo *pjcpu =
+					&per_cpu(cpuinfo, j);
+
+				if (pjcpu->target_freq > max_freq)
+					max_freq = pjcpu->target_freq;
+			}
+
+			if (max_freq != pcpu->policy->cur)
+				__cpufreq_driver_target(pcpu->policy,
+							max_freq,
+							CPUFREQ_RELATION_H);
+			trace_cpufreq_interactivex_setspeed(cpu,
+						     pcpu->target_freq,
+						     pcpu->policy->cur);
+
+			up_read(&pcpu->enable_sem);
+		}
+	}
+
+	return 0;
+}
+
+static void cpufreq_interactivex_boost(void)
+{
+	int i;
+	int anyboost = 0;
+	unsigned long flags;
+	struct cpufreq_interactivex_cpuinfo *pcpu;
+
+	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+	for_each_online_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+
+		if (pcpu->target_freq < hispeed_freq) {
+			pcpu->target_freq = hispeed_freq;
+			cpumask_set_cpu(i, &speedchange_cpumask);
+			pcpu->hispeed_validate_time =
+				ktime_to_us(ktime_get());
+			anyboost = 1;
+		}
+
+		/*
+		 * Set floor freq and (re)start timer for when last
+		 * validated.
+		 */
+
+		pcpu->floor_freq = hispeed_freq;
+		pcpu->floor_validate_time = ktime_to_us(ktime_get());
+	}
+
+	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+	if (anyboost)
+		wake_up_process(speedchange_task);
+}
+
+static void __ref interactivex_suspend(int suspend)
+{
+        if (!enabled) return;
+	if (!suspend) { 
+                if (num_online_cpus() < 2) cpu_up(1);
+                pr_info("[imoseyon] interactivex awake cpu1 up\n");
+	} else {
+                if (num_online_cpus() > 1) cpu_down(1);
+                pr_info("[imoseyon] interactivex suspended cpu1 down\n");
+	}
+}
+
+static void interactivex_early_suspend(struct early_suspend *handler) {
+      if (!registration) interactivex_suspend(1);
+}
+
+static void interactivex_late_resume(struct early_suspend *handler) {
+     interactivex_suspend(0);
+}
+
+static struct early_suspend interactivex_power_suspend = {
+        .suspend = interactivex_early_suspend,
+        .resume = interactivex_late_resume,
+        .level = EARLY_SUSPEND_LEVEL_DISABLE_FB + 1,
+};
+
+static int cpufreq_interactivex_notifier(
+	struct notifier_block *nb, unsigned long val, void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpufreq_interactivex_cpuinfo *pcpu;
+	int cpu;
+	unsigned long flags;
+
+	if (val == CPUFREQ_POSTCHANGE) {
+		pcpu = &per_cpu(cpuinfo, freq->cpu);
+		if (!down_read_trylock(&pcpu->enable_sem))
+			return 0;
+		if (!pcpu->governor_enabled) {
+			up_read(&pcpu->enable_sem);
+			return 0;
+		}
+
+		for_each_cpu(cpu, pcpu->policy->cpus) {
+			struct cpufreq_interactivex_cpuinfo *pjcpu =
+				&per_cpu(cpuinfo, cpu);
+			if (cpu != freq->cpu) {
+				if (!down_read_trylock(&pjcpu->enable_sem))
+					continue;
+				if (!pjcpu->governor_enabled) {
+					up_read(&pjcpu->enable_sem);
+					continue;
+				}
+			}
+			spin_lock_irqsave(&pjcpu->load_lock, flags);
+			update_load(cpu);
+			spin_unlock_irqrestore(&pjcpu->load_lock, flags);
+			if (cpu != freq->cpu)
+				up_read(&pjcpu->enable_sem);
+		}
+
+		up_read(&pcpu->enable_sem);
+	}
+	return 0;
+}
+
+static struct notifier_block cpufreq_notifier_block = {
+	.notifier_call = cpufreq_interactivex_notifier,
+};
+
+static unsigned int *get_tokenized_data(const char *buf, int *num_tokens)
+{
+	const char *cp;
+	int i;
+	int ntokens = 1;
+	unsigned int *tokenized_data;
+	int err = -EINVAL;
+
+	cp = buf;
+	while ((cp = strpbrk(cp + 1, " :")))
+		ntokens++;
+
+	if (!(ntokens & 0x1))
+		goto err;
+
+	tokenized_data = kmalloc(ntokens * sizeof(unsigned int), GFP_KERNEL);
+	if (!tokenized_data) {
+		err = -ENOMEM;
+		goto err;
+	}
+
+	cp = buf;
+	i = 0;
+	while (i < ntokens) {
+		if (sscanf(cp, "%u", &tokenized_data[i++]) != 1)
+			goto err_kfree;
+
+		cp = strpbrk(cp, " :");
+		if (!cp)
+			break;
+		cp++;
+	}
+
+	if (i != ntokens)
+		goto err_kfree;
+
+	*num_tokens = ntokens;
+	return tokenized_data;
+
+err_kfree:
+	kfree(tokenized_data);
+err:
+	return ERR_PTR(err);
+}
+
+static ssize_t show_target_loads(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+
+	for (i = 0; i < ntarget_loads; i++)
+		ret += sprintf(buf + ret, "%u%s", target_loads[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret--;
+	ret += sprintf(buf + ret, "\n");
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return ret;
+}
+
+static ssize_t store_target_loads(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_target_loads = NULL;
+	unsigned long flags;
+
+	new_target_loads = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_target_loads))
+		return PTR_RET(new_target_loads);
+
+	spin_lock_irqsave(&target_loads_lock, flags);
+	if (target_loads != default_target_loads)
+		kfree(target_loads);
+	target_loads = new_target_loads;
+	ntarget_loads = ntokens;
+	spin_unlock_irqrestore(&target_loads_lock, flags);
+	return count;
+}
+
+static struct global_attr target_loads_attr =
+	__ATTR(target_loads, S_IRUGO | S_IWUSR,
+		show_target_loads, store_target_loads);
+
+static ssize_t show_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	int i;
+	ssize_t ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+
+	for (i = 0; i < nabove_hispeed_delay; i++)
+		ret += sprintf(buf + ret, "%u%s", above_hispeed_delay[i],
+			       i & 0x1 ? ":" : " ");
+
+	ret --;
+	ret += sprintf(buf + ret, "\n");
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return ret;
+}
+
+static ssize_t store_above_hispeed_delay(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ntokens;
+	unsigned int *new_above_hispeed_delay = NULL;
+	unsigned long flags;
+
+	new_above_hispeed_delay = get_tokenized_data(buf, &ntokens);
+	if (IS_ERR(new_above_hispeed_delay))
+		return PTR_RET(new_above_hispeed_delay);
+
+	spin_lock_irqsave(&above_hispeed_delay_lock, flags);
+	if (above_hispeed_delay != default_above_hispeed_delay)
+		kfree(above_hispeed_delay);
+	above_hispeed_delay = new_above_hispeed_delay;
+	nabove_hispeed_delay = ntokens;
+	spin_unlock_irqrestore(&above_hispeed_delay_lock, flags);
+	return count;
+
+}
+
+static struct global_attr above_hispeed_delay_attr =
+	__ATTR(above_hispeed_delay, S_IRUGO | S_IWUSR,
+		show_above_hispeed_delay, store_above_hispeed_delay);
+
+static ssize_t show_hispeed_freq(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", hispeed_freq);
+}
+
+static ssize_t store_hispeed_freq(struct kobject *kobj,
+				  struct attribute *attr, const char *buf,
+				  size_t count)
+{
+	int ret;
+	long unsigned int val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	hispeed_freq = val;
+	return count;
+}
+
+static struct global_attr hispeed_freq_attr = __ATTR(hispeed_freq, 0644,
+		show_hispeed_freq, store_hispeed_freq);
+
+
+static ssize_t show_go_hispeed_load(struct kobject *kobj,
+				     struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", go_hispeed_load);
+}
+
+static ssize_t store_go_hispeed_load(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	go_hispeed_load = val;
+	return count;
+}
+
+static struct global_attr go_hispeed_load_attr = __ATTR(go_hispeed_load, 0644,
+		show_go_hispeed_load, store_go_hispeed_load);
+
+static ssize_t show_min_sample_time(struct kobject *kobj,
+				struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_sample_time);
+}
+
+static ssize_t store_min_sample_time(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	min_sample_time = val;
+	return count;
+}
+
+static struct global_attr min_sample_time_attr = __ATTR(min_sample_time, 0644,
+		show_min_sample_time, store_min_sample_time);
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = strict_strtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	timer_rate = val;
+	return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+		show_timer_rate, store_timer_rate);
+
+static ssize_t show_timer_slack(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", timer_slack_val);
+}
+
+static ssize_t store_timer_slack(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtol(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	timer_slack_val = val;
+	return count;
+}
+
+define_one_global_rw(timer_slack);
+
+static ssize_t show_boost(struct kobject *kobj, struct attribute *attr,
+			  char *buf)
+{
+	return sprintf(buf, "%d\n", boost_val);
+}
+
+static ssize_t store_boost(struct kobject *kobj, struct attribute *attr,
+			   const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boost_val = val;
+
+	if (boost_val) {
+		trace_cpufreq_interactivex_boost("on");
+		cpufreq_interactivex_boost();
+	} else {
+		trace_cpufreq_interactivex_unboost("off");
+	}
+
+	return count;
+}
+
+define_one_global_rw(boost);
+
+static ssize_t store_boostpulse(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_endtime = ktime_to_us(ktime_get()) + boostpulse_duration_val;
+	trace_cpufreq_interactivex_boost("pulse");
+	cpufreq_interactivex_boost();
+	return count;
+}
+
+static struct global_attr boostpulse =
+	__ATTR(boostpulse, 0200, NULL, store_boostpulse);
+
+static ssize_t show_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", boostpulse_duration_val);
+}
+
+static ssize_t store_boostpulse_duration(
+	struct kobject *kobj, struct attribute *attr, const char *buf,
+	size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+
+	boostpulse_duration_val = val;
+	return count;
+}
+
+define_one_global_rw(boostpulse_duration);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+	unsigned long val;
+
+	ret = kstrtoul(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	io_is_busy = val;
+	return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+		show_io_is_busy, store_io_is_busy);
+
+static struct attribute *interactivex_attributes[] = {
+	&target_loads_attr.attr,
+	&above_hispeed_delay_attr.attr,
+	&hispeed_freq_attr.attr,
+	&go_hispeed_load_attr.attr,
+	&min_sample_time_attr.attr,
+	&timer_rate_attr.attr,
+	&timer_slack.attr,
+	&boost.attr,
+	&boostpulse.attr,
+	&boostpulse_duration.attr,
+	&io_is_busy_attr.attr,
+	NULL,
+};
+
+static struct attribute_group interactivex_attr_group = {
+	.attrs = interactivex_attributes,
+	.name = "interactive",
+};
+
+static int cpufreq_interactivex_idle_notifier(struct notifier_block *nb,
+					     unsigned long val,
+					     void *data)
+{
+	switch (val) {
+	case IDLE_START:
+		cpufreq_interactivex_idle_start();
+		break;
+	case IDLE_END:
+		cpufreq_interactivex_idle_end();
+		break;
+	}
+
+	return 0;
+}
+
+static struct notifier_block cpufreq_interactivex_idle_nb = {
+	.notifier_call = cpufreq_interactivex_idle_notifier,
+};
+
+static int cpufreq_governor_interactive(struct cpufreq_policy *policy,
+		unsigned int event)
+{
+	int rc;
+	unsigned int j;
+	struct cpufreq_interactivex_cpuinfo *pcpu;
+	struct cpufreq_frequency_table *freq_table;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if (!cpu_online(policy->cpu))
+			return -EINVAL;
+
+		mutex_lock(&gov_lock);
+
+		freq_table =
+			cpufreq_frequency_get_table(policy->cpu);
+		if (!hispeed_freq)
+			hispeed_freq = policy->max;
+
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			pcpu->policy = policy;
+			pcpu->target_freq = policy->cur;
+			pcpu->freq_table = freq_table;
+			pcpu->floor_freq = pcpu->target_freq;
+			pcpu->floor_validate_time =
+				ktime_to_us(ktime_get());
+			pcpu->hispeed_validate_time =
+				pcpu->floor_validate_time;
+			down_write(&pcpu->enable_sem);
+			cpufreq_interactivex_timer_start(j);
+			pcpu->governor_enabled = 1;
+			up_write(&pcpu->enable_sem);
+		}
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (++active_count > 1) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		rc = sysfs_create_group(cpufreq_global_kobject,
+				&interactivex_attr_group);
+
+		if (rc) {
+			mutex_unlock(&gov_lock);
+		}
+
+		idle_notifier_register(&cpufreq_interactivex_idle_nb);
+		cpufreq_register_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		mutex_unlock(&gov_lock);
+
+		enabled = 1;
+		registration = 1;
+                register_early_suspend(&interactivex_power_suspend);
+		registration = 0;
+                pr_info("[imoseyon] interactivex start\n");
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&gov_lock);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+			down_write(&pcpu->enable_sem);
+			pcpu->governor_enabled = 0;
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			up_write(&pcpu->enable_sem);
+		}
+
+		if (--active_count > 0) {
+			mutex_unlock(&gov_lock);
+			return 0;
+		}
+
+		cpufreq_unregister_notifier(
+			&cpufreq_notifier_block, CPUFREQ_TRANSITION_NOTIFIER);
+		idle_notifier_unregister(&cpufreq_interactivex_idle_nb);
+		mutex_unlock(&gov_lock);
+
+		enabled = 0;
+                unregister_early_suspend(&interactivex_power_suspend);
+                pr_info("[imoseyon] interactivex inactive\n");
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		if (policy->max < policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > policy->cur)
+			__cpufreq_driver_target(policy,
+					policy->min, CPUFREQ_RELATION_L);
+		for_each_cpu(j, policy->cpus) {
+			pcpu = &per_cpu(cpuinfo, j);
+
+			/* hold write semaphore to avoid race */
+			down_write(&pcpu->enable_sem);
+			if (pcpu->governor_enabled == 0) {
+				up_write(&pcpu->enable_sem);
+				continue;
+			}
+
+			/* update target_freq firstly */
+			if (policy->max < pcpu->target_freq)
+				pcpu->target_freq = policy->max;
+			else if (policy->min > pcpu->target_freq)
+				pcpu->target_freq = policy->min;
+
+			/* Reschedule timer.
+			 * Delete the timers, else the timer callback may
+			 * return without re-arm the timer when failed
+			 * acquire the semaphore. This race may cause timer
+			 * stopped unexpectedly.
+			 */
+			del_timer_sync(&pcpu->cpu_timer);
+			del_timer_sync(&pcpu->cpu_slack_timer);
+			cpufreq_interactivex_timer_start(j);
+			up_write(&pcpu->enable_sem);
+		}
+		break;
+	}
+	return 0;
+}
+
+static void cpufreq_interactivex_nop_timer(unsigned long data)
+{
+}
+
+static int __init cpufreq_interactivex_init(void)
+{
+	unsigned int i;
+	struct cpufreq_interactivex_cpuinfo *pcpu;
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
+	/* Initalize per-cpu timers */
+	for_each_possible_cpu(i) {
+		pcpu = &per_cpu(cpuinfo, i);
+		init_timer_deferrable(&pcpu->cpu_timer);
+		pcpu->cpu_timer.function = cpufreq_interactivex_timer;
+		pcpu->cpu_timer.data = i;
+		init_timer(&pcpu->cpu_slack_timer);
+		pcpu->cpu_slack_timer.function = cpufreq_interactivex_nop_timer;
+		spin_lock_init(&pcpu->load_lock);
+		init_rwsem(&pcpu->enable_sem);
+	}
+
+	spin_lock_init(&target_loads_lock);
+	spin_lock_init(&speedchange_cpumask_lock);
+	spin_lock_init(&above_hispeed_delay_lock);
+	mutex_init(&gov_lock);
+	speedchange_task =
+		kthread_create(cpufreq_interactivex_speedchange_task, NULL,
+			       "cfinteractive");
+	if (IS_ERR(speedchange_task))
+		return PTR_ERR(speedchange_task);
+
+	sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+	get_task_struct(speedchange_task);
+
+	/* NB: wake up so the thread does not look hung to the freezer */
+	wake_up_process(speedchange_task);
+
+	return cpufreq_register_governor(&cpufreq_gov_interactivex);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE
+fs_initcall(cpufreq_interactivex_init);
+#else
+module_init(cpufreq_interactivex_init);
+#endif
+
+static void __exit cpufreq_interactivex_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_interactivex);
+	kthread_stop(speedchange_task);
+	put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_interactivex_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_interactivex' - A cpufreq governor for "
+	"Latency sensitive workloads");
+MODULE_LICENSE("GPL");
+
diff --git a/drivers/cpufreq/cpufreq_ktoonservative.c b/drivers/cpufreq/cpufreq_ktoonservative.c
new file mode 100644
index 00000000000..eec3614f482
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_ktoonservative.c
@@ -0,0 +1,982 @@
+/*
+ *  drivers/cpufreq/cpufreq_ktoonservative.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(57)
+#define DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG	(58)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(52)
+#define DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG	(35)
+#define DEF_CPU_DOWN_BLOCK_CYCLES		(22)
+#define DEF_BOOST_CPU				(1134000)
+#define DEF_BOOST_CPU_TURN_ON_2ND_CORE		(1)
+#define DEF_BOOST_GPU				(450)
+#define DEF_BOOST_HOLD_CYCLES			(22)
+#define DEF_DISABLE_HOTPLUGGING			(0)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+static unsigned int stored_sampling_rate = 45000;
+static unsigned int Lcpu_down_block_cycles = 0;
+static unsigned int Lcpu_up_block_cycles = 0;
+static bool boostpulse_relayf = false;
+static int boost_hold_cycles_cnt = 0;
+static bool screen_is_on = true;
+
+//extern void ktoonservative_is_active(bool val);
+//extern void boost_the_gpu(int freq, int cycles);
+
+//extern void apenable_auto_hotplug(bool state);
+//extern bool apget_enable_auto_hotplug(void);
+//static bool prev_apenable;
+
+//extern void kt_is_active_benabled_gpio(bool val);
+//extern void kt_is_active_benabled_touchkey(bool val);
+//extern void kt_is_active_benabled_power(bool val);
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+struct work_struct hotplug_offline_work;
+struct work_struct hotplug_online_work;
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+	int cpu;
+	unsigned int enable:1;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int sampling_rate_screen_off;
+	unsigned int sampling_down_factor;
+	unsigned int up_threshold;
+	unsigned int up_threshold_hotplug;
+	unsigned int down_threshold;
+	unsigned int down_threshold_hotplug;
+	unsigned int cpu_down_block_cycles;
+	unsigned int boost_cpu;
+	unsigned int boost_turn_on_2nd_core;
+	unsigned int boost_gpu;
+	unsigned int boost_hold_cycles;
+	unsigned int disable_hotplugging;
+	unsigned int no_2nd_cpu_screen_off;
+	unsigned int ignore_nice;
+	unsigned int freq_step;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.up_threshold_hotplug = DEF_FREQUENCY_UP_THRESHOLD_HOTPLUG,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.down_threshold_hotplug = DEF_FREQUENCY_DOWN_THRESHOLD_HOTPLUG,
+	.cpu_down_block_cycles = DEF_CPU_DOWN_BLOCK_CYCLES,
+	.boost_cpu = DEF_BOOST_CPU,
+	.boost_turn_on_2nd_core = DEF_BOOST_CPU_TURN_ON_2ND_CORE,
+	.boost_gpu = DEF_BOOST_GPU,
+	.boost_hold_cycles = DEF_BOOST_HOLD_CYCLES,
+	.disable_hotplugging = DEF_DISABLE_HOTPLUGGING,
+	.no_2nd_cpu_screen_off = 1,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.sampling_rate_screen_off = 45000,
+	.ignore_nice = 0,
+	.freq_step = 5,
+};
+
+static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+						  cputime64_t *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu,
+					    cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+	if (idle_time == -1ULL)
+		idle_time = get_cpu_idle_time_jiffy(cpu, wall);
+
+	return idle_time;
+}
+
+/* keep track of frequency transitions */
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * we only care if our internally tracked freq moves outside
+	 * the 'valid' ranges of freqency available to us otherwise
+	 * we do not change it
+	*/
+	if (this_dbs_info->requested_freq > policy->max
+			|| this_dbs_info->requested_freq < policy->min)
+		this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+/************************** sysfs interface ************************/
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+define_one_global_ro(sampling_rate_min);
+
+static ssize_t show_boost_cpu(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", dbs_tuners_ins.boost_cpu / 1000);
+}
+
+/* cpufreq_ktoonservative Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_rate_screen_off, sampling_rate_screen_off);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(up_threshold_hotplug, up_threshold_hotplug);
+show_one(down_threshold, down_threshold);
+show_one(down_threshold_hotplug, down_threshold_hotplug);
+show_one(cpu_down_block_cycles, cpu_down_block_cycles);
+show_one(boost_turn_on_2nd_core, boost_turn_on_2nd_core);
+show_one(boost_gpu, boost_gpu);
+show_one(boost_hold_cycles, boost_hold_cycles);
+show_one(disable_hotplugging, disable_hotplugging);
+show_one(no_2nd_cpu_screen_off, no_2nd_cpu_screen_off);
+show_one(ignore_nice_load, ignore_nice);
+show_one(freq_step, freq_step);
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	stored_sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_sampling_rate_screen_off(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate_screen_off = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_up_threshold_hotplug(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold_hotplug = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold_hotplug(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold_hotplug = input;
+	return count;
+}
+
+static ssize_t store_cpu_down_block_cycles(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	/* cannot be lower than 11 otherwise freq will not fall */
+	if (input < 0)
+		return -EINVAL;
+
+	dbs_tuners_ins.cpu_down_block_cycles = input;
+	return count;
+}
+
+static ssize_t store_boost_cpu(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input * 1000 > 2106000)
+		input = 2106000;
+	if (input * 1000 < 0)
+		input = 0;
+	dbs_tuners_ins.boost_cpu = input * 1000;
+	return count;
+}
+
+static ssize_t store_boost_turn_on_2nd_core(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (input != 0 && input != 1)
+		input = 0;
+
+	dbs_tuners_ins.boost_turn_on_2nd_core = input;
+	return count;
+}
+
+static ssize_t store_boost_gpu(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (input != 100 && input != 160 && input != 266 && input != 350 && input != 400 && input != 450 && input != 533 && input != 612)
+		input = 0;
+
+	dbs_tuners_ins.boost_gpu = input;
+	return count;
+}
+
+static ssize_t store_boost_hold_cycles(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (input < 0)
+		return -EINVAL;
+
+	dbs_tuners_ins.boost_hold_cycles = input;
+	return count;
+}
+
+static ssize_t store_disable_hotplugging(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (input != 0 && input != 1)
+		input = 0;
+
+	dbs_tuners_ins.disable_hotplugging = input;
+	return count;
+}
+
+static ssize_t store_no_2nd_cpu_screen_off(struct kobject *a, struct attribute *b, const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (input != 0 && input != 1)
+		input = 0;
+
+	dbs_tuners_ins.no_2nd_cpu_screen_off = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice) /* nothing to do */
+		return count;
+
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	/* no need to test here if freq_step is zero as the user might actually
+	 * want this, they would be crazy though :) */
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_rate_screen_off);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold);
+define_one_global_rw(up_threshold_hotplug);
+define_one_global_rw(down_threshold);
+define_one_global_rw(down_threshold_hotplug);
+define_one_global_rw(cpu_down_block_cycles);
+define_one_global_rw(boost_cpu);
+define_one_global_rw(boost_turn_on_2nd_core);
+define_one_global_rw(boost_gpu);
+define_one_global_rw(boost_hold_cycles);
+define_one_global_rw(disable_hotplugging);
+define_one_global_rw(no_2nd_cpu_screen_off);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_rate_screen_off.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&up_threshold_hotplug.attr,
+	&down_threshold.attr,
+	&down_threshold_hotplug.attr,
+	&cpu_down_block_cycles.attr,
+	&boost_cpu.attr,
+	&boost_turn_on_2nd_core.attr,
+	&boost_gpu.attr,
+	&boost_hold_cycles.attr,
+	&disable_hotplugging.attr,
+	&no_2nd_cpu_screen_off.attr,
+	&ignore_nice_load.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "ktoonservative",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load = 0;
+	unsigned int max_load = 0;
+	unsigned int freq_target;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	policy = this_dbs_info->cur_policy;
+
+	if (boostpulse_relayf)
+	{
+		if (stored_sampling_rate != 0 && screen_is_on)
+			dbs_tuners_ins.sampling_rate = stored_sampling_rate;
+		if (boost_hold_cycles_cnt >= dbs_tuners_ins.boost_hold_cycles)
+		{
+			boostpulse_relayf = false;
+			boost_hold_cycles_cnt = 0;
+		}
+		boost_hold_cycles_cnt++;
+
+		this_dbs_info->down_skip = 0;
+		/* if we are already at full speed then break out early */
+		if (this_dbs_info->requested_freq == policy->max || policy->cur > dbs_tuners_ins.boost_cpu || this_dbs_info->requested_freq > dbs_tuners_ins.boost_cpu)
+			return;
+
+		this_dbs_info->requested_freq = dbs_tuners_ins.boost_cpu;
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+	
+	/*
+	 * Every sampling_rate, we check, if current idle time is less
+	 * than 20% (default), then we try to increase frequency
+	 * Every sampling_rate*sampling_down_factor, we check, if current
+	 * idle time is more than 80%, then we try to decrease frequency
+	 *
+	 * Any frequency increase takes it to the maximum frequency.
+	 * Frequency reduction happens at minimum steps of
+	 * 5% (default) of maximum frequency
+	 */
+
+	/* Get Absolute Load */
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+			(cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+			(cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] - j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (load > max_load)
+			max_load = load;
+	}
+
+	/*
+	 * break out if we 'cannot' reduce the speed as the user might
+	 * want freq_step to be zero
+	 */
+	if (dbs_tuners_ins.freq_step == 0)
+		return;
+
+	/* Check for frequency increase is greater than hotplug value */
+	if (max_load > dbs_tuners_ins.up_threshold_hotplug) {
+		if (num_online_cpus() < 2 && policy->cur != policy->min)
+		{
+			if (Lcpu_up_block_cycles > dbs_tuners_ins.cpu_down_block_cycles && (dbs_tuners_ins.no_2nd_cpu_screen_off == 0 || (dbs_tuners_ins.no_2nd_cpu_screen_off == 1 && screen_is_on)))
+			{
+				schedule_work_on(0, &hotplug_online_work);
+				Lcpu_up_block_cycles = 0;
+			}
+			Lcpu_up_block_cycles++;
+		}
+	}
+
+	/* Check for frequency increase */
+	if (max_load > dbs_tuners_ins.up_threshold) {
+		this_dbs_info->down_skip = 0;
+
+		/* if we are already at full speed then break out early */
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		/* max freq cannot be less than 100. But who knows.... */
+		if (unlikely(freq_target == 0))
+			freq_target = 5;
+
+		this_dbs_info->requested_freq += freq_target;
+		if (this_dbs_info->requested_freq > policy->max)
+			this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	if (max_load < dbs_tuners_ins.down_threshold_hotplug && !dbs_tuners_ins.disable_hotplugging) {
+		if (num_online_cpus() > 1)
+		{
+			if (Lcpu_down_block_cycles > dbs_tuners_ins.cpu_down_block_cycles)
+			{
+				schedule_work_on(0, &hotplug_offline_work);
+				Lcpu_down_block_cycles = 0;
+			}
+			Lcpu_down_block_cycles++;
+		}
+	}
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus 10 points under the threshold.
+	 */
+	if (max_load < (dbs_tuners_ins.down_threshold - 10)) {
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		this_dbs_info->requested_freq -= freq_target;
+		if (this_dbs_info->requested_freq < policy->min)
+			this_dbs_info->requested_freq = policy->min;
+
+		/*
+		 * if we cannot reduce the frequency anymore, break out early
+		 */
+		if (policy->cur == policy->min)
+			return;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+void screen_is_on_relay_kt(bool state)
+{
+	screen_is_on = state;
+	if (state == true)
+	{
+		if (stored_sampling_rate > 0)
+			dbs_tuners_ins.sampling_rate = stored_sampling_rate; //max(input, min_sampling_rate);
+		//pr_alert("SCREEN_IS_ON1: %d-%d\n", dbs_tuners_ins.sampling_rate, stored_sampling_rate);
+	}
+	else
+	{
+		stored_sampling_rate = dbs_tuners_ins.sampling_rate;
+		dbs_tuners_ins.sampling_rate = dbs_tuners_ins.sampling_rate_screen_off;
+		//pr_alert("SCREEN_IS_ON2: %d-%d\n", dbs_tuners_ins.sampling_rate, stored_sampling_rate);
+	}
+	
+}
+
+void boostpulse_relay(void)
+{
+	if (!boostpulse_relayf)
+	{
+		if (dbs_tuners_ins.boost_gpu > 0)
+		{
+			//int bpc = (dbs_tuners_ins.boost_hold_cycles / 2);
+			//if (dbs_tuners_ins.boost_hold_cycles > 0)
+				//boost_the_gpu(dbs_tuners_ins.boost_gpu, bpc);
+			//else
+				//boost_the_gpu(dbs_tuners_ins.boost_gpu, 0);
+		}
+		if (num_online_cpus() < 2 && dbs_tuners_ins.boost_turn_on_2nd_core)
+			schedule_work_on(0, &hotplug_online_work);
+		else if (dbs_tuners_ins.boost_turn_on_2nd_core == 0 && dbs_tuners_ins.boost_cpu == 0 && dbs_tuners_ins.boost_gpu == 0)
+			return;
+
+		boostpulse_relayf = true;
+		boost_hold_cycles_cnt = 0;
+		dbs_tuners_ins.sampling_rate = min_sampling_rate;
+		//pr_info("BOOSTPULSE RELAY KT");
+	}
+	else
+	{
+		if (dbs_tuners_ins.boost_gpu > 0)
+		{
+			//int bpc = (dbs_tuners_ins.boost_hold_cycles / 2);
+			//if (dbs_tuners_ins.boost_hold_cycles > 0)
+				//boost_the_gpu(dbs_tuners_ins.boost_gpu, bpc);
+			//else
+				//boost_the_gpu(dbs_tuners_ins.boost_gpu, 0);
+		}
+		boost_hold_cycles_cnt = 0;
+	}
+}
+
+static void __cpuinit hotplug_offline_work_fn(struct work_struct *work)
+{
+	int cpu;
+	//pr_info("ENTER OFFLINE");
+	for_each_online_cpu(cpu) {
+		if (likely(cpu_online(cpu) && (cpu))) {
+			cpu_down(cpu);
+			//pr_info("auto_hotplug: CPU%d down.\n", cpu);
+			break;
+		}
+	}
+}
+
+static void __cpuinit hotplug_online_work_fn(struct work_struct *work)
+{
+	int cpu;
+	//pr_info("ENTER ONLINE");
+	for_each_possible_cpu(cpu) {
+		if (likely(!cpu_online(cpu) && (cpu))) {
+			cpu_up(cpu);
+			//pr_info("auto_hotplug: CPU%d up.\n", cpu);
+			break;
+		}
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+	delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+	//	ktoonservative_is_active(true);		
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = (MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10)) / 20;
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate,
+					MIN_LATENCY_MULTIPLIER * latency);
+			dbs_tuners_ins.sampling_rate = 45000;
+				//max((min_sampling_rate * 20),
+				    //latency * LATENCY_MULTIPLIER);
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+	//	ktoonservative_is_active(false);		
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		/*
+		 * Stop the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 0)
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_ktoonservative = {
+	.name			= "ktoonservative",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	INIT_WORK(&hotplug_offline_work, hotplug_offline_work_fn);
+	INIT_WORK(&hotplug_online_work, hotplug_online_work_fn);
+	
+	return cpufreq_register_governor(&cpufreq_gov_ktoonservative);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_ktoonservative);
+}
+
+
+MODULE_AUTHOR("Alexander Clouter <alex@digriz.org.uk>");
+MODULE_DESCRIPTION("'cpufreq_ktoonservative' - A dynamic cpufreq governor for "
+		"Low Latency Frequency Transition capable processors "
+		"optimised for use in a battery environment");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_lionheart.c b/drivers/cpufreq/cpufreq_lionheart.c
new file mode 100644
index 00000000000..56a5ca069ac
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_lionheart.c
@@ -0,0 +1,553 @@
+/*
+ * drivers/cpufreq/cpufreq_lionheart.c
+ *
+ * Patched & tweaked: knzo
+ *
+ * Based on the Conservative governor by:
+ *
+ *    Copyright (C)  2001 Russell King
+ *              (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                        Jun Nakajima <jun.nakajima@intel.com>
+ *              (C)  2009 Alexander Clouter <alex@digriz.org.uk>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <asm/cputime.h>
+#include <linux/kthread.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/cpumask.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/input.h>
+#include <linux/workqueue.h>
+#include <linux/slab.h>
+#include <linux/earlysuspend.h>
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(65)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(30)
+#define MIN_SAMPLING_RATE_RATIO			(2)
+
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define DEF_SAMPLING_DOWN_FACTOR		(1)
+#define MAX_SAMPLING_DOWN_FACTOR		(10)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	cputime64_t prev_cpu_idle;
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+	int cpu;
+	unsigned int enable:1;
+
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cs_cpu_dbs_info);
+
+static unsigned int dbs_enable;	
+
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int sampling_down_factor;
+	unsigned int up_threshold;
+	unsigned int down_threshold;
+	unsigned int ignore_nice;
+	unsigned int freq_step;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.ignore_nice = 0,
+	.freq_step = 5,
+};
+
+static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,
+							cputime64_t *wall)
+{
+	cputime64_t idle_time;
+	cputime64_t cur_wall_time;
+	cputime64_t busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = (cur_wall_time - busy_time);
+	if (wall)
+		*wall = (cputime64_t)jiffies_to_usecs(cur_wall_time);
+
+	return (cputime64_t)jiffies_to_usecs(idle_time);
+}
+
+static inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, wall);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+
+	return idle_time;
+}
+
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,
+							freq->cpu);
+
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	policy = this_dbs_info->cur_policy;
+
+	if (this_dbs_info->requested_freq > policy->max
+			|| this_dbs_info->requested_freq < policy->min)
+		this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(down_threshold, down_threshold);
+show_one(ignore_nice_load, ignore_nice);
+show_one(freq_step, freq_step);
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > 100 ||
+			input <= dbs_tuners_ins.down_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.up_threshold = input;
+	return count;
+}
+
+static ssize_t store_down_threshold(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input < 11 || input > 100 ||
+			input >= dbs_tuners_ins.up_threshold)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_threshold = input;
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice)
+		return count;
+
+	dbs_tuners_ins.ignore_nice = input;
+
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 100)
+		input = 100;
+
+	dbs_tuners_ins.freq_step = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(up_threshold);
+define_one_global_rw(down_threshold);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(freq_step);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+	&ignore_nice_load.attr,
+	&freq_step.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "Lionheart",
+};
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int load = 0;
+	unsigned int max_load = 0;
+	unsigned int freq_target;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+
+	policy = this_dbs_info->cur_policy;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		cputime64_t cur_wall_time, cur_idle_time;
+		unsigned int idle_time, wall_time;
+
+		j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int) (cur_wall_time - j_dbs_info->prev_cpu_wall);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int) (cur_idle_time - j_dbs_info->prev_cpu_idle);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			cputime64_t cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = (kcpustat_cpu(j).cpustat[CPUTIME_NICE] - j_dbs_info->prev_cpu_nice);
+
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		if (load > max_load)
+			max_load = load;
+	}
+
+	if (dbs_tuners_ins.freq_step == 0)
+		return;
+
+	if (max_load > dbs_tuners_ins.up_threshold) {
+		this_dbs_info->down_skip = 0;
+
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		if (unlikely(freq_target == 0))
+			freq_target = 5;
+
+		this_dbs_info->requested_freq += freq_target;
+		if (this_dbs_info->requested_freq > policy->max)
+			this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	if (max_load < (dbs_tuners_ins.down_threshold - 10)) {
+		freq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;
+
+		this_dbs_info->requested_freq -= freq_target;
+		if (this_dbs_info->requested_freq < policy->min)
+			this_dbs_info->requested_freq = policy->min;
+
+		if (policy->cur == policy->min)
+			return;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	int delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	// delay -= jiffies % delay;
+
+	dbs_info->enable = 1;
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	dbs_info->enable = 0;
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cs_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		mutex_init(&this_dbs_info->timer_mutex);
+		dbs_enable++;
+
+		if (dbs_enable == 1) {
+			unsigned int latency;
+
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = 10000;
+			dbs_tuners_ins.sampling_rate = 10000;
+
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_lock(&dbs_mutex);
+		dbs_enable--;
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		if (dbs_enable == 0)
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+
+		mutex_unlock(&dbs_mutex);
+		if (!dbs_enable)
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART
+static
+#endif
+struct cpufreq_governor cpufreq_gov_lionheart = {
+	.name			= "Lionheart",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_lionheart);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_lionheart);
+}
+
+MODULE_AUTHOR("knzo");
+MODULE_DESCRIPTION("'cpufreq_lionheart' - A brave and agile conservative-based governor.");
+MODULE_LICENSE("GPL");
+
+fs_initcall(cpufreq_gov_dbs_init);
+module_exit(cpufreq_gov_dbs_exit);
+
+
+
diff --git a/drivers/cpufreq/cpufreq_nightmare.c b/drivers/cpufreq/cpufreq_nightmare.c
new file mode 100644
index 00000000000..974761f393a
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_nightmare.c
@@ -0,0 +1,871 @@
+/*
+ *  drivers/cpufreq/cpufreq_nightmare.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ * 
+ * Created by Alucard_24@xda
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+static void do_nightmare_timer(struct work_struct *work);
+static int cpufreq_governor_nightmare(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+static
+#endif
+struct cpufreq_governor cpufreq_gov_nightmare = {
+	.name                   = "nightmare",
+	.governor               = cpufreq_governor_nightmare,
+	.owner                  = THIS_MODULE,
+};
+
+struct cpufreq_nightmare_cpuinfo {
+	cputime64_t prev_cpu_wall;
+	cputime64_t prev_cpu_idle;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	struct cpufreq_policy *cur_policy;
+	int cpu;
+	unsigned int enable:1;
+	/*
+	 * mutex that serializes governor limit change with
+	 * do_nightmare_timer invocation. We do not want do_nightmare_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_nightmare_cpuinfo, od_nightmare_cpuinfo);
+
+static unsigned int nightmare_enable;	/* number of CPUs using this policy */
+/*
+ * nightmare_mutex protects nightmare_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(nightmare_mutex);
+
+/*static atomic_t min_freq_limit[NR_CPUS];
+static atomic_t max_freq_limit[NR_CPUS];*/
+
+/* nightmare tuners */
+static struct nightmare_tuners {
+	atomic_t sampling_rate;
+	atomic_t inc_cpu_load_at_min_freq;
+	atomic_t inc_cpu_load;
+	atomic_t dec_cpu_load;
+	atomic_t freq_for_responsiveness;
+	atomic_t freq_for_responsiveness_max;
+	atomic_t freq_up_brake_at_min_freq;
+	atomic_t freq_up_brake;
+	atomic_t freq_step_at_min_freq;
+	atomic_t freq_step;
+	atomic_t freq_step_dec;
+	atomic_t freq_step_dec_at_max_freq;
+#ifdef CONFIG_CPU_EXYNOS4210
+	atomic_t up_sf_step;
+	atomic_t down_sf_step;
+#endif
+} nightmare_tuners_ins = {
+	.sampling_rate = ATOMIC_INIT(60000),
+	.inc_cpu_load_at_min_freq = ATOMIC_INIT(60),
+	.inc_cpu_load = ATOMIC_INIT(70),
+	.dec_cpu_load = ATOMIC_INIT(50),
+#ifdef CONFIG_CPU_EXYNOS4210
+	.freq_for_responsiveness = ATOMIC_INIT(200000),
+	.freq_for_responsiveness_max = ATOMIC_INIT(1200000),
+#else
+	.freq_for_responsiveness = ATOMIC_INIT(540000),
+	.freq_for_responsiveness_max = ATOMIC_INIT(1890000),
+#endif
+	.freq_step_at_min_freq = ATOMIC_INIT(20),
+	.freq_step = ATOMIC_INIT(20),
+	.freq_up_brake_at_min_freq = ATOMIC_INIT(30),
+	.freq_up_brake = ATOMIC_INIT(30),
+	.freq_step_dec = ATOMIC_INIT(10),
+	.freq_step_dec_at_max_freq = ATOMIC_INIT(10),
+#ifdef CONFIG_CPU_EXYNOS4210
+	.up_sf_step = ATOMIC_INIT(0),
+	.down_sf_step = ATOMIC_INIT(0),
+#endif
+};
+
+/************************** sysfs interface ************************/
+
+/* cpufreq_nightmare Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", atomic_read(&nightmare_tuners_ins.object));		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(inc_cpu_load_at_min_freq, inc_cpu_load_at_min_freq);
+show_one(inc_cpu_load, inc_cpu_load);
+show_one(dec_cpu_load, dec_cpu_load);
+show_one(freq_for_responsiveness, freq_for_responsiveness);
+show_one(freq_for_responsiveness_max, freq_for_responsiveness_max);
+show_one(freq_step_at_min_freq, freq_step_at_min_freq);
+show_one(freq_step, freq_step);
+show_one(freq_up_brake_at_min_freq, freq_up_brake_at_min_freq);
+show_one(freq_up_brake, freq_up_brake);
+show_one(freq_step_dec, freq_step_dec);
+show_one(freq_step_dec_at_max_freq, freq_step_dec_at_max_freq);
+#ifdef CONFIG_CPU_EXYNOS4210
+show_one(up_sf_step, up_sf_step);
+show_one(down_sf_step, down_sf_step);
+#endif
+
+/*#define show_freqlimit_param(file_name, cpu)		\
+static ssize_t show_##file_name##_##cpu		\
+(struct kobject *kobj, struct attribute *attr, char *buf)		\
+{									\
+	return sprintf(buf, "%d\n", atomic_read(&file_name[cpu]));	\
+}
+
+#define store_freqlimit_param(file_name, cpu)		\
+static ssize_t store_##file_name##_##cpu		\
+(struct kobject *kobj, struct attribute *attr,				\
+	const char *buf, size_t count)					\
+{									\
+	unsigned int input;						\
+	int ret;							\
+	ret = sscanf(buf, "%d", &input);				\
+	if (ret != 1)							\
+		return -EINVAL;						\
+	if (input == atomic_read(&file_name[cpu])) {		\
+		return count;	\
+	}	\
+	atomic_set(&file_name[cpu], input);			\
+	return count;							\
+}*/
+
+/* min freq limit for awaking */
+/*show_freqlimit_param(min_freq_limit, 0);
+show_freqlimit_param(min_freq_limit, 1);
+#if NR_CPUS >= 4
+show_freqlimit_param(min_freq_limit, 2);
+show_freqlimit_param(min_freq_limit, 3);
+#endif*/
+/* max freq limit for awaking */
+/*show_freqlimit_param(max_freq_limit, 0);
+show_freqlimit_param(max_freq_limit, 1);
+#if NR_CPUS >= 4
+show_freqlimit_param(max_freq_limit, 2);
+show_freqlimit_param(max_freq_limit, 3);
+#endif*/
+/* min freq limit for awaking */
+/*store_freqlimit_param(min_freq_limit, 0);
+store_freqlimit_param(min_freq_limit, 1);
+#if NR_CPUS >= 4
+store_freqlimit_param(min_freq_limit, 2);
+store_freqlimit_param(min_freq_limit, 3);
+#endif*/
+/* max freq limit for awaking */
+/*store_freqlimit_param(max_freq_limit, 0);
+store_freqlimit_param(max_freq_limit, 1);
+#if NR_CPUS >= 4
+store_freqlimit_param(max_freq_limit, 2);
+store_freqlimit_param(max_freq_limit, 3);
+#endif
+define_one_global_rw(min_freq_limit_0);
+define_one_global_rw(min_freq_limit_1);
+#if NR_CPUS >= 4
+define_one_global_rw(min_freq_limit_2);
+define_one_global_rw(min_freq_limit_3);
+#endif
+define_one_global_rw(max_freq_limit_0);
+define_one_global_rw(max_freq_limit_1);
+#if NR_CPUS >= 4
+define_one_global_rw(max_freq_limit_2);
+define_one_global_rw(max_freq_limit_3);
+#endif*/
+
+/**
+ * update_sampling_rate - update sampling rate effective immediately if needed.
+ * @new_rate: new sampling rate
+ *
+ * If new rate is smaller than the old, simply updaing
+ * nightmare_tuners_ins.sampling_rate might not be appropriate. For example,
+ * if the original sampling_rate was 1 second and the requested new sampling
+ * rate is 10 ms because the user needs immediate reaction from ondemand
+ * governor, but not sure if higher frequency will be required or not,
+ * then, the governor may change the sampling rate too late; up to 1 second
+ * later. Thus, if we are reducing the sampling rate, we need to make the
+ * new value effective immediately.
+ */
+static void update_sampling_rate(unsigned int new_rate)
+{
+	int cpu;
+
+	atomic_set(&nightmare_tuners_ins.sampling_rate,new_rate);
+
+	get_online_cpus();
+	for_each_online_cpu(cpu) {
+		struct cpufreq_policy *policy;
+		struct cpufreq_nightmare_cpuinfo *nightmare_cpuinfo;
+		unsigned long next_sampling, appointed_at;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (!policy)
+			continue;
+		nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, policy->cpu);
+		cpufreq_cpu_put(policy);
+
+		mutex_lock(&nightmare_cpuinfo->timer_mutex);
+
+		if (!delayed_work_pending(&nightmare_cpuinfo->work)) {
+			mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+			continue;
+		}
+
+		next_sampling  = jiffies + usecs_to_jiffies(new_rate);
+		appointed_at = nightmare_cpuinfo->work.timer.expires;
+
+
+		if (time_before(next_sampling, appointed_at)) {
+
+			mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+			cancel_delayed_work_sync(&nightmare_cpuinfo->work);
+			mutex_lock(&nightmare_cpuinfo->timer_mutex);
+
+			#ifdef CONFIG_CPU_EXYNOS4210
+				mod_delayed_work_on(nightmare_cpuinfo->cpu, system_wq, &nightmare_cpuinfo->work, usecs_to_jiffies(new_rate));
+			#else
+				queue_delayed_work_on(nightmare_cpuinfo->cpu, system_wq, &nightmare_cpuinfo->work, usecs_to_jiffies(new_rate));
+			#endif
+		}
+		mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+	}
+	put_online_cpus();
+}
+
+/* sampling_rate */
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(input,10000);
+	
+	if (input == atomic_read(&nightmare_tuners_ins.sampling_rate))
+		return count;
+
+	update_sampling_rate(input);
+
+	return count;
+}
+
+/* inc_cpu_load_at_min_freq */
+static ssize_t store_inc_cpu_load_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1) {
+		return -EINVAL;
+	}
+
+	input = min(input,atomic_read(&nightmare_tuners_ins.inc_cpu_load));
+
+	if (input == atomic_read(&nightmare_tuners_ins.inc_cpu_load_at_min_freq))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.inc_cpu_load_at_min_freq,input);
+
+	return count;
+}
+
+/* inc_cpu_load */
+static ssize_t store_inc_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.inc_cpu_load))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.inc_cpu_load,input);
+
+	return count;
+}
+
+/* dec_cpu_load */
+static ssize_t store_dec_cpu_load(struct kobject *a, struct attribute *b,
+					const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,95),5);
+
+	if (input == atomic_read(&nightmare_tuners_ins.dec_cpu_load))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.dec_cpu_load,input);
+
+	return count;
+}
+
+/* freq_for_responsiveness */
+static ssize_t store_freq_for_responsiveness(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_for_responsiveness))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_for_responsiveness,input);
+
+	return count;
+}
+
+/* freq_for_responsiveness_max */
+static ssize_t store_freq_for_responsiveness_max(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_for_responsiveness_max))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_for_responsiveness_max,input);
+
+	return count;
+}
+
+/* freq_step_at_min_freq */
+static ssize_t store_freq_step_at_min_freq(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_at_min_freq))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_step_at_min_freq,input);
+
+	return count;
+}
+
+/* freq_step */
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+			       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.freq_step,input);
+
+	return count;
+}
+
+/* freq_up_brake_at_min_freq */
+static ssize_t store_freq_up_brake_at_min_freq(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_up_brake_at_min_freq)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_up_brake_at_min_freq,input);
+
+	return count;
+}
+
+/* freq_up_brake */
+static ssize_t store_freq_up_brake(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_up_brake)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_up_brake,input);
+
+	return count;
+}
+
+/* freq_step_dec */
+static ssize_t store_freq_step_dec(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_dec)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_step_dec,input);
+
+	return count;
+}
+
+/* freq_step_dec_at_max_freq */
+static ssize_t store_freq_step_dec_at_max_freq(struct kobject *a, struct attribute *b,
+				       const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,100),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.freq_step_dec_at_max_freq)) {/* nothing to do */
+		return count;
+	}
+
+	atomic_set(&nightmare_tuners_ins.freq_step_dec_at_max_freq,input);
+
+	return count;
+}
+#ifdef CONFIG_CPU_EXYNOS4210
+/* up_sf_step */
+static ssize_t store_up_sf_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,99),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.up_sf_step))
+		return count;
+
+	 atomic_set(&nightmare_tuners_ins.up_sf_step,input);
+
+	return count;
+}
+
+/* down_sf_step */
+static ssize_t store_down_sf_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	int input;
+	int ret;
+
+	ret = sscanf(buf, "%d", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	input = max(min(input,99),0);
+
+	if (input == atomic_read(&nightmare_tuners_ins.down_sf_step))
+		return count;
+
+	atomic_set(&nightmare_tuners_ins.down_sf_step,input);
+
+	return count;
+}
+#endif
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(inc_cpu_load_at_min_freq);
+define_one_global_rw(inc_cpu_load);
+define_one_global_rw(dec_cpu_load);
+define_one_global_rw(freq_for_responsiveness);
+define_one_global_rw(freq_for_responsiveness_max);
+define_one_global_rw(freq_step_at_min_freq);
+define_one_global_rw(freq_step);
+define_one_global_rw(freq_up_brake_at_min_freq);
+define_one_global_rw(freq_up_brake);
+define_one_global_rw(freq_step_dec);
+define_one_global_rw(freq_step_dec_at_max_freq);
+#ifdef CONFIG_CPU_EXYNOS4210
+define_one_global_rw(up_sf_step);
+define_one_global_rw(down_sf_step);
+#endif
+
+static struct attribute *nightmare_attributes[] = {
+	&sampling_rate.attr,
+	/*&min_freq_limit_0.attr,
+	&min_freq_limit_1.attr,
+#if NR_CPUS >= 4
+	&min_freq_limit_2.attr,
+	&min_freq_limit_3.attr,
+#endif
+	&max_freq_limit_0.attr,
+	&max_freq_limit_1.attr,
+#if NR_CPUS >= 4
+	&max_freq_limit_2.attr,
+	&max_freq_limit_3.attr,
+#endif*/
+	&inc_cpu_load_at_min_freq.attr,
+	&inc_cpu_load.attr,
+	&dec_cpu_load.attr,
+	&freq_for_responsiveness.attr,
+	&freq_for_responsiveness_max.attr,
+	&freq_step_at_min_freq.attr,
+	&freq_step.attr,
+	&freq_up_brake_at_min_freq.attr,
+	&freq_up_brake.attr,
+	&freq_step_dec.attr,
+	&freq_step_dec_at_max_freq.attr,
+#ifdef CONFIG_CPU_EXYNOS4210
+	&up_sf_step.attr,
+	&down_sf_step.attr,
+#endif
+	NULL
+};
+
+static struct attribute_group nightmare_attr_group = {
+	.attrs = nightmare_attributes,
+	.name = "nightmare",
+};
+
+/************************** sysfs end ************************/
+
+static void nightmare_check_cpu(struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo)
+{
+	struct cpufreq_policy *cpu_policy;
+	unsigned int min_freq;
+	unsigned int max_freq;
+#ifdef CONFIG_CPU_EXYNOS4210
+	int up_sf_step = atomic_read(&nightmare_tuners_ins.up_sf_step);
+	int down_sf_step = atomic_read(&nightmare_tuners_ins.down_sf_step);
+#endif
+	unsigned int freq_for_responsiveness;
+	unsigned int freq_for_responsiveness_max;
+	int dec_cpu_load;
+	int inc_cpu_load;
+	int freq_step;
+	int freq_up_brake;
+	int freq_step_dec;
+	cputime64_t cur_wall_time, cur_idle_time;
+	unsigned int wall_time, idle_time;
+	unsigned int index = 0;
+	unsigned int tmp_freq = 0;
+	unsigned int next_freq = 0;
+	int cur_load = -1;
+	unsigned int cpu;
+	
+	cpu = this_nightmare_cpuinfo->cpu;
+	cpu_policy = this_nightmare_cpuinfo->cur_policy;
+
+	cur_idle_time = get_cpu_idle_time_us(cpu, NULL);
+	cur_idle_time += get_cpu_iowait_time_us(cpu, &cur_wall_time);
+
+	wall_time = (unsigned int)
+			(cur_wall_time - this_nightmare_cpuinfo->prev_cpu_wall);
+	this_nightmare_cpuinfo->prev_cpu_wall = cur_wall_time;
+
+	idle_time = (unsigned int)
+			(cur_idle_time - this_nightmare_cpuinfo->prev_cpu_idle);
+	this_nightmare_cpuinfo->prev_cpu_idle = cur_idle_time;
+
+	/*min_freq = atomic_read(&min_freq_limit[cpu]);
+	max_freq = atomic_read(&max_freq_limit[cpu]);*/
+
+	freq_for_responsiveness = atomic_read(&nightmare_tuners_ins.freq_for_responsiveness);
+	freq_for_responsiveness_max = atomic_read(&nightmare_tuners_ins.freq_for_responsiveness_max);
+	dec_cpu_load = atomic_read(&nightmare_tuners_ins.dec_cpu_load);
+	inc_cpu_load = atomic_read(&nightmare_tuners_ins.inc_cpu_load);
+	freq_step = atomic_read(&nightmare_tuners_ins.freq_step);
+	freq_up_brake = atomic_read(&nightmare_tuners_ins.freq_up_brake);
+	freq_step_dec = atomic_read(&nightmare_tuners_ins.freq_step_dec);
+
+	if (!cpu_policy)
+		return;
+
+	/*printk(KERN_ERR "TIMER CPU[%u], wall[%u], idle[%u]\n",cpu, wall_time, idle_time);*/
+	if (wall_time >= idle_time) { /*if wall_time < idle_time, evaluate cpu load next time*/
+		cur_load = wall_time > idle_time ? (100 * (wall_time - idle_time)) / wall_time : 1;/*if wall_time is equal to idle_time cpu_load is equal to 1*/
+		tmp_freq = cpu_policy->cur;
+		/* Checking Frequency Limit */
+		/*if (max_freq > cpu_policy->max)
+			max_freq = cpu_policy->max;
+		if (min_freq < cpu_policy->min)
+			min_freq = cpu_policy->min;*/
+		min_freq = cpu_policy->min;
+		max_freq = cpu_policy->max;		
+		/* CPUs Online Scale Frequency*/
+		if (cpu_policy->cur < freq_for_responsiveness) {
+			inc_cpu_load = atomic_read(&nightmare_tuners_ins.inc_cpu_load_at_min_freq);
+			freq_step = atomic_read(&nightmare_tuners_ins.freq_step_at_min_freq);
+			freq_up_brake = atomic_read(&nightmare_tuners_ins.freq_up_brake_at_min_freq);
+		} else if (cpu_policy->cur > freq_for_responsiveness_max) {
+			freq_step_dec = atomic_read(&nightmare_tuners_ins.freq_step_dec_at_max_freq);
+		}		
+		/* Check for frequency increase or for frequency decrease */
+#ifdef CONFIG_CPU_EXYNOS4210
+		if (cur_load >= inc_cpu_load && cpu_policy->cur < max_freq) {
+			tmp_freq = max(min((cpu_policy->cur + ((cur_load + freq_step - freq_up_brake == 0 ? 1 : cur_load + freq_step - freq_up_brake) * 2000)), max_freq), min_freq);
+		} else if (cur_load < dec_cpu_load && cpu_policy->cur > min_freq) {
+			tmp_freq = max(min((cpu_policy->cur - ((100 - cur_load + freq_step_dec == 0 ? 1 : 100 - cur_load + freq_step_dec) * 2000)), max_freq), min_freq);
+		}
+		next_freq = (tmp_freq / 100000) * 100000;
+		if ((next_freq > cpu_policy->cur
+			&& (tmp_freq % 100000 > up_sf_step * 1000))
+			|| (next_freq < cpu_policy->cur
+			&& (tmp_freq % 100000 > down_sf_step * 1000))) {
+				next_freq += 100000;
+		}
+#else
+		if (cur_load >= inc_cpu_load && cpu_policy->cur < max_freq) {
+			tmp_freq = max(min((cpu_policy->cur + ((cur_load + freq_step - freq_up_brake == 0 ? 1 : cur_load + freq_step - freq_up_brake) * 3840)), max_freq), min_freq);
+		} else if (cur_load < dec_cpu_load && cpu_policy->cur > min_freq) {
+			tmp_freq = max(min((cpu_policy->cur - ((100 - cur_load + freq_step_dec == 0 ? 1 : 100 - cur_load + freq_step_dec) * 3840)), max_freq), min_freq);
+		}
+		cpufreq_frequency_table_target(cpu_policy, this_nightmare_cpuinfo->freq_table, tmp_freq,
+			CPUFREQ_RELATION_H, &index);
+		if (this_nightmare_cpuinfo->freq_table[index].frequency != cpu_policy->cur) {
+			cpufreq_frequency_table_target(cpu_policy, this_nightmare_cpuinfo->freq_table, tmp_freq,
+				CPUFREQ_RELATION_L, &index);
+		}
+	 	next_freq = this_nightmare_cpuinfo->freq_table[index].frequency;
+#endif
+		/*printk(KERN_ERR "FREQ CALC.: CPU[%u], load[%d], target freq[%u], cur freq[%u], min freq[%u], max_freq[%u]\n",cpu, cur_load, next_freq, cpu_policy->cur, cpu_policy->min, max_freq);*/
+		if (next_freq != cpu_policy->cur && cpu_online(cpu)) {
+			__cpufreq_driver_target(cpu_policy, next_freq, CPUFREQ_RELATION_L);
+		}
+	}
+
+}
+
+static void do_nightmare_timer(struct work_struct *work)
+{
+	struct cpufreq_nightmare_cpuinfo *nightmare_cpuinfo;
+	int delay;
+	unsigned int cpu;
+
+	nightmare_cpuinfo = container_of(work, struct cpufreq_nightmare_cpuinfo, work.work);
+	cpu = nightmare_cpuinfo->cpu;
+
+	mutex_lock(&nightmare_cpuinfo->timer_mutex);
+	nightmare_check_cpu(nightmare_cpuinfo);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	delay = usecs_to_jiffies(atomic_read(&nightmare_tuners_ins.sampling_rate));
+	if (num_online_cpus() > 1) {
+		delay -= jiffies % delay;
+	}
+
+#ifdef CONFIG_CPU_EXYNOS4210
+	mod_delayed_work_on(cpu, system_wq, &nightmare_cpuinfo->work, delay);
+#else
+	queue_delayed_work_on(cpu, system_wq, &nightmare_cpuinfo->work, delay);
+#endif
+	mutex_unlock(&nightmare_cpuinfo->timer_mutex);
+}
+
+static int cpufreq_governor_nightmare(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu;
+	struct cpufreq_nightmare_cpuinfo *this_nightmare_cpuinfo;
+	int rc, delay;
+
+	cpu = policy->cpu;
+	this_nightmare_cpuinfo = &per_cpu(od_nightmare_cpuinfo, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&nightmare_mutex);
+
+		this_nightmare_cpuinfo->cur_policy = policy;
+
+		this_nightmare_cpuinfo->prev_cpu_idle = get_cpu_idle_time_us(cpu, NULL);
+		this_nightmare_cpuinfo->prev_cpu_idle += get_cpu_iowait_time_us(cpu, &this_nightmare_cpuinfo->prev_cpu_wall);
+
+		this_nightmare_cpuinfo->freq_table = cpufreq_frequency_get_table(cpu);
+		this_nightmare_cpuinfo->cpu = cpu;
+
+		mutex_init(&this_nightmare_cpuinfo->timer_mutex);
+
+		nightmare_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (nightmare_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&nightmare_attr_group);
+			if (rc) {
+				mutex_unlock(&nightmare_mutex);
+				return rc;
+			}
+		}
+
+		/*if (atomic_read(&min_freq_limit[cpu]) == 0)
+			atomic_set(&min_freq_limit[cpu], policy->min);
+
+		if (atomic_read(&max_freq_limit[cpu]) == 0)
+			atomic_set(&max_freq_limit[cpu], policy->max);*/
+
+		mutex_unlock(&nightmare_mutex);
+
+		delay=usecs_to_jiffies(atomic_read(&nightmare_tuners_ins.sampling_rate));
+		if (num_online_cpus() > 1) {
+			delay -= jiffies % delay;
+		}
+
+		this_nightmare_cpuinfo->enable = 1;
+#ifdef CONFIG_CPU_EXYNOS4210
+		INIT_DEFERRABLE_WORK(&this_nightmare_cpuinfo->work, do_nightmare_timer);
+		mod_delayed_work_on(this_nightmare_cpuinfo->cpu, system_wq, &this_nightmare_cpuinfo->work, delay);
+#else
+		INIT_DELAYED_WORK_DEFERRABLE(&this_nightmare_cpuinfo->work, do_nightmare_timer);
+		queue_delayed_work_on(this_nightmare_cpuinfo->cpu, system_wq, &this_nightmare_cpuinfo->work, delay);
+#endif
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_nightmare_cpuinfo->enable = 0;
+		cancel_delayed_work_sync(&this_nightmare_cpuinfo->work);
+
+		mutex_lock(&nightmare_mutex);
+		nightmare_enable--;
+		mutex_destroy(&this_nightmare_cpuinfo->timer_mutex);
+
+		if (!nightmare_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &nightmare_attr_group);			
+		}
+		mutex_unlock(&nightmare_mutex);
+		
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_nightmare_cpuinfo->timer_mutex);
+		if (policy->max < this_nightmare_cpuinfo->cur_policy->cur)
+			__cpufreq_driver_target(this_nightmare_cpuinfo->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_nightmare_cpuinfo->cur_policy->cur)
+			__cpufreq_driver_target(this_nightmare_cpuinfo->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		mutex_unlock(&this_nightmare_cpuinfo->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_nightmare_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_nightmare);
+}
+
+static void __exit cpufreq_gov_nightmare_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_nightmare);
+}
+
+MODULE_AUTHOR("Alucard24@XDA");
+MODULE_DESCRIPTION("'cpufreq_nightmare' - A dynamic cpufreq/cpuhotplug governor v4.1 (SnapDragon)");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE
+fs_initcall(cpufreq_gov_nightmare_init);
+#else
+module_init(cpufreq_gov_nightmare_init);
+#endif
+module_exit(cpufreq_gov_nightmare_exit);
+
diff --git a/drivers/cpufreq/cpufreq_ondemandplus.c b/drivers/cpufreq/cpufreq_ondemandplus.c
new file mode 100644
index 00000000000..25f222bd2a5
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_ondemandplus.c
@@ -0,0 +1,982 @@
+/*
+ * drivers/cpufreq/cpufreq_ondemandplus.c
+ * Copyright (C) 2013 Boy Petersen
+ * Copyright (C) 2014 Kyriacos Elpidorou [for the Note 4 Adaptation]
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *
+ * based upon:
+ *
+ *
+ *         drivers/cpufreq/cpufreq_interactive.c
+ *
+ *         Copyright (C) 2010 Google, Inc.
+ *
+ *         This software is licensed under the terms of the GNU General Public
+ *         License version 2, as published by the Free Software Foundation, and
+ *         may be copied, distributed, and modified under those terms.
+ *
+ *         This program is distributed in the hope that it will be useful,
+ *         but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *         MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *         GNU General Public License for more details.
+ *
+ *         Author: Mike Chan (mike@android.com)
+ *
+ *
+ * and:
+ *
+ *         drivers/cpufreq/cpufreq_ondemand.c
+ *
+ *         Copyright (C)  2001 Russell King
+ *                     (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                                 Jun Nakajima <jun.nakajima@intel.com>
+ *
+ *         This program is free software; you can redistribute it and/or modify
+ *         it under the terms of the GNU General Public License version 2 as
+ *         published by the Free Software Foundation.
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/time.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+#include <asm/cputime.h>
+#include <linux/module.h>
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/cpufreq_ondemandplus.h>
+
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct cpufreq_ondemandplus_cpuinfo {
+        struct timer_list cpu_timer;
+        int timer_idlecancel;
+        u64 time_in_idle;
+        u64 idle_exit_time;
+        u64 timer_run_time;
+        int idling;
+        u64 target_set_time;
+        u64 target_set_time_in_idle;
+        struct cpufreq_policy *policy;
+        struct cpufreq_frequency_table *freq_table;
+        unsigned int target_freq;
+        int governor_enabled;
+};
+
+static DEFINE_PER_CPU(struct cpufreq_ondemandplus_cpuinfo, cpuinfo);
+
+/* realtime thread handles frequency scaling */
+static struct task_struct *speedchange_task;
+static cpumask_t speedchange_cpumask;
+static spinlock_t speedchange_cpumask_lock;
+
+/*
+ * Tunables start
+ */
+
+#define DEFAULT_TIMER_RATE (20 * USEC_PER_MSEC)
+static unsigned long timer_rate;
+
+#define DEFAULT_UP_THRESHOLD 70
+static unsigned long up_threshold;
+
+#define DEFAULT_DOWN_DIFFERENTIAL 20
+static unsigned long down_differential;
+
+#define DEFAULT_MIN_FREQ 300000
+static u64 allowed_min;
+
+#define DEFAULT_MAX_FREQ 2649600
+static u64 allowed_max;
+
+#define DEFAULT_INTER_HIFREQ 2342400
+static u64 inter_hifreq;
+
+#define DEFAULT_INTER_LOFREQ 300000
+static u64 inter_lofreq;
+
+#define SUSPEND_FREQ 300000
+static u64 suspend_frequency;
+
+#define DEFAULT_INTER_STAYCYCLES 2
+static unsigned long inter_staycycles;
+
+#define DEFAULT_STAYCYCLES_RESETFREQ 652800
+static u64 staycycles_resetfreq;
+
+#define DEFAULT_IO_IS_BUSY 0
+static unsigned int io_is_busy;
+
+/*
+ * Tunables end
+ */
+
+static int cpufreq_governor_ondemandplus(struct cpufreq_policy *policy,
+                unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+static
+#endif
+struct cpufreq_governor cpufreq_gov_ondemandplus = {
+        .name = "ondemandplus",
+        .governor = cpufreq_governor_ondemandplus,
+        .max_transition_latency = 10000000,
+        .owner = THIS_MODULE,
+};
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, bool io_is_busy)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+static void cpufreq_ondemandplus_timer(unsigned long data)
+{
+        unsigned int delta_idle;
+        unsigned int delta_time;
+        int cpu_load;
+        unsigned int load_freq;
+        int load_since_change;
+        u64 time_in_idle;
+        u64 idle_exit_time;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, data);
+        u64 now_idle;
+        unsigned int new_freq;
+        unsigned int index;
+        static unsigned int stay_counter;
+        unsigned long flags;
+
+        smp_rmb();
+
+        if (!pcpu->governor_enabled)
+                goto exit;
+
+        /*
+         * Once pcpu->timer_run_time is updated to >= pcpu->idle_exit_time,
+         * this lets idle exit know the current idle time sample has
+         * been processed, and idle exit can generate a new sample and
+         * re-arm the timer.  This prevents a concurrent idle
+         * exit on that CPU from writing a new set of info at the same time
+         * the timer function runs (the timer function can't use that info
+         * until more time passes).
+         */
+
+        time_in_idle = pcpu->time_in_idle;
+        idle_exit_time = pcpu->idle_exit_time;
+        now_idle = get_cpu_idle_time(data, &pcpu->timer_run_time, 0);
+        smp_wmb();
+
+        /* If we raced with cancelling a timer, skip. */
+        if (!idle_exit_time)
+                goto exit;
+
+        delta_idle = (unsigned int) (now_idle - time_in_idle);
+        delta_time = (unsigned int) (pcpu->timer_run_time - idle_exit_time);
+
+        /*
+         * If timer ran less than 1ms after short-term sample started, retry.
+         */
+        if (delta_time < 1000)
+                goto rearm;
+
+        if (delta_idle > delta_time)
+                cpu_load = 0;
+        else
+                cpu_load = 100 * (delta_time - delta_idle) / delta_time;
+
+        delta_idle = (unsigned int) (now_idle -        pcpu->target_set_time_in_idle);
+        delta_time = (unsigned int) (pcpu->timer_run_time - pcpu->target_set_time);
+
+        if ((delta_time == 0) || (delta_idle > delta_time))
+                load_since_change = 0;
+        else
+                load_since_change =
+                        100 * (delta_time - delta_idle) / delta_time;
+
+        /*
+         * If short-term load (since last idle timer started or
+         * timer function re-armed itself) is higher than long-term 
+         * load (since last frequency change), use short-term load
+         * to be able to scale up quickly.
+         * When long-term load is higher than short-term load, 
+         * use the average of short-term load and long-term load
+         * (instead of just long-term load) to be able to scale
+         * down faster, with the long-term load being able to delay 
+         * down scaling a little to maintain responsiveness.
+         */
+        if (load_since_change > cpu_load) {
+                cpu_load = (cpu_load + load_since_change) / 2;
+        }
+
+        load_freq = cpu_load * pcpu->target_freq;
+
+        new_freq = pcpu->target_freq;
+
+        /* suspended scaling behavior */
+        if (allowed_max == suspend_frequency) {
+                if (stay_counter) {
+                        stay_counter = 0;
+                }
+                
+                /* Check for frequency increase */
+                if (load_freq > up_threshold * pcpu->target_freq) {
+                        /* if we are already at full speed then break out early */
+                        if (pcpu->target_freq < suspend_frequency) {
+                                
+                                new_freq = pcpu->target_freq + pcpu->policy->max / 10;
+
+                                if (new_freq > suspend_frequency) {
+                                        new_freq = suspend_frequency;
+                                }
+                                
+                                cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, new_freq,
+                                                CPUFREQ_RELATION_L, &index);
+                                
+                                new_freq = pcpu->freq_table[index].frequency;
+                        }
+
+                /* Check for frequency decrease */
+
+                /*
+                * The optimal frequency is the frequency that is the lowest that
+                * can support the current CPU usage without triggering the up
+                * policy. To be safe, we focus 10 points under the threshold.
+                */
+                } else if (load_freq < (up_threshold - down_differential) *
+                                pcpu->target_freq) {
+                        /* if we are already at full speed then break out early */
+                        if (pcpu->target_freq != pcpu->policy->min) {
+
+                                new_freq = pcpu->target_freq - pcpu->policy->max / 10;
+
+                                if (new_freq < pcpu->policy->min) {
+                                        new_freq = pcpu->policy->min;
+                                }
+                        
+                                cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, new_freq,
+                                                CPUFREQ_RELATION_H, &index);
+                                
+                                new_freq = pcpu->freq_table[index].frequency;
+                        }
+                }
+        /* screen-on scaling behavior */
+        } else {
+                /* Check for frequency increase */
+                if (load_freq > up_threshold * pcpu->target_freq) {
+                        /* if we are already at full speed then break out early */
+                        if (pcpu->target_freq < pcpu->policy->max) {
+
+                                if (stay_counter == 0 && inter_staycycles != 0) {
+                                        new_freq = inter_lofreq;
+                                        stay_counter++;
+                                } else if (stay_counter == 1 && inter_staycycles != 1) {
+                                        new_freq = inter_hifreq;
+                                        stay_counter++;
+                                } else if (stay_counter < inter_staycycles) {
+                                        stay_counter++;
+                                        goto rearm;
+                                } else {
+                                        new_freq = pcpu->policy->max;
+                                }
+                        }
+                }
+
+                /* Check for frequency decrease */
+
+                /*
+                * The optimal frequency is the frequency that is the lowest that
+                * can support the current CPU usage without triggering the up
+                * policy. To be safe, we focus 10 points under the threshold.
+                */
+                if (load_freq < (up_threshold - down_differential) *
+                                pcpu->target_freq) {
+                        
+                        if (pcpu->target_freq != allowed_min) {
+                                new_freq = load_freq /
+                                                (up_threshold - down_differential);
+
+                                if (new_freq <= staycycles_resetfreq) {
+                                        stay_counter = 0;
+                                }
+
+                                if (new_freq < allowed_min) {
+                                        new_freq = allowed_min;
+                                }
+                        }
+                } else if (pcpu->target_freq == pcpu->policy->max && 
+                                load_freq < (up_threshold - down_differential / 2) * 
+                                pcpu->target_freq) {
+                        new_freq = load_freq / (up_threshold - down_differential * 2 / 3);
+                }
+
+        }
+
+        if (cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table,
+                                           new_freq, CPUFREQ_RELATION_H,
+                                           &index)) {
+                pr_warn_once("timer %d: cpufreq_frequency_table_target error\n",
+                             (int) data);
+                goto rearm;
+        }
+
+        new_freq = pcpu->freq_table[index].frequency;        
+
+        if (pcpu->target_freq == new_freq) {
+                trace_cpufreq_ondemandplus_already(data, cpu_load,
+                                                  pcpu->target_freq, new_freq);
+                goto rearm_if_notmax;
+        }
+
+        trace_cpufreq_ondemandplus_target(data, cpu_load, pcpu->target_freq,
+                                         new_freq);
+        pcpu->target_set_time_in_idle = now_idle;
+        pcpu->target_set_time = pcpu->timer_run_time;
+
+        pcpu->target_freq = new_freq;
+        spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+        cpumask_set_cpu(data, &speedchange_cpumask);
+        spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+        wake_up_process(speedchange_task);
+
+rearm_if_notmax:
+        /*
+         * Already set max speed and don't see a need to change that,
+         * wait until next idle to re-evaluate, don't need timer.
+         */
+        if (pcpu->target_freq == pcpu->policy->max)
+                goto exit;
+
+rearm:
+        if (!timer_pending(&pcpu->cpu_timer)) {
+                /*
+                 * If already at min: if that CPU is idle, don't set timer.
+                 * Else cancel the timer if that CPU goes idle.  We don't
+                 * need to re-evaluate speed until the next idle exit.
+                 */
+                 
+                unsigned int cur_min_policy;
+                if (allowed_max == suspend_frequency) {
+                        cur_min_policy = pcpu->policy->min;
+                } else {
+                        cur_min_policy = allowed_min;
+                }
+                
+                if (pcpu->target_freq == cur_min_policy) {
+                        smp_rmb();
+
+                        if (pcpu->idling)
+                                goto exit;
+
+                        pcpu->timer_idlecancel = 1;
+                }
+
+                pcpu->time_in_idle = get_cpu_idle_time(
+                        data, &pcpu->idle_exit_time, 0);
+                mod_timer(&pcpu->cpu_timer,
+                        jiffies + usecs_to_jiffies(timer_rate));
+        }
+
+exit:
+        return;
+}
+
+static void cpufreq_ondemandplus_idle_start(void)
+{
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, smp_processor_id());
+        int pending;
+
+        if (!pcpu->governor_enabled)
+                return;
+
+        pcpu->idling = 1;
+        smp_wmb();
+        pending = timer_pending(&pcpu->cpu_timer);
+
+        if (pcpu->target_freq != pcpu->policy->min) {
+#ifdef CONFIG_SMP
+                /*
+                 * Entering idle while not at lowest speed.  On some
+                 * platforms this can hold the other CPU(s) at that speed
+                 * even though the CPU is idle. Set a timer to re-evaluate
+                 * speed so this idle CPU doesn't hold the other CPUs above
+                 * min indefinitely.  This should probably be a quirk of
+                 * the CPUFreq driver.
+                 */
+                if (!pending) {
+                        pcpu->time_in_idle = get_cpu_idle_time(
+                                smp_processor_id(), &pcpu->idle_exit_time, 0);
+                        pcpu->timer_idlecancel = 0;
+                        mod_timer(&pcpu->cpu_timer,
+                                  jiffies + usecs_to_jiffies(timer_rate));
+                }
+#endif
+        } else {
+                /*
+                 * If at min speed and entering idle after load has
+                 * already been evaluated, and a timer has been set just in
+                 * case the CPU suddenly goes busy, cancel that timer.  The
+                 * CPU didn't go busy; we'll recheck things upon idle exit.
+                 */
+                if (pending && pcpu->timer_idlecancel) {
+                        del_timer(&pcpu->cpu_timer);
+                        /*
+                         * Ensure last timer run time is after current idle
+                         * sample start time, so next idle exit will always
+                         * start a new idle sampling period.
+                         */
+                        pcpu->idle_exit_time = 0;
+                        pcpu->timer_idlecancel = 0;
+                }
+        }
+
+}
+
+static void cpufreq_ondemandplus_idle_end(void)
+{
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, smp_processor_id());
+
+        pcpu->idling = 0;
+        smp_wmb();
+
+        /*
+         * Arm the timer for 1-2 ticks later if not already, and if the timer
+         * function has already processed the previous load sampling
+         * interval.  (If the timer is not pending but has not processed
+         * the previous interval, it is probably racing with us on another
+         * CPU.  Let it compute load based on the previous sample and then
+         * re-arm the timer for another interval when it's done, rather
+         * than updating the interval start time to be "now", which doesn't
+         * give the timer function enough time to make a decision on this
+         * run.)
+         */
+        if (timer_pending(&pcpu->cpu_timer) == 0 &&
+            pcpu->timer_run_time >= pcpu->idle_exit_time &&
+            pcpu->governor_enabled) {
+                pcpu->time_in_idle =
+                        get_cpu_idle_time(smp_processor_id(),
+                                             &pcpu->idle_exit_time, 0);
+                pcpu->timer_idlecancel = 0;
+                mod_timer(&pcpu->cpu_timer,
+                          jiffies + usecs_to_jiffies(timer_rate));
+        }
+
+}
+
+static int cpufreq_ondemandplus_speedchange_task(void *data)
+{
+        unsigned int cpu;
+        cpumask_t tmp_mask;
+        unsigned long flags;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu;
+
+        while (1) {
+                set_current_state(TASK_INTERRUPTIBLE);
+                spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+
+                if (cpumask_empty(&speedchange_cpumask)) {
+                        spin_unlock_irqrestore(&speedchange_cpumask_lock,
+                                                flags);
+                        schedule();
+
+                        if (kthread_should_stop())
+                                break;
+
+                        spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+                }
+
+                set_current_state(TASK_RUNNING);
+                tmp_mask = speedchange_cpumask;
+                cpumask_clear(&speedchange_cpumask);
+                spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+
+                for_each_cpu(cpu, &tmp_mask) {
+                        unsigned int j;
+                        unsigned int max_freq = 0;
+
+                        pcpu = &per_cpu(cpuinfo, cpu);
+                        smp_rmb();
+
+                        if (!pcpu->governor_enabled)
+                                continue;
+
+                        for_each_cpu(j, pcpu->policy->cpus) {
+                                struct cpufreq_ondemandplus_cpuinfo *pjcpu =
+                                        &per_cpu(cpuinfo, j);
+
+                                if (pjcpu->target_freq > max_freq)
+                                        max_freq = pjcpu->target_freq;
+                        }
+
+                        if (max_freq != pcpu->policy->cur)
+                                __cpufreq_driver_target(pcpu->policy,
+                                                        max_freq,
+                                                        CPUFREQ_RELATION_H);
+                        /*trace_cpufreq_ondemandplus_setspeed(cpu,
+                                                pcpu->target_freq,
+                                                pcpu->policy->cur);
+			*/
+                }
+        }
+
+        return 0;
+}
+
+static ssize_t show_timer_rate(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%lu\n", timer_rate);
+}
+
+static ssize_t store_timer_rate(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = strict_strtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+
+        timer_rate = val;
+        return count;
+}
+
+static struct global_attr timer_rate_attr = __ATTR(timer_rate, 0644,
+                show_timer_rate, store_timer_rate);
+        
+static ssize_t show_up_threshold(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%lu\n", up_threshold);
+}
+
+static ssize_t store_up_threshold(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = strict_strtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+                
+        if (val > 100)
+                val = 100;
+
+        if (val < 1)
+                val = 1;
+                
+        up_threshold = val;
+        return count;
+}
+
+static struct global_attr up_threshold_attr = __ATTR(up_threshold, 0644,
+                show_up_threshold, store_up_threshold);
+                
+static ssize_t show_down_differential(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%lu\n", down_differential);
+}
+
+static ssize_t store_down_differential(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = strict_strtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+
+        if (val > 100)
+                val = 100;
+
+        down_differential = val;
+        return count;
+}
+
+static struct global_attr down_differential_attr = __ATTR(down_differential, 0644,
+                show_down_differential, store_down_differential);
+                
+static ssize_t show_inter_hifreq(struct kobject *kobj,
+                                 struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%llu\n", inter_hifreq);
+}
+
+static ssize_t store_inter_hifreq(struct kobject *kobj,
+                                  struct attribute *attr, const char *buf,
+                                  size_t count)
+{
+        int ret;
+        u64 val;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, smp_processor_id());
+        unsigned int index;
+
+        ret = strict_strtoull(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+        
+        index = 0;
+        cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
+                CPUFREQ_RELATION_L, &index);
+        val = pcpu->freq_table[index].frequency;
+
+        if (val > pcpu->policy->max)
+                val = pcpu->policy->max;
+
+        if (val < allowed_min)
+                val = allowed_min;
+
+        inter_hifreq = val;
+        return count;
+}
+
+static struct global_attr inter_hifreq_attr = __ATTR(inter_hifreq, 0644,
+                show_inter_hifreq, store_inter_hifreq);
+                
+static ssize_t show_inter_lofreq(struct kobject *kobj,
+                                 struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%llu\n", inter_lofreq);
+}
+
+static ssize_t store_inter_lofreq(struct kobject *kobj,
+                                  struct attribute *attr, const char *buf,
+                                  size_t count)
+{
+        int ret;
+        u64 val;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, smp_processor_id());
+        unsigned int index;
+
+        ret = strict_strtoull(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+
+        index = 0;
+        cpufreq_frequency_table_target(pcpu->policy, pcpu->freq_table, val,
+                        CPUFREQ_RELATION_H, &index);
+        val = pcpu->freq_table[index].frequency;
+
+        if (val > pcpu->policy->max)
+                val = pcpu->policy->max;
+
+        if (val < allowed_min)
+                val = allowed_min;
+        
+        inter_lofreq = val;
+        return count;
+}
+
+static struct global_attr inter_lofreq_attr = __ATTR(inter_lofreq, 0644,
+                show_inter_lofreq, store_inter_lofreq);
+
+static ssize_t show_inter_staycycles(struct kobject *kobj,
+                                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%lu\n", inter_staycycles);
+}
+
+static ssize_t store_inter_staycycles(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = strict_strtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+                
+        if (val > 10)
+                val = 10;
+                
+        inter_staycycles = val;
+        return count;
+}
+
+static struct global_attr inter_staycycles_attr = __ATTR(inter_staycycles, 0644,
+                show_inter_staycycles, store_inter_staycycles);
+                
+static ssize_t show_staycycles_resetfreq(struct kobject *kobj,
+                                 struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%llu\n", staycycles_resetfreq);
+}
+
+static ssize_t store_staycycles_resetfreq(struct kobject *kobj,
+                                  struct attribute *attr, const char *buf,
+                                  size_t count)
+{
+        int ret;
+        u64 val;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu =
+                &per_cpu(cpuinfo, smp_processor_id());
+
+        ret = strict_strtoull(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+                
+        if (val > pcpu->policy->max)
+                val = pcpu->policy->max;
+
+        if (val < allowed_min)
+                val = allowed_min;
+
+        staycycles_resetfreq = val;
+        return count;
+}
+
+static struct global_attr staycycles_resetfreq_attr = __ATTR(staycycles_resetfreq, 0644,
+                show_staycycles_resetfreq, store_staycycles_resetfreq);
+
+static ssize_t show_io_is_busy(struct kobject *kobj,
+                        struct attribute *attr, char *buf)
+{
+        return sprintf(buf, "%u\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *kobj,
+                        struct attribute *attr, const char *buf, size_t count)
+{
+        int ret;
+        unsigned long val;
+
+        ret = kstrtoul(buf, 0, &val);
+        if (ret < 0)
+                return ret;
+        io_is_busy = val;
+        return count;
+}
+
+static struct global_attr io_is_busy_attr = __ATTR(io_is_busy, 0644,
+                show_io_is_busy, store_io_is_busy);
+
+static struct attribute *ondemandplus_attributes[] = {
+        &timer_rate_attr.attr,
+        &up_threshold_attr.attr,
+        &down_differential_attr.attr,
+        &inter_hifreq_attr.attr,
+        &inter_lofreq_attr.attr,
+        &inter_staycycles_attr.attr,
+        &staycycles_resetfreq_attr.attr,
+        &io_is_busy_attr.attr,
+        NULL,
+};
+
+static struct attribute_group ondemandplus_attr_group = {
+        .attrs = ondemandplus_attributes,
+        .name = "ondemandplus",
+};
+
+static int cpufreq_ondemandplus_idle_notifier(struct notifier_block *nb,
+                                             unsigned long val,
+                                             void *data)
+{
+        switch (val) {
+        case IDLE_START:
+                cpufreq_ondemandplus_idle_start();
+                break;
+        case IDLE_END:
+                cpufreq_ondemandplus_idle_end();
+                break;
+        }
+
+        return 0;
+}
+
+static struct notifier_block cpufreq_ondemandplus_idle_nb = {
+        .notifier_call = cpufreq_ondemandplus_idle_notifier,
+};
+
+static int cpufreq_governor_ondemandplus(struct cpufreq_policy *policy,
+                unsigned int event)
+{
+        int rc;
+        unsigned int j;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu;
+        struct cpufreq_frequency_table *freq_table;
+
+        switch (event) {
+        case CPUFREQ_GOV_START:
+                if (!cpu_online(policy->cpu))
+                        return -EINVAL;
+
+                freq_table =
+                        cpufreq_frequency_get_table(policy->cpu);
+
+                for_each_cpu(j, policy->cpus) {
+                        pcpu = &per_cpu(cpuinfo, j);
+                        pcpu->policy = policy;
+                        pcpu->target_freq = policy->cur;
+                        pcpu->freq_table = freq_table;
+                        pcpu->target_set_time_in_idle =
+                                get_cpu_idle_time(j,
+                                             &pcpu->target_set_time, 0);
+                        pcpu->governor_enabled = 1;
+                        smp_wmb();
+                }
+
+                /*
+                 * Do not register the idle hook and create sysfs
+                 * entries if we have already done so.
+                 */
+                if (atomic_inc_return(&active_count) > 1)
+                        return 0;
+
+                rc = sysfs_create_group(cpufreq_global_kobject,
+                                &ondemandplus_attr_group);
+                if (rc)
+                        return rc;
+
+                idle_notifier_register(&cpufreq_ondemandplus_idle_nb);
+                break;
+
+        case CPUFREQ_GOV_STOP:
+                for_each_cpu(j, policy->cpus) {
+                        pcpu = &per_cpu(cpuinfo, j);
+                        pcpu->governor_enabled = 0;
+                        smp_wmb();
+                        del_timer_sync(&pcpu->cpu_timer);
+
+                        /*
+                         * Reset idle exit time since we may cancel the timer
+                         * before it can run after the last idle exit time,
+                         * to avoid tripping the check in idle exit for a timer
+                         * that is trying to run.
+                         */
+                        pcpu->idle_exit_time = 0;
+                }
+
+                if (atomic_dec_return(&active_count) > 0)
+                        return 0;
+
+                idle_notifier_unregister(&cpufreq_ondemandplus_idle_nb);
+                sysfs_remove_group(cpufreq_global_kobject,
+                                &ondemandplus_attr_group);
+
+                break;
+
+        case CPUFREQ_GOV_LIMITS:
+                if (policy->max < policy->cur)
+                        __cpufreq_driver_target(policy,
+                                        policy->max, CPUFREQ_RELATION_H);
+                else if (policy->min > policy->cur)
+                        __cpufreq_driver_target(policy,
+                                        policy->min, CPUFREQ_RELATION_L);
+                break;
+        }
+        return 0;
+}
+
+static int __init cpufreq_ondemandplus_init(void)
+{
+        unsigned int i;
+        struct cpufreq_ondemandplus_cpuinfo *pcpu;
+        struct sched_param param = { .sched_priority = 0 };
+
+        timer_rate = DEFAULT_TIMER_RATE;
+        up_threshold = DEFAULT_UP_THRESHOLD;
+        down_differential = DEFAULT_DOWN_DIFFERENTIAL;
+        inter_hifreq = DEFAULT_INTER_HIFREQ;
+        allowed_min = DEFAULT_MIN_FREQ;
+        allowed_max = DEFAULT_MAX_FREQ;
+        suspend_frequency = SUSPEND_FREQ;
+        inter_lofreq = DEFAULT_INTER_LOFREQ;
+        inter_staycycles = DEFAULT_INTER_STAYCYCLES;
+        staycycles_resetfreq = DEFAULT_STAYCYCLES_RESETFREQ;
+        io_is_busy = DEFAULT_IO_IS_BUSY;
+
+        /* Initalize per-cpu timers */
+        for_each_possible_cpu(i) {
+                pcpu = &per_cpu(cpuinfo, i);
+                init_timer(&pcpu->cpu_timer);
+                pcpu->cpu_timer.function = cpufreq_ondemandplus_timer;
+                pcpu->cpu_timer.data = i;
+        }
+
+        spin_lock_init(&speedchange_cpumask_lock);
+        speedchange_task =
+                kthread_create(cpufreq_ondemandplus_speedchange_task, NULL,
+                                "cfondemandplus");
+        if (IS_ERR(speedchange_task))
+                return PTR_ERR(speedchange_task);
+
+        sched_setscheduler_nocheck(speedchange_task, SCHED_FIFO, &param);
+        get_task_struct(speedchange_task);
+
+        /* NB: wake up so the thread does not look hung to the freezer */
+        wake_up_process(speedchange_task);
+
+        return cpufreq_register_governor(&cpufreq_gov_ondemandplus);
+
+        put_task_struct(speedchange_task);
+        return -ENOMEM;
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS
+fs_initcall(cpufreq_ondemandplus_init);
+#else
+module_init(cpufreq_ondemandplus_init);
+#endif
+
+static void __exit cpufreq_ondemandplus_exit(void)
+{
+        cpufreq_unregister_governor(&cpufreq_gov_ondemandplus);
+        kthread_stop(speedchange_task);
+        put_task_struct(speedchange_task);
+}
+
+module_exit(cpufreq_ondemandplus_exit);
+
+MODULE_AUTHOR("Mike Chan <mike@android.com>");
+MODULE_DESCRIPTION("'cpufreq_ondemandplus' - A cpufreq governor for "
+        "semi-aggressive scaling");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpufreq/cpufreq_pegasusq.c b/drivers/cpufreq/cpufreq_pegasusq.c
new file mode 100644
index 00000000000..8878885f858
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_pegasusq.c
@@ -0,0 +1,719 @@
+/*
+ *  drivers/cpufreq/cpufreq_pegasusq.c
+ *
+ *  Copyright (C)  2011 Samsung Electronics co. ltd
+ *    ByungChang Cha <bc.cha@samsung.com>
+ *
+ *  Based on ondemand governor
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/mutex.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/ktime.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/suspend.h>
+#include <linux/reboot.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_SAMPLING_DOWN_FACTOR		(2)
+#define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#define DEF_FREQUENCY_DOWN_DIFFERENTIAL		(5)
+#define DEF_FREQUENCY_UP_THRESHOLD		(85)
+
+/* for multiple freq_step */
+#define DEF_UP_THRESHOLD_DIFF	(5)
+
+#define DEF_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+#define MIN_FREQUENCY_UP_THRESHOLD		(11)
+#define MAX_FREQUENCY_UP_THRESHOLD		(100)
+#define DEF_SAMPLING_RATE			(50000)
+#define MIN_SAMPLING_RATE			(10000)
+
+#define DEF_FREQ_STEP				(37)
+/* for multiple freq_step */
+#define DEF_FREQ_STEP_DEC			(13)
+
+#define DEF_START_DELAY				(0)
+
+#define UP_THRESHOLD_AT_MIN_FREQ		(40)
+#define FREQ_FOR_RESPONSIVENESS			(2265600)
+/* for fast decrease */
+#define FREQ_FOR_FAST_DOWN			(1574400)
+#define UP_THRESHOLD_AT_FAST_DOWN		(80)
+
+static unsigned int min_sampling_rate;
+
+static void do_dbs_timer(struct work_struct *work);
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+static
+#endif
+struct cpufreq_governor cpufreq_gov_pegasusq = {
+	.name                   = "pegasusq",
+	.governor               = cpufreq_governor_dbs,
+	.owner                  = THIS_MODULE,
+};
+
+/* Sampling types */
+enum {DBS_NORMAL_SAMPLE, DBS_SUB_SAMPLE};
+
+struct cpu_dbs_info_s {
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	unsigned int prev_cpu_wall_delta;
+	u64 prev_cpu_nice;
+	struct cpufreq_policy *cur_policy;
+	struct delayed_work work;
+	struct cpufreq_frequency_table *freq_table;
+	unsigned int rate_mult;
+	int cpu;
+	/*
+	 * percpu mutex that serializes governor limit change with
+	 * do_dbs_timer invocation. We do not want do_dbs_timer to run
+	 * when user is changing the governor or limits.
+	 */
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, od_cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+
+static struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int up_threshold;
+	unsigned int down_differential;
+	unsigned int ignore_nice;
+	unsigned int sampling_down_factor;
+	unsigned int io_is_busy;
+	/* pegasusq tuners */
+	unsigned int freq_step;
+	unsigned int max_freq;
+	unsigned int min_freq;
+	unsigned int up_threshold_at_min_freq;
+	unsigned int freq_for_responsiveness;
+} dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.down_differential = DEF_FREQUENCY_DOWN_DIFFERENTIAL,
+	.ignore_nice = 0,
+	.freq_step = DEF_FREQ_STEP,
+	.up_threshold_at_min_freq = UP_THRESHOLD_AT_MIN_FREQ,
+	.freq_for_responsiveness = FREQ_FOR_RESPONSIVENESS,
+};
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, bool io_is_busy)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall)
+{
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+/************************** sysfs interface ************************/
+
+static ssize_t show_sampling_rate_min(struct kobject *kobj,
+				      struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", min_sampling_rate);
+}
+
+define_one_global_ro(sampling_rate_min);
+
+/* cpufreq_pegasusq Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct kobject *kobj, struct attribute *attr, char *buf)              \
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(io_is_busy, io_is_busy);
+show_one(up_threshold, up_threshold);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(ignore_nice_load, ignore_nice);
+show_one(down_differential, down_differential);
+show_one(freq_step, freq_step);
+show_one(up_threshold_at_min_freq, up_threshold_at_min_freq);
+show_one(freq_for_responsiveness, freq_for_responsiveness);
+
+static ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);
+
+	return count;
+}
+
+/* io_is_busy */
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+				const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.io_is_busy = !!input;
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct kobject *a, struct attribute *b,
+				  const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+			input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold = input;
+
+	return count;
+}
+
+static ssize_t store_sampling_down_factor(struct kobject *a,
+					  struct attribute *b,
+					  const char *buf, size_t count)
+{
+	unsigned int input, j;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.sampling_down_factor = input;
+
+	/* Reset down sampling multiplier in case it was active */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->rate_mult = 1;
+	}
+	return count;
+}
+
+/* ignore_nice_load */
+static ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,
+				      const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	if (input == dbs_tuners_ins.ignore_nice)  {/* nothing to do */
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *dbs_info;
+		dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&dbs_info->prev_cpu_wall, dbs_tuners_ins.io_is_busy);
+		if (dbs_tuners_ins.ignore_nice)
+			dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+	}
+	return count;
+}
+
+static ssize_t store_down_differential(struct kobject *a, struct attribute *b,
+				    const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.down_differential = min(input, 100u);
+
+	return count;
+}
+
+static ssize_t store_freq_step(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	dbs_tuners_ins.freq_step = min(input, 100u);
+	return count;
+}
+
+static ssize_t store_up_threshold_at_min_freq(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+
+	if (ret != 1 || input > MAX_FREQUENCY_UP_THRESHOLD ||
+	    input < MIN_FREQUENCY_UP_THRESHOLD) {
+		return -EINVAL;
+	}
+	dbs_tuners_ins.up_threshold_at_min_freq = input;
+	return count;
+}
+
+static ssize_t store_freq_for_responsiveness(struct kobject *a, struct attribute *b,
+				   const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+	dbs_tuners_ins.freq_for_responsiveness = input;
+	return count;
+}
+
+define_one_global_rw(sampling_rate);
+define_one_global_rw(io_is_busy);
+define_one_global_rw(up_threshold);
+define_one_global_rw(sampling_down_factor);
+define_one_global_rw(ignore_nice_load);
+define_one_global_rw(down_differential);
+define_one_global_rw(freq_step);
+define_one_global_rw(up_threshold_at_min_freq);
+define_one_global_rw(freq_for_responsiveness);
+
+static struct attribute *dbs_attributes[] = {
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&up_threshold.attr,
+	&sampling_down_factor.attr,
+	&ignore_nice_load.attr,
+	&io_is_busy.attr,
+	&down_differential.attr,
+	&freq_step.attr,
+	&up_threshold_at_min_freq.attr,
+	&freq_for_responsiveness.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "pegasusq",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_freq_increase(struct cpufreq_policy *p, unsigned int freq)
+{
+#if !defined(CONFIG_ARCH_EXYNOS4) && !defined(CONFIG_ARCH_EXYNOS5)
+	if (p->cur == p->max)
+		return;
+#endif
+
+	__cpufreq_driver_target(p, freq, CPUFREQ_RELATION_L);
+}
+
+static void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)
+{
+	unsigned int max_load_freq;
+
+	struct cpufreq_policy *policy;
+	unsigned int j;
+	int up_threshold = dbs_tuners_ins.up_threshold;
+
+	policy = this_dbs_info->cur_policy;
+
+	/* Get Absolute Load - in terms of freq */
+	max_load_freq = 0;
+
+	for_each_cpu(j, policy->cpus) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+		u64 prev_wall_time, prev_idle_time, prev_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int load, load_freq;
+		int freq_avg;
+
+		j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+		prev_wall_time = j_dbs_info->prev_cpu_wall;
+		prev_idle_time = j_dbs_info->prev_cpu_idle;
+		prev_iowait_time = j_dbs_info->prev_cpu_iowait;
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, dbs_tuners_ins.io_is_busy);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = (unsigned int)
+				(cur_wall_time - prev_wall_time);
+		j_dbs_info->prev_cpu_wall = cur_wall_time;
+
+		idle_time = (unsigned int)
+				(cur_idle_time - prev_idle_time);
+		j_dbs_info->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = (unsigned int)
+				(cur_iowait_time - prev_iowait_time);
+		j_dbs_info->prev_cpu_iowait = cur_iowait_time;
+
+		if (dbs_tuners_ins.ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] -
+					 j_dbs_info->prev_cpu_nice;
+			/*
+			 * Assumption: nice time between sampling periods will
+			 * be less than 2^32 jiffies for 32 bit sys
+			 */
+			cur_nice_jiffies = (unsigned long)
+					cputime64_to_jiffies64(cur_nice);
+
+			j_dbs_info->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		if (dbs_tuners_ins.io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		load = 100 * (wall_time - idle_time) / wall_time;
+
+		freq_avg = __cpufreq_driver_getavg(policy, j);
+		if (freq_avg <= 0)
+			freq_avg = policy->cur;
+
+		load_freq = load * freq_avg;
+		if (load_freq > max_load_freq)
+			max_load_freq = load_freq;
+	}
+
+	/* Check for frequency increase */
+	if (policy->cur < dbs_tuners_ins.freq_for_responsiveness)
+		up_threshold = dbs_tuners_ins.up_threshold_at_min_freq;
+	/* for fast frequency decrease */
+	else
+		up_threshold = dbs_tuners_ins.up_threshold;
+
+	if (max_load_freq > up_threshold * policy->cur) {
+		/* for multiple freq_step */
+		int inc = policy->max * (dbs_tuners_ins.freq_step
+					- DEF_FREQ_STEP_DEC * 2) / 100;
+		int target = 0;
+
+		/* for multiple freq_step */
+		if (max_load_freq > (up_threshold + DEF_UP_THRESHOLD_DIFF * 2)
+			* policy->cur)
+			inc = policy->max * dbs_tuners_ins.freq_step / 100;
+		else if (max_load_freq > (up_threshold + DEF_UP_THRESHOLD_DIFF)
+			* policy->cur)
+			inc = policy->max * (dbs_tuners_ins.freq_step
+					- DEF_FREQ_STEP_DEC) / 100;
+
+		target = min(policy->max, policy->cur + inc);
+
+		/* If switching to max speed, apply sampling_down_factor */
+		if (policy->cur < policy->max && target == policy->max)
+			this_dbs_info->rate_mult =
+				dbs_tuners_ins.sampling_down_factor;
+		dbs_freq_increase(policy, target);
+		return;
+	}
+
+	/* Check for frequency decrease */
+#if !defined(CONFIG_ARCH_EXYNOS4) && !defined(CONFIG_ARCH_EXYNOS5)
+	/* if we cannot reduce the frequency anymore, break out early */
+	if (policy->cur == policy->min)
+		return;
+#endif
+
+	/*
+	 * The optimal frequency is the frequency that is the lowest that
+	 * can support the current CPU usage without triggering the up
+	 * policy. To be safe, we focus DOWN_DIFFERENTIAL points under
+	 * the threshold.
+	 */
+	if (max_load_freq <
+	    (dbs_tuners_ins.up_threshold - dbs_tuners_ins.down_differential) *
+			policy->cur) {
+		unsigned int freq_next;
+		unsigned int down_thres;
+
+		freq_next = max_load_freq /
+			(dbs_tuners_ins.up_threshold -
+				dbs_tuners_ins.down_differential);
+
+		/* No longer fully busy, reset rate_mult */
+		this_dbs_info->rate_mult = 1;
+
+		if (freq_next < policy->min)
+			freq_next = policy->min;
+
+		down_thres = dbs_tuners_ins.up_threshold_at_min_freq
+			- dbs_tuners_ins.down_differential;
+
+		if (freq_next < dbs_tuners_ins.freq_for_responsiveness
+			&& (max_load_freq / freq_next) > down_thres)
+			freq_next = dbs_tuners_ins.freq_for_responsiveness;
+
+		if (policy->cur == freq_next)
+			return;
+
+		__cpufreq_driver_target(policy, freq_next,
+					CPUFREQ_RELATION_L);
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	struct cpu_dbs_info_s *dbs_info =
+		container_of(work, struct cpu_dbs_info_s, work.work);
+	unsigned int cpu = dbs_info->cpu;
+	int delay;
+
+	mutex_lock(&dbs_info->timer_mutex);
+
+	dbs_check_cpu(dbs_info);
+	/* We want all CPUs to do sampling nearly on
+	 * same jiffy
+	 */
+	delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate
+				 * dbs_info->rate_mult);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	schedule_delayed_work_on(cpu, &dbs_info->work, delay);
+	mutex_unlock(&dbs_info->timer_mutex);
+}
+
+static inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)
+{
+	/* We want all CPUs to do sampling nearly on same jiffy */
+	int delay = usecs_to_jiffies(DEF_START_DELAY * 1000 * 1000
+				     + dbs_tuners_ins.sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+
+	INIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);
+
+	schedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);
+}
+
+static inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)
+{
+	cancel_delayed_work_sync(&dbs_info->work);
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(od_cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		dbs_tuners_ins.max_freq = policy->max;
+		dbs_tuners_ins.min_freq = policy->min;
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable++;
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(od_cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,
+						&j_dbs_info->prev_cpu_wall, dbs_tuners_ins.io_is_busy);
+			if (dbs_tuners_ins.ignore_nice) {
+				j_dbs_info->prev_cpu_nice =
+						kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+			}
+		}
+		this_dbs_info->cpu = cpu;
+		this_dbs_info->rate_mult = 1;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&dbs_attr_group);
+			if (rc) {
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+
+			min_sampling_rate = MIN_SAMPLING_RATE;
+			dbs_tuners_ins.sampling_rate = DEF_SAMPLING_RATE;
+			dbs_tuners_ins.io_is_busy = 0;
+		}
+		mutex_unlock(&dbs_mutex);
+
+		mutex_init(&this_dbs_info->timer_mutex);
+
+		dbs_timer_init(this_dbs_info);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+
+		dbs_timer_exit(this_dbs_info);
+
+		mutex_destroy(&this_dbs_info->timer_mutex);
+
+		mutex_lock(&dbs_mutex);
+
+		dbs_enable--;
+
+		if (!dbs_enable) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &dbs_attr_group);
+		}
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_dbs_info->timer_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(this_dbs_info->cur_policy,
+				policy->min, CPUFREQ_RELATION_L);
+		dbs_check_cpu(this_dbs_info);
+		mutex_unlock(&this_dbs_info->timer_mutex);
+
+		break;
+	}
+	return 0;
+}
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	int ret;
+
+	ret = cpufreq_register_governor(&cpufreq_gov_pegasusq);
+	if (ret)
+		goto err_reg;
+
+	return ret;
+
+err_reg:
+	kfree(&dbs_tuners_ins);
+	return ret;
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_pegasusq);
+	kfree(&dbs_tuners_ins);
+}
+
+MODULE_AUTHOR("ByungChang Cha <bc.cha@samsung.com>");
+MODULE_DESCRIPTION("'cpufreq_pegasusq' - A dynamic cpufreq governor");
+MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_PEGASUSQ
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_smartass2.c b/drivers/cpufreq/cpufreq_smartass2.c
new file mode 100644
index 00000000000..2ef94bf91a9
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartass2.c
@@ -0,0 +1,880 @@
+/*
+ * drivers/cpufreq/cpufreq_smartass2.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Erasmux / Tuner: ch33kybutt
+ *
+ * Based on the interactive governor By Mike Chan (mike@android.com)
+ * which was adaptated to 2.6.29 kernel by Nadlabak (pavel@doshaska.net)
+ *
+ * SMP support based on mod by faux123
+ *
+ * For a general overview of smartassV2 see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <asm/cputime.h>
+#include <linux/earlysuspend.h>
+
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use when awake. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+#define DEFAULT_AWAKE_IDEAL_FREQ 600000
+static unsigned int awake_ideal_freq;
+
+/*
+ * The "ideal" frequency to use when suspended.
+ * When set to 0, the governor will not track the suspended state (meaning
+ * that practically when sleep_ideal_freq==0 the awake_ideal_freq is used
+ * also when suspended).
+ */
+#define DEFAULT_SLEEP_IDEAL_FREQ 100000
+static unsigned int sleep_ideal_freq;
+
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+#define DEFAULT_RAMP_UP_STEP 0
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+#define DEFAULT_RAMP_DOWN_STEP 200000
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+#define DEFAULT_MAX_CPU_LOAD 85
+static unsigned long max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+#define DEFAULT_MIN_CPU_LOAD 45
+static unsigned long min_cpu_load;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+#define DEFAULT_UP_RATE_US 24000;
+static unsigned long up_rate_us;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+#define DEFAULT_DOWN_RATE_US 48000;
+static unsigned long down_rate_us;
+
+/*
+ * The frequency to set when waking up from sleep.
+ * When sleep_ideal_freq=0 this will have no effect.
+ */
+#define DEFAULT_SLEEP_WAKEUP_FREQ 99999999
+static unsigned int sleep_wakeup_freq;
+
+/*
+ * Sampling rate, I highly recommend to leave it at 2.
+ */
+#define DEFAULT_SAMPLE_RATE_JIFFIES 4
+static unsigned int sample_rate_jiffies;
+
+
+/*************** End of tunables ***************/
+
+
+static void (*pm_idle_old)(void);
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct smartass_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct timer_list timer;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	u64 freq_change_time;
+	u64 freq_change_time_in_idle;
+	int cur_cpu_load;
+	int old_freq;
+	int ramp_dir;
+	unsigned int enable;
+	int ideal_speed;
+};
+static DEFINE_PER_CPU(struct smartass_info_s, smartass_info);
+
+/* Workqueues handle frequency scaling */
+static struct workqueue_struct *up_wq;
+static struct workqueue_struct *down_wq;
+static struct work_struct freq_scale_work;
+
+static cpumask_t work_cpumask;
+static spinlock_t cpumask_lock;
+
+static unsigned int suspended;
+
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) printk(KERN_DEBUG msg); \
+	} while (0)
+
+enum {
+	SMARTASS_DEBUG_JUMPS=1,
+	SMARTASS_DEBUG_LOAD=2,
+	SMARTASS_DEBUG_ALG=4
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+static unsigned long debug_mask;
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartass2 = {
+	.name = "smartassV2",
+	.governor = cpufreq_governor_smartass,
+	.max_transition_latency = 9000000,
+	.owner = THIS_MODULE,
+};
+
+inline static void smartass_update_min_max(struct smartass_info_s *this_smartass, struct cpufreq_policy *policy, int suspend) {
+	if (suspend) {
+		this_smartass->ideal_speed = // sleep_ideal_freq; but make sure it obeys the policy min/max
+			policy->max > sleep_ideal_freq ?
+			(sleep_ideal_freq > policy->min ? sleep_ideal_freq : policy->min) : policy->max;
+	} else {
+		this_smartass->ideal_speed = // awake_ideal_freq; but make sure it obeys the policy min/max
+			policy->min < awake_ideal_freq ?
+			(awake_ideal_freq < policy->max ? awake_ideal_freq : policy->max) : policy->min;
+	}
+}
+
+inline static void smartass_update_min_max_allcpus(void) {
+	unsigned int i;
+	for_each_online_cpu(i) {
+		struct smartass_info_s *this_smartass = &per_cpu(smartass_info, i);
+		if (this_smartass->enable)
+			smartass_update_min_max(this_smartass,this_smartass->cur_policy,suspended);
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy, int freq) {
+	if (freq > (int)policy->max)
+		return policy->max;
+	if (freq < (int)policy->min)
+		return policy->min;
+	return freq;
+}
+
+inline static void reset_timer(unsigned long cpu, struct smartass_info_s *this_smartass) {
+	this_smartass->time_in_idle = get_cpu_idle_time_us(cpu, &this_smartass->idle_exit_time);
+	mod_timer(&this_smartass->timer, jiffies + sample_rate_jiffies);
+}
+
+inline static void work_cpumask_set(unsigned long cpu) {
+	unsigned long flags;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	cpumask_set_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+}
+
+inline static int work_cpumask_test_and_clear(unsigned long cpu) {
+	unsigned long flags;
+	int res = 0;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	res = cpumask_test_and_clear_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+	return res;
+}
+
+inline static int target_freq(struct cpufreq_policy *policy, struct smartass_info_s *this_smartass,
+			      int new_freq, int old_freq, int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartass->freq_table;
+
+	if (new_freq == old_freq)
+		return 0;
+	new_freq = validate_freq(policy,new_freq);
+	if (new_freq == old_freq)
+		return 0;
+
+	if (table &&
+	    !cpufreq_frequency_table_target(policy,table,new_freq,prefered_relation,&index))
+	{
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation==CPUFREQ_RELATION_H
+			    && !cpufreq_frequency_table_target(policy,table,new_freq,
+							       CPUFREQ_RELATION_L,&index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq && prefered_relation==CPUFREQ_RELATION_L
+				&& !cpufreq_frequency_table_target(policy,table,new_freq,
+								   CPUFREQ_RELATION_H,&index))
+				target = table[index].frequency;
+		}
+
+		if (target == old_freq) {
+			// We should not get here:
+			// If we got here we tried to change to a validated new_freq which is different
+			// from old_freq, so there is no reason for us to remain at same frequency.
+			printk(KERN_WARNING "Smartass: frequency change failed: %d to %d => %d\n",
+			       old_freq,new_freq,target);
+			return 0;
+		}
+	}
+	else target = new_freq;
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	dprintk(SMARTASS_DEBUG_JUMPS,"SmartassQ: jumping from %d to %d => %d (%d)\n",
+		old_freq,new_freq,target,policy->cur);
+
+	return target;
+}
+
+static void cpufreq_smartass_timer(unsigned long cpu)
+{
+	u64 delta_idle;
+	u64 delta_time;
+	int cpu_load;
+	int old_freq;
+	u64 update_time;
+	u64 now_idle;
+	int queued_work = 0;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	now_idle = get_cpu_idle_time_us(cpu, &update_time);
+	old_freq = policy->cur;
+
+	if (this_smartass->idle_exit_time == 0 || update_time == this_smartass->idle_exit_time)
+		return;
+
+	delta_idle = now_idle - this_smartass->time_in_idle;
+	delta_time = update_time - this_smartass->idle_exit_time;
+
+	// If timer ran less than 1ms after short-term sample started, retry.
+	if (delta_time < 1000) {
+		if (!timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+		return;
+	}
+
+	if (delta_idle > delta_time)
+		cpu_load = 0;
+	else
+		cpu_load = 100 * (unsigned int)(delta_time - delta_idle) / (unsigned int)delta_time;
+
+	dprintk(SMARTASS_DEBUG_LOAD,"smartassT @ %d: load %d (delta_time %llu)\n",
+		old_freq,cpu_load,delta_time);
+
+	this_smartass->cur_cpu_load = cpu_load;
+	this_smartass->old_freq = old_freq;
+
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate_us:
+	if (cpu_load > max_cpu_load || delta_idle == 0)
+	{
+		if (old_freq < policy->max &&
+			 (old_freq < this_smartass->ideal_speed || delta_idle == 0 ||
+			  (update_time - this_smartass->freq_change_time) >= up_rate_us))
+		{
+			dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp up: load %d (delta_idle %llu)\n",
+				old_freq,cpu_load,delta_idle);
+			this_smartass->ramp_dir = 1;
+			work_cpumask_set(cpu);
+			queue_work(up_wq, &freq_scale_work);
+			queued_work = 1;
+		}
+		else this_smartass->ramp_dir = 0;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate_us:
+	else if (cpu_load < min_cpu_load && old_freq > policy->min &&
+		 (old_freq > this_smartass->ideal_speed ||
+		  (update_time - this_smartass->freq_change_time) >= down_rate_us))
+	{
+		dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp down: load %d (delta_idle %llu)\n",
+			old_freq,cpu_load,delta_idle);
+		this_smartass->ramp_dir = -1;
+		work_cpumask_set(cpu);
+		queue_work(down_wq, &freq_scale_work);
+		queued_work = 1;
+	}
+	else this_smartass->ramp_dir = 0;
+
+	// To avoid unnecessary load when the CPU is already at high load, we don't
+	// reset ourselves if we are at max speed. If and when there are idle cycles,
+	// the idle loop will activate the timer.
+	// Additionally, if we queued some work, the work task will reset the timer
+	// after it has done its adjustments.
+	if (!queued_work && old_freq < policy->max)
+		reset_timer(cpu,this_smartass);
+}
+
+static void cpufreq_idle(void)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	if (!this_smartass->enable) {
+		pm_idle_old();
+		return;
+	}
+
+	if (policy->cur == policy->min && timer_pending(&this_smartass->timer))
+		del_timer(&this_smartass->timer);
+
+	pm_idle_old();
+
+	if (!timer_pending(&this_smartass->timer))
+		reset_timer(smp_processor_id(), this_smartass);
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartass_freq_change_time_work(struct work_struct *work)
+{
+	unsigned int cpu;
+	int new_freq;
+	int old_freq;
+	int ramp_dir;
+	struct smartass_info_s *this_smartass;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_L;
+	for_each_possible_cpu(cpu) {
+		this_smartass = &per_cpu(smartass_info, cpu);
+		if (!work_cpumask_test_and_clear(cpu))
+			continue;
+
+		ramp_dir = this_smartass->ramp_dir;
+		this_smartass->ramp_dir = 0;
+
+		old_freq = this_smartass->old_freq;
+		policy = this_smartass->cur_policy;
+
+		if (old_freq != policy->cur) {
+			// frequency was changed by someone else?
+			printk(KERN_WARNING "Smartass: frequency changed by 3rd party: %d to %d\n",
+			       old_freq,policy->cur);
+			new_freq = old_freq;
+		}
+		else if (ramp_dir > 0 && nr_running() > 1) {
+			// ramp up logic:
+			if (old_freq < this_smartass->ideal_speed)
+				new_freq = this_smartass->ideal_speed;
+			else if (ramp_up_step) {
+				new_freq = old_freq + ramp_up_step;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else {
+				new_freq = policy->max;
+				relation = CPUFREQ_RELATION_H;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp up: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else if (ramp_dir < 0) {
+			// ramp down logic:
+			if (old_freq > this_smartass->ideal_speed) {
+				new_freq = this_smartass->ideal_speed;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else if (ramp_down_step)
+				new_freq = old_freq - ramp_down_step;
+			else {
+				// Load heuristics: Adjust new_freq such that, assuming a linear
+				// scaling of load vs. frequency, the load in the new frequency
+				// will be max_cpu_load:
+				new_freq = old_freq * this_smartass->cur_cpu_load / max_cpu_load;
+				if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+					new_freq = old_freq -1;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp down: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else { // ramp_dir==0 ?! Could the timer change its mind about a queued ramp up/down
+		       // before the work task gets to run?
+		       // This may also happen if we refused to ramp up because the nr_running()==1
+			new_freq = old_freq;
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d nothing: ramp_dir=%d nr_running=%lu\n",
+				old_freq,ramp_dir,nr_running());
+		}
+
+		// do actual ramp up (returns 0, if frequency change failed):
+		new_freq = target_freq(policy,this_smartass,new_freq,old_freq,relation);
+		if (new_freq)
+			this_smartass->freq_change_time_in_idle =
+				get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		// reset timer:
+		if (new_freq < policy->max)
+			reset_timer(cpu,this_smartass);
+		// if we are maxed out, it is pointless to use the timer
+		// (idle cycles wake up the timer when the timer comes)
+		else if (timer_pending(&this_smartass->timer))
+			del_timer(&this_smartass->timer);
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_up_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", up_rate_us);
+}
+
+static ssize_t store_up_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate_us = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_down_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", down_rate_us);
+}
+
+static ssize_t store_down_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate_us = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_ideal_freq);
+}
+
+static ssize_t store_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		sleep_ideal_freq = input;
+		if (suspended)
+			smartass_update_min_max_allcpus();
+	}
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_wakeup_freq);
+}
+
+static ssize_t store_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		sleep_wakeup_freq = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!suspended)
+			smartass_update_min_max_allcpus();
+	}
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sample_rate_jiffies);
+}
+
+static ssize_t store_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 1000)
+		sample_rate_jiffies = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	else return -EINVAL;
+	return count;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	else return -EINVAL;
+	return count;
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate_us);
+define_global_rw_attr(down_rate_us);
+define_global_rw_attr(sleep_ideal_freq);
+define_global_rw_attr(sleep_wakeup_freq);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(sample_rate_jiffies);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+
+static struct attribute * smartass_attributes[] = {
+	&debug_mask_attr.attr,
+	&up_rate_us_attr.attr,
+	&down_rate_us_attr.attr,
+	&sleep_ideal_freq_attr.attr,
+	&sleep_wakeup_freq_attr.attr,
+	&awake_ideal_freq_attr.attr,
+	&sample_rate_jiffies_attr.attr,
+	&ramp_up_step_attr.attr,
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr,
+	&min_cpu_load_attr.attr,
+	NULL,
+};
+
+static struct attribute_group smartass_attr_group = {
+	.attrs = smartass_attributes,
+	.name = "smartass",
+};
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *new_policy,
+		unsigned int event)
+{
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		this_smartass->cur_policy = new_policy;
+
+		this_smartass->enable = 1;
+
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		this_smartass->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_smartass->freq_table)
+			printk(KERN_WARNING "Smartass: no frequency table for cpu %d?!\n",cpu);
+
+		smp_wmb();
+
+		// Do not register the idle hook and create sysfs
+		// entries if we have already done so.
+		if (atomic_inc_return(&active_count) <= 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&smartass_attr_group);
+			if (rc)
+				return rc;
+
+			pm_idle_old = pm_idle;
+			pm_idle = cpufreq_idle;
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		if (this_smartass->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartass->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->min, CPUFREQ_RELATION_L);
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_smartass->enable = 0;
+		smp_wmb();
+		del_timer(&this_smartass->timer);
+		flush_work(&freq_scale_work);
+		this_smartass->idle_exit_time = 0;
+
+		if (atomic_dec_return(&active_count) <= 1) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &smartass_attr_group);
+			pm_idle = pm_idle_old;
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static void smartass_suspend(int cpu, int suspend)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+	unsigned int new_freq;
+
+	if (!this_smartass->enable)
+		return;
+
+	smartass_update_min_max(this_smartass,policy,suspend);
+	if (!suspend) { // resume at max speed:
+		new_freq = validate_freq(policy,sleep_wakeup_freq);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: awaking at %d\n",new_freq);
+
+		__cpufreq_driver_target(policy, new_freq,
+					CPUFREQ_RELATION_L);
+	} else {
+		// to avoid wakeup issues with quick sleep/wakeup don't change actual frequency when entering sleep
+		// to allow some time to settle down. Instead we just reset our statistics (and reset the timer).
+		// Eventually, the timer will adjust the frequency if necessary.
+
+		this_smartass->freq_change_time_in_idle =
+			get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: suspending at %d\n",policy->cur);
+	}
+
+	reset_timer(smp_processor_id(),this_smartass);
+}
+
+static void smartass_early_suspend(struct early_suspend *handler) {
+	int i;
+	if (suspended || sleep_ideal_freq==0) // disable behavior for sleep_ideal_freq==0
+		return;
+	suspended = 1;
+	for_each_online_cpu(i)
+		smartass_suspend(i,1);
+}
+
+static void smartass_late_resume(struct early_suspend *handler) {
+	int i;
+	if (!suspended) // already not suspended so nothing to do
+		return;
+	suspended = 0;
+	for_each_online_cpu(i)
+		smartass_suspend(i,0);
+}
+
+static struct early_suspend smartass_power_suspend = {
+	.suspend = smartass_early_suspend,
+	.resume = smartass_late_resume,
+#ifdef CONFIG_MACH_HERO
+	.level = EARLY_SUSPEND_LEVEL_DISABLE_FB + 1,
+#endif
+};
+
+static int __init cpufreq_smartass_init(void)
+{
+	unsigned int i;
+	struct smartass_info_s *this_smartass;
+	debug_mask = 0;
+	up_rate_us = DEFAULT_UP_RATE_US;
+	down_rate_us = DEFAULT_DOWN_RATE_US;
+	sleep_ideal_freq = DEFAULT_SLEEP_IDEAL_FREQ;
+	sleep_wakeup_freq = DEFAULT_SLEEP_WAKEUP_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	sample_rate_jiffies = DEFAULT_SAMPLE_RATE_JIFFIES;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+
+	spin_lock_init(&cpumask_lock);
+
+	suspended = 0;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i) {
+		this_smartass = &per_cpu(smartass_info, i);
+		this_smartass->enable = 0;
+		this_smartass->cur_policy = 0;
+		this_smartass->ramp_dir = 0;
+		this_smartass->time_in_idle = 0;
+		this_smartass->idle_exit_time = 0;
+		this_smartass->freq_change_time = 0;
+		this_smartass->freq_change_time_in_idle = 0;
+		this_smartass->cur_cpu_load = 0;
+		// intialize timer:
+		init_timer_deferrable(&this_smartass->timer);
+		this_smartass->timer.function = cpufreq_smartass_timer;
+		this_smartass->timer.data = i;
+		work_cpumask_test_and_clear(i);
+	}
+
+	// Scale up is high priority
+	up_wq = alloc_workqueue("ksmartass_up", WQ_HIGHPRI, 1);
+	down_wq = alloc_workqueue("ksmartass_down", 0, 1);
+	if (!up_wq || !down_wq)
+		return -ENOMEM;
+
+	INIT_WORK(&freq_scale_work, cpufreq_smartass_freq_change_time_work);
+
+	register_early_suspend(&smartass_power_suspend);
+
+	return cpufreq_register_governor(&cpufreq_gov_smartass2);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2
+fs_initcall(cpufreq_smartass_init);
+#else
+module_init(cpufreq_smartass_init);
+#endif
+
+static void __exit cpufreq_smartass_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_smartass2);
+	destroy_workqueue(up_wq);
+	destroy_workqueue(down_wq);
+}
+
+MODULE_AUTHOR ("Erasmux");
+MODULE_DESCRIPTION ("'cpufreq_smartass2' - A smart cpufreq governor");
+MODULE_LICENSE ("GPL");
+
+module_exit(cpufreq_smartass_exit);
diff --git a/drivers/cpufreq/cpufreq_smartassH3.c b/drivers/cpufreq/cpufreq_smartassH3.c
new file mode 100644
index 00000000000..c004d24c527
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartassH3.c
@@ -0,0 +1,903 @@
+/*
+ * drivers/cpufreq/cpufreq_smartassH3.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Erasmux
+ *
+ * Based on the interactive governor By Mike Chan (mike@android.com)
+ * which was adaptated to 2.6.29 kernel by Nadlabak (pavel@doshaska.net)
+ *
+ * SMP support based on mod by faux123
+ *
+ * ZTE Skate specific tweaks by H3ROS @ MoDaCo, integrated by C3C0 @ MoDaCo
+ *
+ * For a general overview of smartassV2 see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <linux/notifier.h>
+#include <asm/cputime.h>
+#include <linux/earlysuspend.h>
+
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use when awake. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+#define DEFAULT_AWAKE_IDEAL_FREQ 378000
+static unsigned int awake_ideal_freq;
+
+/*
+ * The "ideal" frequency to use when suspended.
+ * When set to 0, the governor will not track the suspended state (meaning
+ * that practically when sleep_ideal_freq==0 the awake_ideal_freq is used
+ * also when suspended).
+ */
+#define DEFAULT_SLEEP_IDEAL_FREQ 378000
+static unsigned int sleep_ideal_freq;
+
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+#define DEFAULT_RAMP_UP_STEP 80000
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+#define DEFAULT_RAMP_DOWN_STEP 80000
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+#define DEFAULT_MAX_CPU_LOAD 85
+static unsigned long max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+#define DEFAULT_MIN_CPU_LOAD 70
+static unsigned long min_cpu_load;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+#define DEFAULT_UP_RATE_US 48000;
+static unsigned long up_rate_us;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+#define DEFAULT_DOWN_RATE_US 49000;
+static unsigned long down_rate_us;
+
+/*
+ * The frequency to set when waking up from sleep.
+ * When sleep_ideal_freq=0 this will have no effect.
+ */
+#define DEFAULT_SLEEP_WAKEUP_FREQ 99999999
+static unsigned int sleep_wakeup_freq;
+
+/*
+ * Sampling rate, I highly recommend to leave it at 2.
+ */
+#define DEFAULT_SAMPLE_RATE_JIFFIES 2
+static unsigned int sample_rate_jiffies;
+
+
+/*************** End of tunables ***************/
+
+
+static void (*pm_idle_old)(void);
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct smartass_info_s {
+  struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct timer_list timer;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	u64 freq_change_time;
+	u64 freq_change_time_in_idle;
+	int cur_cpu_load;
+	int old_freq;
+	int ramp_dir;
+	unsigned int enable;
+	int ideal_speed;
+};
+static DEFINE_PER_CPU(struct smartass_info_s, smartass_info);
+
+/* Workqueues handle frequency scaling */
+static struct workqueue_struct *up_wq;
+static struct workqueue_struct *down_wq;
+static struct work_struct freq_scale_work;
+
+static cpumask_t work_cpumask;
+static spinlock_t cpumask_lock;
+
+static unsigned int suspended;
+
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) printk(KERN_DEBUG msg); \
+	} while (0)
+
+enum {
+	SMARTASS_DEBUG_JUMPS=1,
+	SMARTASS_DEBUG_LOAD=2,
+	SMARTASS_DEBUG_ALG=4
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+static unsigned long debug_mask;
+
+static int cpufreq_governor_smartass_h3(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartass_h3 = {
+	.name = "smartassH3",
+	.governor = cpufreq_governor_smartass_h3,
+	.max_transition_latency = 9000000,
+	.owner = THIS_MODULE,
+};
+
+inline static void smartass_update_min_max(struct smartass_info_s *this_smartass, struct cpufreq_policy *policy, int suspend) {
+	if (suspend) {
+		this_smartass->ideal_speed = // sleep_ideal_freq; but make sure it obeys the policy min/max
+			policy->max > sleep_ideal_freq ?
+			(sleep_ideal_freq > policy->min ? sleep_ideal_freq : policy->min) : policy->max;
+	} else {
+		this_smartass->ideal_speed = // awake_ideal_freq; but make sure it obeys the policy min/max
+			policy->min < awake_ideal_freq ?
+			(awake_ideal_freq < policy->max ? awake_ideal_freq : policy->max) : policy->min;
+	}
+}
+
+inline static void smartass_update_min_max_allcpus(void) {
+	unsigned int i;
+	for_each_online_cpu(i) {
+		struct smartass_info_s *this_smartass = &per_cpu(smartass_info, i);
+		if (this_smartass->enable)
+			smartass_update_min_max(this_smartass,this_smartass->cur_policy,suspended);
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy, int freq) {
+	if (freq > (int)policy->max)
+		return policy->max;
+	if (freq < (int)policy->min)
+		return policy->min;
+	return freq;
+}
+
+inline static void reset_timer(unsigned long cpu, struct smartass_info_s *this_smartass) {
+	this_smartass->time_in_idle = get_cpu_idle_time_us(cpu, &this_smartass->idle_exit_time);
+	mod_timer(&this_smartass->timer, jiffies + sample_rate_jiffies);
+}
+
+inline static void work_cpumask_set(unsigned long cpu) {
+	unsigned long flags;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	cpumask_set_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+}
+
+inline static int work_cpumask_test_and_clear(unsigned long cpu) {
+	unsigned long flags;
+	int res = 0;
+	spin_lock_irqsave(&cpumask_lock, flags);
+	res = cpumask_test_and_clear_cpu(cpu, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+	return res;
+}
+
+inline static int target_freq(struct cpufreq_policy *policy, struct smartass_info_s *this_smartass,
+			      int new_freq, int old_freq, int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartass->freq_table;
+
+	if (new_freq == old_freq)
+		return 0;
+	new_freq = validate_freq(policy,new_freq);
+	if (new_freq == old_freq)
+		return 0;
+
+	if (table &&
+	    !cpufreq_frequency_table_target(policy,table,new_freq,prefered_relation,&index))
+	{
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation==CPUFREQ_RELATION_H
+			    && !cpufreq_frequency_table_target(policy,table,new_freq,
+							       CPUFREQ_RELATION_L,&index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq && prefered_relation==CPUFREQ_RELATION_L
+				&& !cpufreq_frequency_table_target(policy,table,new_freq,
+								   CPUFREQ_RELATION_H,&index))
+				target = table[index].frequency;
+		}
+
+		if (target == old_freq) {
+			// We should not get here:
+			// If we got here we tried to change to a validated new_freq which is different
+			// from old_freq, so there is no reason for us to remain at same frequency.
+			printk(KERN_WARNING "Smartass: frequency change failed: %d to %d => %d\n",
+			       old_freq,new_freq,target);
+			return 0;
+		}
+	}
+	else target = new_freq;
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	dprintk(SMARTASS_DEBUG_JUMPS,"SmartassQ: jumping from %d to %d => %d (%d)\n",
+		old_freq,new_freq,target,policy->cur);
+
+	return target;
+}
+
+static void cpufreq_smartass_timer(unsigned long cpu)
+{
+	u64 delta_idle;
+	u64 delta_time;
+	int cpu_load;
+	int old_freq;
+	u64 update_time;
+	u64 now_idle;
+	int queued_work = 0;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	now_idle = get_cpu_idle_time_us(cpu, &update_time);
+	old_freq = policy->cur;
+
+	if (this_smartass->idle_exit_time == 0 || update_time == this_smartass->idle_exit_time)
+		return;
+
+	delta_idle = cputime64_sub(now_idle, this_smartass->time_in_idle);
+	delta_time = cputime64_sub(update_time, this_smartass->idle_exit_time);
+
+	// If timer ran less than 1ms after short-term sample started, retry.
+	if (delta_time < 1000) {
+		if (!timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+		return;
+	}
+
+	if (delta_idle > delta_time)
+		cpu_load = 0;
+	else
+		cpu_load = 100 * (unsigned int)(delta_time - delta_idle) / (unsigned int)delta_time;
+
+	dprintk(SMARTASS_DEBUG_LOAD,"smartassT @ %d: load %d (delta_time %llu)\n",
+		old_freq,cpu_load,delta_time);
+
+	this_smartass->cur_cpu_load = cpu_load;
+	this_smartass->old_freq = old_freq;
+
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate_us:
+	if (cpu_load > max_cpu_load || delta_idle == 0)
+	{
+		if (old_freq < policy->max &&
+			 (old_freq < this_smartass->ideal_speed || delta_idle == 0 ||
+			  cputime64_sub(update_time, this_smartass->freq_change_time) >= up_rate_us))
+		{
+			dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp up: load %d (delta_idle %llu)\n",
+				old_freq,cpu_load,delta_idle);
+			this_smartass->ramp_dir = 1;
+			work_cpumask_set(cpu);
+			queue_work(up_wq, &freq_scale_work);
+			queued_work = 1;
+		}
+		else this_smartass->ramp_dir = 0;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate_us:
+	else if (cpu_load < min_cpu_load && old_freq > policy->min &&
+		 (old_freq > this_smartass->ideal_speed ||
+		  cputime64_sub(update_time, this_smartass->freq_change_time) >= down_rate_us))
+	{
+		dprintk(SMARTASS_DEBUG_ALG,"smartassT @ %d ramp down: load %d (delta_idle %llu)\n",
+			old_freq,cpu_load,delta_idle);
+		this_smartass->ramp_dir = -1;
+		work_cpumask_set(cpu);
+		queue_work(down_wq, &freq_scale_work);
+		queued_work = 1;
+	}
+	else this_smartass->ramp_dir = 0;
+
+	// To avoid unnecessary load when the CPU is already at high load, we don't
+	// reset ourselves if we are at max speed. If and when there are idle cycles,
+	// the idle loop will activate the timer.
+	// Additionally, if we queued some work, the work task will reset the timer
+	// after it has done its adjustments.
+	if (!queued_work && old_freq < policy->max)
+		reset_timer(cpu,this_smartass);
+}
+
+static void cpufreq_idle(void)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	if (!this_smartass->enable) {
+		pm_idle_old();
+		return;
+	}
+
+	if (policy->cur == policy->min && timer_pending(&this_smartass->timer))
+		del_timer(&this_smartass->timer);
+
+	pm_idle_old();
+
+	if (!timer_pending(&this_smartass->timer))
+		reset_timer(smp_processor_id(), this_smartass);
+}
+
+static int cpufreq_idle_notifier(struct notifier_block *nb,
+	unsigned long val, void *data) {
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	if (!this_smartass->enable)
+		return NOTIFY_DONE;
+
+	if (val == IDLE_START) {
+		if (policy->cur == policy->max && !timer_pending(&this_smartass->timer)) {
+			reset_timer(smp_processor_id(), this_smartass);
+		} else if (policy->cur == policy->min) {
+			if (timer_pending(&this_smartass->timer))
+				del_timer(&this_smartass->timer);
+		}
+	} else if (val == IDLE_END) {
+		if (policy->cur == policy->min && !timer_pending(&this_smartass->timer))
+			reset_timer(smp_processor_id(), this_smartass);
+	}
+
+	return NOTIFY_OK;
+}
+static struct notifier_block cpufreq_idle_nb = {
+	.notifier_call = cpufreq_idle_notifier,
+};
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartass_freq_change_time_work(struct work_struct *work)
+{
+	unsigned int cpu;
+	int new_freq;
+	int old_freq;
+	int ramp_dir;
+	struct smartass_info_s *this_smartass;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_L;
+	for_each_possible_cpu(cpu) {
+		this_smartass = &per_cpu(smartass_info, cpu);
+		if (!work_cpumask_test_and_clear(cpu))
+			continue;
+
+		ramp_dir = this_smartass->ramp_dir;
+		this_smartass->ramp_dir = 0;
+
+		old_freq = this_smartass->old_freq;
+		policy = this_smartass->cur_policy;
+
+		if (old_freq != policy->cur) {
+			// frequency was changed by someone else?
+			printk(KERN_WARNING "Smartass: frequency changed by 3rd party: %d to %d\n",
+			       old_freq,policy->cur);
+			new_freq = old_freq;
+		}
+		else if (ramp_dir > 0 && nr_running() > 1) {
+			// ramp up logic:
+			if (old_freq < this_smartass->ideal_speed)
+				new_freq = this_smartass->ideal_speed;
+			else if (ramp_up_step) {
+				new_freq = old_freq + ramp_up_step;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else {
+				new_freq = policy->max;
+				relation = CPUFREQ_RELATION_H;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp up: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else if (ramp_dir < 0) {
+			// ramp down logic:
+			if (old_freq > this_smartass->ideal_speed) {
+				new_freq = this_smartass->ideal_speed;
+				relation = CPUFREQ_RELATION_H;
+			}
+			else if (ramp_down_step)
+				new_freq = old_freq - ramp_down_step;
+			else {
+				// Load heuristics: Adjust new_freq such that, assuming a linear
+				// scaling of load vs. frequency, the load in the new frequency
+				// will be max_cpu_load:
+				new_freq = old_freq * this_smartass->cur_cpu_load / max_cpu_load;
+				if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+					new_freq = old_freq -1;
+			}
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d ramp down: ramp_dir=%d ideal=%d\n",
+				old_freq,ramp_dir,this_smartass->ideal_speed);
+		}
+		else { // ramp_dir==0 ?! Could the timer change its mind about a queued ramp up/down
+		       // before the work task gets to run?
+		       // This may also happen if we refused to ramp up because the nr_running()==1
+			new_freq = old_freq;
+			dprintk(SMARTASS_DEBUG_ALG,"smartassQ @ %d nothing: ramp_dir=%d nr_running=%lu\n",
+				old_freq,ramp_dir,nr_running());
+		}
+
+		// do actual ramp up (returns 0, if frequency change failed):
+		new_freq = target_freq(policy,this_smartass,new_freq,old_freq,relation);
+		if (new_freq)
+			this_smartass->freq_change_time_in_idle =
+				get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		// reset timer:
+		if (new_freq < policy->max)
+			reset_timer(cpu,this_smartass);
+		// if we are maxed out, it is pointless to use the timer
+		// (idle cycles wake up the timer when the timer comes)
+		else if (timer_pending(&this_smartass->timer))
+			del_timer(&this_smartass->timer);
+
+		cpufreq_notify_utilization(policy,
+			(this_smartass->cur_cpu_load * policy->cur) / policy->max);
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	return res;
+}
+
+static ssize_t show_up_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", up_rate_us);
+}
+
+static ssize_t store_up_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate_us = input;
+	return res;
+}
+
+static ssize_t show_down_rate_us(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", down_rate_us);
+}
+
+static ssize_t store_down_rate_us(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate_us = input;
+	return res;
+}
+
+static ssize_t show_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_ideal_freq);
+}
+
+static ssize_t store_sleep_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		sleep_ideal_freq = input;
+		if (suspended)
+			smartass_update_min_max_allcpus();
+	}
+	return res;
+}
+
+static ssize_t show_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_wakeup_freq);
+}
+
+static ssize_t store_sleep_wakeup_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		sleep_wakeup_freq = input;
+	return res;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!suspended)
+			smartass_update_min_max_allcpus();
+	}
+	return res;
+}
+
+static ssize_t show_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", sample_rate_jiffies);
+}
+
+static ssize_t store_sample_rate_jiffies(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 1000)
+		sample_rate_jiffies = input;
+	return res;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	return res;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	return res;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	return res;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr, const char *buf, size_t count)
+{
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	return res;
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate_us);
+define_global_rw_attr(down_rate_us);
+define_global_rw_attr(sleep_ideal_freq);
+define_global_rw_attr(sleep_wakeup_freq);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(sample_rate_jiffies);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+
+static struct attribute * smartass_attributes[] = {
+	&debug_mask_attr.attr,
+	&up_rate_us_attr.attr,
+	&down_rate_us_attr.attr,
+	&sleep_ideal_freq_attr.attr,
+	&sleep_wakeup_freq_attr.attr,
+	&awake_ideal_freq_attr.attr,
+	&sample_rate_jiffies_attr.attr,
+	&ramp_up_step_attr.attr,
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr,
+	&min_cpu_load_attr.attr,
+	NULL,
+};
+
+static struct attribute_group smartass_attr_group = {
+	.attrs = smartass_attributes,
+	.name = "smartassH3",
+};
+
+static int cpufreq_governor_smartass_h3(struct cpufreq_policy *new_policy,
+		unsigned int event)
+{
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		this_smartass->cur_policy = new_policy;
+
+		this_smartass->enable = 1;
+
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		this_smartass->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_smartass->freq_table)
+			printk(KERN_WARNING "Smartass: no frequency table for cpu %d?!\n",cpu);
+
+		smp_wmb();
+
+		// Do not register the idle hook and create sysfs
+		// entries if we have already done so.
+		if (atomic_inc_return(&active_count) <= 1) {
+			rc = sysfs_create_group(cpufreq_global_kobject,
+						&smartass_attr_group);
+			if (rc)
+				return rc;
+
+			pm_idle_old = pm_idle;
+			pm_idle = cpufreq_idle;
+			idle_notifier_register(&cpufreq_idle_nb);
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		smartass_update_min_max(this_smartass,new_policy,suspended);
+
+		if (this_smartass->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartass->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTASS_DEBUG_JUMPS,"SmartassI: jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartass->cur_policy,
+						new_policy->min, CPUFREQ_RELATION_L);
+		}
+
+		if (this_smartass->cur_policy->cur < new_policy->max && !timer_pending(&this_smartass->timer))
+			reset_timer(cpu,this_smartass);
+
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_smartass->enable = 0;
+		smp_wmb();
+		del_timer(&this_smartass->timer);
+		flush_work(&freq_scale_work);
+		this_smartass->idle_exit_time = 0;
+
+		if (atomic_dec_return(&active_count) <= 1) {
+			sysfs_remove_group(cpufreq_global_kobject,
+					   &smartass_attr_group);
+			pm_idle = pm_idle_old;
+			idle_notifier_unregister(&cpufreq_idle_nb);
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static void smartass_suspend(int cpu, int suspend)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+	unsigned int new_freq;
+
+	if (!this_smartass->enable)
+		return;
+
+	smartass_update_min_max(this_smartass,policy,suspend);
+	if (!suspend) { // resume at max speed:
+		new_freq = validate_freq(policy,sleep_wakeup_freq);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: awaking at %d\n",new_freq);
+
+		__cpufreq_driver_target(policy, new_freq,
+					CPUFREQ_RELATION_L);
+	} else {
+		// to avoid wakeup issues with quick sleep/wakeup don't change actual frequency when entering sleep
+		// to allow some time to settle down. Instead we just reset our statistics (and reset the timer).
+		// Eventually, the timer will adjust the frequency if necessary.
+
+		this_smartass->freq_change_time_in_idle =
+			get_cpu_idle_time_us(cpu,&this_smartass->freq_change_time);
+
+		dprintk(SMARTASS_DEBUG_JUMPS,"SmartassS: suspending at %d\n",policy->cur);
+	}
+
+	reset_timer(smp_processor_id(),this_smartass);
+}
+
+static void smartass_early_suspend(struct early_suspend *handler) {
+	int i;
+	if (suspended || sleep_ideal_freq==0) // disable behavior for sleep_ideal_freq==0
+		return;
+	suspended = 1;
+	for_each_online_cpu(i)
+		smartass_suspend(i,1);
+}
+
+static void smartass_late_resume(struct early_suspend *handler) {
+	int i;
+	if (!suspended) // already not suspended so nothing to do
+		return;
+	suspended = 0;
+	for_each_online_cpu(i)
+		smartass_suspend(i,0);
+}
+
+static struct early_suspend smartass_power_suspend = {
+	.suspend = smartass_early_suspend,
+	.resume = smartass_late_resume,
+#ifdef CONFIG_MACH_HERO
+	.level = EARLY_SUSPEND_LEVEL_DISABLE_FB + 1,
+#endif
+};
+
+static int __init cpufreq_smartass_init(void)
+{
+	unsigned int i;
+	struct smartass_info_s *this_smartass;
+	debug_mask = 0;
+	up_rate_us = DEFAULT_UP_RATE_US;
+	down_rate_us = DEFAULT_DOWN_RATE_US;
+	sleep_ideal_freq = DEFAULT_SLEEP_IDEAL_FREQ;
+	sleep_wakeup_freq = DEFAULT_SLEEP_WAKEUP_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	sample_rate_jiffies = DEFAULT_SAMPLE_RATE_JIFFIES;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+
+	spin_lock_init(&cpumask_lock);
+
+	suspended = 0;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i) {
+		this_smartass = &per_cpu(smartass_info, i);
+		this_smartass->enable = 0;
+		this_smartass->cur_policy = 0;
+		this_smartass->ramp_dir = 0;
+		this_smartass->time_in_idle = 0;
+		this_smartass->idle_exit_time = 0;
+		this_smartass->freq_change_time = 0;
+		this_smartass->freq_change_time_in_idle = 0;
+		this_smartass->cur_cpu_load = 0;
+		// intialize timer:
+		init_timer_deferrable(&this_smartass->timer);
+		this_smartass->timer.function = cpufreq_smartass_timer;
+		this_smartass->timer.data = i;
+		work_cpumask_test_and_clear(i);
+	}
+
+	// Scale up is high priority
+	up_wq = create_workqueue("ksmartass_up");
+	down_wq = create_workqueue("ksmartass_down");
+	if (!up_wq || !down_wq)
+		return -ENOMEM;
+
+	INIT_WORK(&freq_scale_work, cpufreq_smartass_freq_change_time_work);
+
+	register_early_suspend(&smartass_power_suspend);
+
+	return cpufreq_register_governor(&cpufreq_gov_smartass_h3);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3
+fs_initcall(cpufreq_smartass_init);
+#else
+module_init(cpufreq_smartass_init);
+#endif
+
+static void __exit cpufreq_smartass_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_smartass_h3);
+	destroy_workqueue(up_wq);
+	destroy_workqueue(down_wq);
+}
+
+module_exit(cpufreq_smartass_exit);
+
+MODULE_AUTHOR ("Erasmux, moded by H3ROS & C3C0");
+MODULE_DESCRIPTION ("'cpufreq_smartassH3' - A smart cpufreq governor");
+MODULE_LICENSE ("GPL");
diff --git a/drivers/cpufreq/cpufreq_smartmax.c b/drivers/cpufreq/cpufreq_smartmax.c
new file mode 100644
index 00000000000..06471f4ea21
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartmax.c
@@ -0,0 +1,1400 @@
+/*
+ * drivers/cpufreq/cpufreq_smartmax.c
+ *
+ * Copyright (C) 2013 maxwen
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Author: maxwen
+ *
+ * Based on the ondemand and smartassV2 governor
+ *
+ * ondemand:
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *
+ * smartassV2:
+ * Author: Erasmux
+ *
+ * For a general overview of CPU governors see the relavent part in
+ * Documentation/cpu-freq/governors.txt
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <linux/jiffies.h>
+#include <linux/input.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/kernel_stat.h>
+
+/******************** Tunable parameters: ********************/
+
+/*
+ * The "ideal" frequency to use. The governor will ramp up faster
+ * towards the ideal frequency and slower after it has passed it. Similarly,
+ * lowering the frequency towards the ideal frequency is faster than below it.
+ */
+
+#define DEFAULT_SUSPEND_IDEAL_FREQ 422400
+#define DEFAULT_AWAKE_IDEAL_FREQ 1804800
+#define DEFAULT_RAMP_UP_STEP 307200
+#define DEFAULT_RAMP_DOWN_STEP 230400
+#define DEFAULT_MAX_CPU_LOAD 70
+#define DEFAULT_MIN_CPU_LOAD 40
+#define DEFAULT_UP_RATE 30000
+#define DEFAULT_DOWN_RATE 60000
+#define DEFAULT_SAMPLING_RATE 30000
+#define DEFAULT_INPUT_BOOST_DURATION 1200000
+#define DEFAULT_TOUCH_POKE_FREQ 2112000
+#define DEFAULT_BOOST_FREQ 2112000
+#define DEFAULT_IO_IS_BUSY 0
+#define DEFAULT_IGNORE_NICE 1
+
+static unsigned int suspend_ideal_freq;
+static unsigned int awake_ideal_freq;
+/*
+ * Freqeuncy delta when ramping up above the ideal freqeuncy.
+ * Zero disables and causes to always jump straight to max frequency.
+ * When below the ideal freqeuncy we always ramp up to the ideal freq.
+ */
+static unsigned int ramp_up_step;
+
+/*
+ * Freqeuncy delta when ramping down below the ideal freqeuncy.
+ * Zero disables and will calculate ramp down according to load heuristic.
+ * When above the ideal freqeuncy we always ramp down to the ideal freq.
+ */
+static unsigned int ramp_down_step;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+static unsigned int max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+static unsigned int min_cpu_load;
+
+/*
+ * The minimum amount of time in usecs to spend at a frequency before we can ramp up.
+ * Notice we ignore this when we are below the ideal frequency.
+ */
+static unsigned int up_rate;
+
+/*
+ * The minimum amount of time in usecs to spend at a frequency before we can ramp down.
+ * Notice we ignore this when we are above the ideal frequency.
+ */
+static unsigned int down_rate;
+
+/* in usecs */
+static unsigned int sampling_rate;
+
+/* in usecs */
+static unsigned int input_boost_duration;
+
+static unsigned int touch_poke_freq;
+static bool touch_poke = true;
+
+/*
+ * should ramp_up steps during boost be possible
+ */
+static bool ramp_up_during_boost = true;
+
+/*
+ * external boost interface - boost if duration is written
+ * to sysfs for boost_duration
+ */
+static unsigned int boost_freq;
+static bool boost = true;
+
+/* in usecs */
+static unsigned int boost_duration = 0;
+
+/* Consider IO as busy */
+static unsigned int io_is_busy;
+
+static unsigned int ignore_nice;
+
+/*************** End of tunables ***************/
+
+static unsigned int dbs_enable; /* number of CPUs using this policy */
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct smartmax_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct cpufreq_frequency_table *freq_table;
+	struct delayed_work work;
+	u64 prev_cpu_idle;
+	u64 prev_cpu_iowait;
+	u64 prev_cpu_wall;
+	u64 prev_cpu_nice;
+	u64 freq_change_time;
+	unsigned int cur_cpu_load;
+	unsigned int old_freq;
+	int ramp_dir;
+	unsigned int ideal_speed;
+	unsigned int cpu;
+	struct mutex timer_mutex;
+};
+static DEFINE_PER_CPU(struct smartmax_info_s, smartmax_info);
+
+#define dprintk(flag,msg...) do { \
+	if (debug_mask & flag) pr_info("[smartmax]" ":" msg); \
+	} while (0)
+
+enum {
+	SMARTMAX_DEBUG_JUMPS = 1,
+	SMARTMAX_DEBUG_LOAD = 2,
+	SMARTMAX_DEBUG_ALG = 4,
+	SMARTMAX_DEBUG_BOOST = 8,
+	SMARTMAX_DEBUG_INPUT = 16,
+	SMARTMAX_DEBUG_SUSPEND = 32
+};
+
+/*
+ * Combination of the above debug flags.
+ */
+//static unsigned long debug_mask = SMARTMAX_DEBUG_LOAD|SMARTMAX_DEBUG_JUMPS|SMARTMAX_DEBUG_ALG|SMARTMAX_DEBUG_BOOST|SMARTMAX_DEBUG_INPUT|SMARTMAX_DEBUG_SUSPEND;
+static unsigned long debug_mask;
+
+#define SMARTMAX_STAT 0
+#if SMARTMAX_STAT
+static u64 timer_stat[4] = {0, 0, 0, 0};
+#endif
+
+/*
+ * dbs_mutex protects dbs_enable in governor start/stop.
+ */
+static DEFINE_MUTEX(dbs_mutex);
+static struct workqueue_struct *smartmax_wq;
+
+static bool boost_task_alive = false;
+static struct task_struct *boost_task;
+static u64 boost_end_time = 0ULL;
+static unsigned int cur_boost_freq = 0;
+static unsigned int cur_boost_duration = 0;
+static bool boost_running = false;
+static unsigned int ideal_freq;
+static bool is_suspended = false;
+static unsigned int min_sampling_rate;
+
+#define LATENCY_MULTIPLIER			(1000)
+#define MIN_LATENCY_MULTIPLIER			(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers with CPUFREQ_ETERNAL)
+ * this governor will not work.
+ * All times here are in uS.
+ */
+#define MIN_SAMPLING_RATE_RATIO			(2)
+#define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
+
+static int cpufreq_governor_smartmax(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartmax = { 
+    .name = "smartmax", 
+    .governor = cpufreq_governor_smartmax, 
+    .max_transition_latency = TRANSITION_LATENCY_LIMIT, 
+    .owner = THIS_MODULE,
+    };
+
+static inline u64 get_cpu_iowait_time(unsigned int cpu, u64 *wall) {
+	u64 iowait_time = get_cpu_iowait_time_us(cpu, wall);
+
+	if (iowait_time == -1ULL)
+		return 0;
+
+	return iowait_time;
+}
+
+inline static void smartmax_update_min_max(
+		struct smartmax_info_s *this_smartmax, struct cpufreq_policy *policy) {
+	this_smartmax->ideal_speed = // ideal_freq; but make sure it obeys the policy min/max
+			policy->min < ideal_freq ?
+					(ideal_freq < policy->max ? ideal_freq : policy->max) :
+					policy->min;
+
+}
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time  = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = jiffies_to_usecs(cur_wall_time);
+
+	return jiffies_to_usecs(idle_time);
+}
+
+static inline u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, bool io_is_busy)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_is_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+
+inline static void smartmax_update_min_max_allcpus(void) {
+	unsigned int cpu;
+
+	for_each_online_cpu(cpu)
+	{
+		struct smartmax_info_s *this_smartmax = &per_cpu(smartmax_info, cpu);
+		if (this_smartmax->cur_policy){
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			smartmax_update_min_max(this_smartmax, this_smartmax->cur_policy);
+			
+			unlock_policy_rwsem_write(cpu);
+		}
+	}
+}
+
+inline static unsigned int validate_freq(struct cpufreq_policy *policy,
+		int freq) {
+	if (freq > (int) policy->max)
+		return policy->max;
+	if (freq < (int) policy->min)
+		return policy->min;
+	return freq;
+}
+
+/* We want all CPUs to do sampling nearly on same jiffy */
+static inline unsigned int get_timer_delay(void) {
+	unsigned int delay = usecs_to_jiffies(sampling_rate);
+
+	if (num_online_cpus() > 1)
+		delay -= jiffies % delay;
+	return delay;
+}
+
+static inline void dbs_timer_init(struct smartmax_info_s *this_smartmax) {
+	int delay = get_timer_delay();
+
+	INIT_DELAYED_WORK_DEFERRABLE(&this_smartmax->work, do_dbs_timer);
+	schedule_delayed_work_on(this_smartmax->cpu, &this_smartmax->work, delay);
+}
+
+static inline void dbs_timer_exit(struct smartmax_info_s *this_smartmax) {
+	cancel_delayed_work_sync(&this_smartmax->work);
+}
+
+inline static void target_freq(struct cpufreq_policy *policy,
+		struct smartmax_info_s *this_smartmax, int new_freq, int old_freq,
+		int prefered_relation) {
+	int index, target;
+	struct cpufreq_frequency_table *table = this_smartmax->freq_table;
+	unsigned int cpu = this_smartmax->cpu;
+
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+
+	// apply policy limits - just to be sure
+	new_freq = validate_freq(policy, new_freq);
+
+	if (!cpufreq_frequency_table_target(policy, table, new_freq,
+					prefered_relation, &index)) {
+		target = table[index].frequency;
+		if (target == old_freq) {
+			// if for example we are ramping up to *at most* current + ramp_up_step
+			// but there is no such frequency higher than the current, try also
+			// to ramp up to *at least* current + ramp_up_step.
+			if (new_freq > old_freq && prefered_relation == CPUFREQ_RELATION_H
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_L, &index))
+				target = table[index].frequency;
+			// simlarly for ramping down:
+			else if (new_freq < old_freq
+					&& prefered_relation == CPUFREQ_RELATION_L
+					&& !cpufreq_frequency_table_target(policy, table, new_freq,
+							CPUFREQ_RELATION_H, &index))
+				target = table[index].frequency;
+		}
+
+		// no change
+		if (target == old_freq)
+			return;
+	} else {
+		dprintk(SMARTMAX_DEBUG_ALG, "frequency change failed\n");
+		return;
+	}
+
+	dprintk(SMARTMAX_DEBUG_JUMPS, "%d: jumping to %d (%d) cpu %d\n", old_freq, new_freq, target, cpu);
+
+	__cpufreq_driver_target(policy, target, prefered_relation);
+
+	// remember last time we changed frequency
+	this_smartmax->freq_change_time = ktime_to_us(ktime_get());
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartmax_freq_change(struct smartmax_info_s *this_smartmax) {
+	unsigned int cpu;
+	unsigned int new_freq = 0;
+	unsigned int old_freq;
+	int ramp_dir;
+	struct cpufreq_policy *policy;
+	unsigned int relation = CPUFREQ_RELATION_L;
+
+	ramp_dir = this_smartmax->ramp_dir;
+	old_freq = this_smartmax->old_freq;
+	policy = this_smartmax->cur_policy;
+	cpu = this_smartmax->cpu;
+
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s\n", old_freq, __func__);
+	
+	if (old_freq != policy->cur) {
+		// frequency was changed by someone else?
+		dprintk(SMARTMAX_DEBUG_ALG, "%d: frequency changed by 3rd party to %d\n",
+				old_freq, policy->cur);
+		new_freq = old_freq;
+	} else if (ramp_dir > 0 && nr_running() > 1) {
+		// ramp up logic:
+		if (old_freq < this_smartmax->ideal_speed)
+			new_freq = this_smartmax->ideal_speed;
+		else if (ramp_up_step) {
+			new_freq = old_freq + ramp_up_step;
+			relation = CPUFREQ_RELATION_H;
+		} else {
+			new_freq = policy->max;
+			relation = CPUFREQ_RELATION_H;
+		}
+	} else if (ramp_dir < 0) {
+		// ramp down logic:
+		if (old_freq > this_smartmax->ideal_speed) {
+			new_freq = this_smartmax->ideal_speed;
+			relation = CPUFREQ_RELATION_H;
+		} else if (ramp_down_step)
+			new_freq = old_freq - ramp_down_step;
+		else {
+			// Load heuristics: Adjust new_freq such that, assuming a linear
+			// scaling of load vs. frequency, the load in the new frequency
+			// will be max_cpu_load:
+			new_freq = old_freq * this_smartmax->cur_cpu_load / max_cpu_load;
+			if (new_freq > old_freq) // min_cpu_load > max_cpu_load ?!
+				new_freq = old_freq - 1;
+		}
+	}
+
+	if (new_freq!=0){
+		target_freq(policy, this_smartmax, new_freq, old_freq, relation);
+	}
+	
+	this_smartmax->ramp_dir = 0;
+}
+
+static inline void cpufreq_smartmax_get_ramp_direction(struct smartmax_info_s *this_smartmax, u64 now)
+{
+	unsigned int cur_load = this_smartmax->cur_cpu_load;
+	unsigned int cur = this_smartmax->old_freq;
+	struct cpufreq_policy *policy = this_smartmax->cur_policy;
+	
+	// Scale up if load is above max or if there where no idle cycles since coming out of idle,
+	// additionally, if we are at or above the ideal_speed, verify we have been at this frequency
+	// for at least up_rate:
+	if (cur_load > max_cpu_load && cur < policy->max
+			&& (cur < this_smartmax->ideal_speed
+				|| (now - this_smartmax->freq_change_time) >= up_rate)) {
+		dprintk(SMARTMAX_DEBUG_ALG,
+				"%d: ramp up: load %d\n", cur, cur_load);
+		this_smartmax->ramp_dir = 1;
+	}
+	// Similarly for scale down: load should be below min and if we are at or below ideal
+	// frequency we require that we have been at this frequency for at least down_rate:
+	else if (cur_load < min_cpu_load && cur > policy->min
+			&& (cur > this_smartmax->ideal_speed
+				|| (now - this_smartmax->freq_change_time) >= down_rate)) {
+		dprintk(SMARTMAX_DEBUG_ALG,
+				"%d: ramp down: load %d\n", cur, cur_load);
+		this_smartmax->ramp_dir = -1;
+	}
+}
+
+static void cpufreq_smartmax_timer(struct smartmax_info_s *this_smartmax) {
+	unsigned int cur;
+	struct cpufreq_policy *policy = this_smartmax->cur_policy;
+	u64 now = ktime_to_us(ktime_get());
+	/* Extrapolated load of this CPU */
+	unsigned int load_at_max_freq = 0;
+	unsigned int j = 0;
+	unsigned int cpu = this_smartmax->cpu;
+
+#if SMARTMAX_STAT 
+	u64 diff = 0;
+
+	if (timer_stat[cpu])
+		diff = now - timer_stat[cpu];
+
+	timer_stat[cpu] = now;
+	printk(KERN_DEBUG "[smartmax]:cpu %d %lld\n", cpu, diff);
+#endif
+
+	cur = policy->cur;
+
+	dprintk(SMARTMAX_DEBUG_ALG, "%d: %s cpu %d %lld\n", cur, __func__, cpu, now);
+
+	for_each_cpu(j, policy->cpus)
+	{
+		struct smartmax_info_s *j_this_smartmax;
+		u64 cur_wall_time, cur_idle_time, cur_iowait_time;
+		unsigned int idle_time, wall_time, iowait_time;
+		unsigned int cur_load;
+		
+		j_this_smartmax = &per_cpu(smartmax_info, j);
+
+		cur_idle_time = get_cpu_idle_time(j, &cur_wall_time, io_is_busy);
+		cur_iowait_time = get_cpu_iowait_time(j, &cur_wall_time);
+
+		wall_time = cur_wall_time - j_this_smartmax->prev_cpu_wall;
+		j_this_smartmax->prev_cpu_wall = cur_wall_time;
+
+		idle_time = cur_idle_time - j_this_smartmax->prev_cpu_idle;
+		j_this_smartmax->prev_cpu_idle = cur_idle_time;
+
+		iowait_time = cur_iowait_time - j_this_smartmax->prev_cpu_iowait;
+		j_this_smartmax->prev_cpu_iowait = cur_iowait_time;
+
+		if (ignore_nice) {
+			u64 cur_nice;
+			unsigned long cur_nice_jiffies;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_30
+			cur_nice = kstat_cpu(j).cpustat.nice - j_this_smartmax->prev_cpu_nice;
+			cur_nice_jiffies = (unsigned long) cputime64_to_jiffies64(cur_nice);
+
+			j_this_smartmax->prev_cpu_nice = kstat_cpu(j).cpustat.nice;
+#else
+			cur_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE] - j_this_smartmax->prev_cpu_nice;
+			cur_nice_jiffies = (unsigned long) cputime64_to_jiffies64(cur_nice);
+
+			j_this_smartmax->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+
+#endif
+
+			idle_time += jiffies_to_usecs(cur_nice_jiffies);
+		}
+
+		/*
+		 * For the purpose of ondemand, waiting for disk IO is an
+		 * indication that you're performance critical, and not that
+		 * the system is actually idle. So subtract the iowait time
+		 * from the cpu idle time.
+		 */
+		if (io_is_busy && idle_time >= iowait_time)
+			idle_time -= iowait_time;
+
+		if (unlikely(!wall_time || wall_time < idle_time))
+			continue;
+
+		cur_load = 100 * (wall_time - idle_time) / wall_time;
+		j_this_smartmax->cur_cpu_load = cur_load;
+	}
+
+	/* calculate the scaled load across CPU */
+	load_at_max_freq = (this_smartmax->cur_cpu_load * policy->cur)/policy->cpuinfo.max_freq;
+
+	cpufreq_notify_utilization(policy, load_at_max_freq);
+
+	dprintk(SMARTMAX_DEBUG_LOAD, "%d: load %d\n", cpu, this_smartmax->cur_cpu_load);
+
+	this_smartmax->old_freq = cur;
+	this_smartmax->ramp_dir = 0;
+
+	cpufreq_smartmax_get_ramp_direction(this_smartmax, now);
+
+	// no changes
+	if (this_smartmax->ramp_dir == 0)		
+		return;
+
+	// boost - but not block ramp up steps based on load if requested
+	if (boost_running){
+		if (now < boost_end_time) {
+			dprintk(SMARTMAX_DEBUG_BOOST, "%d: cpu %d boost running %llu %llu\n", cur, cpu, now, boost_end_time);
+		
+			if (this_smartmax->ramp_dir == -1)
+				return;
+			else {
+				if (ramp_up_during_boost)
+					dprintk(SMARTMAX_DEBUG_BOOST, "%d: cpu %d boost running but ramp_up above boost freq requested\n", cur, cpu);
+				else
+					return;
+			}
+		} else
+			boost_running = false;
+	}
+
+	cpufreq_smartmax_freq_change(this_smartmax);
+}
+
+static void do_dbs_timer(struct work_struct *work) {
+	struct smartmax_info_s *this_smartmax =
+			container_of(work, struct smartmax_info_s, work.work);
+	unsigned int cpu = this_smartmax->cpu;
+	int delay = get_timer_delay();
+
+	mutex_lock(&this_smartmax->timer_mutex);
+
+	cpufreq_smartmax_timer(this_smartmax);
+
+	queue_delayed_work_on(cpu, smartmax_wq, &this_smartmax->work, delay);
+	mutex_unlock(&this_smartmax->timer_mutex);
+}
+
+static void update_idle_time(bool online) {
+	int j = 0;
+
+	for_each_possible_cpu(j)
+	{
+		struct smartmax_info_s *j_this_smartmax;
+
+		if (online && !cpu_online(j)) {
+			continue;
+		}
+		j_this_smartmax = &per_cpu(smartmax_info, j);
+
+		j_this_smartmax->prev_cpu_idle = get_cpu_idle_time(j,
+				&j_this_smartmax->prev_cpu_wall, io_is_busy);
+				
+		if (ignore_nice)
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_30
+			j_this_smartmax->prev_cpu_nice = kstat_cpu(j) .cpustat.nice;
+#else
+			j_this_smartmax->prev_cpu_nice = kcpustat_cpu(j).cpustat[CPUTIME_NICE];
+#endif
+	}
+}
+
+static ssize_t show_debug_mask(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%lu\n", debug_mask);
+}
+
+static ssize_t store_debug_mask(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+		debug_mask = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_up_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", up_rate);
+}
+
+static ssize_t store_up_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		up_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_down_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", down_rate);
+}
+
+static ssize_t store_down_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0 && input <= 100000000)
+		down_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", awake_ideal_freq);
+}
+
+static ssize_t store_awake_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		awake_ideal_freq = input;
+		if (!is_suspended){
+			ideal_freq = awake_ideal_freq;
+			smartmax_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", suspend_ideal_freq);
+}
+
+static ssize_t store_suspend_ideal_freq(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0) {
+		suspend_ideal_freq = input;
+		if (is_suspended){
+			ideal_freq = suspend_ideal_freq;
+			smartmax_update_min_max_allcpus();
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_up_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_down_step(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", ramp_down_step);
+}
+
+static ssize_t store_ramp_down_step(struct kobject *kobj,
+		struct attribute *attr, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+		ramp_down_step = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+		max_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+		min_cpu_load = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", sampling_rate);
+}
+
+static ssize_t store_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= min_sampling_rate)
+		sampling_rate = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_touch_poke_freq(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", touch_poke_freq);
+}
+
+static ssize_t store_touch_poke_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0){
+		touch_poke_freq = input;
+	
+		if (touch_poke_freq == 0)
+			touch_poke = false;
+		else
+			touch_poke = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_input_boost_duration(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%u\n", input_boost_duration);
+}
+
+static ssize_t store_input_boost_duration(struct kobject *a,
+		struct attribute *b, const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000)
+		input_boost_duration = input;
+	else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_ramp_up_during_boost(struct kobject *kobj,
+		struct attribute *attr, char *buf) {
+	return sprintf(buf, "%d\n", ramp_up_during_boost);
+}
+
+static ssize_t store_ramp_up_during_boost(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input == 0)
+			ramp_up_during_boost = false;
+		else if (input == 1)
+			ramp_up_during_boost = true;
+		else
+			return -EINVAL;
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_boost_freq(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%u\n", boost_freq);
+}
+
+static ssize_t store_boost_freq(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		boost_freq = input;
+		if (boost_freq == 0)
+			boost = false;
+		else
+			boost = true;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_boost_duration(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", boost_running);
+}
+
+static ssize_t store_boost_duration(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 10000){
+		boost_duration = input;
+		if (boost) {
+			// no need to bother if currently a boost is running anyway
+			if (boost_task_alive && boost_running)
+				return count;
+
+			if (boost_task_alive) {
+				cur_boost_freq = boost_freq;
+				cur_boost_duration = boost_duration;
+				wake_up_process(boost_task);
+			}
+		}
+	} else
+		return -EINVAL;
+	return count;
+}
+
+static ssize_t show_io_is_busy(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", io_is_busy);
+}
+
+static ssize_t store_io_is_busy(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == io_is_busy) { /* nothing to do */
+			return count;
+		}
+		io_is_busy = input;
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_ignore_nice(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", ignore_nice);
+}
+
+static ssize_t store_ignore_nice(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	ssize_t res;
+	unsigned long input;
+
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0) {
+		if (input > 1)
+			input = 1;
+		if (input == ignore_nice) { /* nothing to do */
+			return count;
+		}
+		ignore_nice = input;
+		/* we need to re-evaluate prev_cpu_idle */
+		update_idle_time(true);
+	} else
+		return -EINVAL;	
+	return count;
+}
+
+static ssize_t show_min_sampling_rate(struct kobject *kobj, struct attribute *attr,
+		char *buf) {
+	return sprintf(buf, "%d\n", min_sampling_rate);
+}
+
+static ssize_t store_min_sampling_rate(struct kobject *a, struct attribute *b,
+		const char *buf, size_t count) {
+	return -EINVAL;	
+}
+
+#define define_global_rw_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0644, show_##_name, store_##_name)
+
+#define define_global_ro_attr(_name)		\
+static struct global_attr _name##_attr =	\
+	__ATTR(_name, 0444, show_##_name, store_##_name)
+
+define_global_rw_attr(debug_mask);
+define_global_rw_attr(up_rate);
+define_global_rw_attr(down_rate);
+define_global_rw_attr(ramp_up_step);
+define_global_rw_attr(ramp_down_step);
+define_global_rw_attr(max_cpu_load);
+define_global_rw_attr(min_cpu_load);
+define_global_rw_attr(sampling_rate);
+define_global_rw_attr(touch_poke_freq);
+define_global_rw_attr(input_boost_duration);
+define_global_rw_attr(boost_freq);
+define_global_rw_attr(boost_duration);
+define_global_rw_attr(io_is_busy);
+define_global_rw_attr(ignore_nice);
+define_global_rw_attr(ramp_up_during_boost);
+define_global_rw_attr(awake_ideal_freq);
+define_global_rw_attr(suspend_ideal_freq);
+define_global_ro_attr(min_sampling_rate);
+
+static struct attribute * smartmax_attributes[] = { 
+	&debug_mask_attr.attr,
+	&up_rate_attr.attr, 
+	&down_rate_attr.attr, 
+	&ramp_up_step_attr.attr, 
+	&ramp_down_step_attr.attr,
+	&max_cpu_load_attr.attr, 
+	&min_cpu_load_attr.attr,
+	&sampling_rate_attr.attr, 
+	&touch_poke_freq_attr.attr,
+	&input_boost_duration_attr.attr, 
+	&boost_freq_attr.attr, 
+	&boost_duration_attr.attr, 
+	&io_is_busy_attr.attr,
+	&ignore_nice_attr.attr, 
+	&ramp_up_during_boost_attr.attr, 
+	&awake_ideal_freq_attr.attr,
+	&suspend_ideal_freq_attr.attr,		
+	&min_sampling_rate_attr.attr,
+	NULL , };
+
+static struct attribute_group smartmax_attr_group = { 
+	.attrs = smartmax_attributes, 
+	.name = "smartmax", 
+	};
+
+static int cpufreq_smartmax_boost_task(void *data) {
+	struct smartmax_info_s *this_smartmax;
+	u64 now;
+	struct cpufreq_policy *policy;
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_TEGRA
+	unsigned int cpu;
+	bool start_boost = false;
+#endif
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+
+		if (kthread_should_stop())
+			break;
+
+		set_current_state(TASK_RUNNING);
+
+		if (boost_running)
+			continue;
+
+#ifdef CONFIG_CPU_FREQ_GOV_SMARTMAX_TEGRA
+		/* on tegra there is only one cpu clock so we only need to boost cpu 0 
+		   all others will run at the same speed */
+		this_smartmax = &per_cpu(smartmax_info, 0);
+		if (!this_smartmax)
+			continue;
+
+		policy = this_smartmax->cur_policy;
+		if (!policy)
+			continue;
+
+        if (lock_policy_rwsem_write(0) < 0)
+        	continue;
+		
+		tegra_input_boost(policy, cur_boost_freq, CPUFREQ_RELATION_H);
+	
+        this_smartmax->prev_cpu_idle = get_cpu_idle_time(0,
+						&this_smartmax->prev_cpu_wall, io_is_busy);
+
+        unlock_policy_rwsem_write(0);
+#else		
+		for_each_online_cpu(cpu){
+			this_smartmax = &per_cpu(smartmax_info, cpu);
+			if (!this_smartmax)
+				continue;
+
+			if (lock_policy_rwsem_write(cpu) < 0)
+				continue;
+
+			policy = this_smartmax->cur_policy;
+			if (!policy){
+				unlock_policy_rwsem_write(cpu);
+				continue;
+			}
+
+			mutex_lock(&this_smartmax->timer_mutex);
+
+			if (policy->cur < cur_boost_freq) {
+				start_boost = true;
+				dprintk(SMARTMAX_DEBUG_BOOST, "input boost cpu %d to %d\n", cpu, cur_boost_freq);
+				target_freq(policy, this_smartmax, cur_boost_freq, this_smartmax->old_freq, CPUFREQ_RELATION_H);
+				this_smartmax->prev_cpu_idle = get_cpu_idle_time(cpu, &this_smartmax->prev_cpu_wall, io_is_busy);
+			}
+			mutex_unlock(&this_smartmax->timer_mutex);
+
+			unlock_policy_rwsem_write(cpu);
+		}
+#endif
+
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_TEGRA
+		if (start_boost) {
+#endif
+
+		boost_running = true;
+		now = ktime_to_us(ktime_get());
+		boost_end_time = now + (cur_boost_duration * num_online_cpus());
+		dprintk(SMARTMAX_DEBUG_BOOST, "%s %llu %llu\n", __func__, now, boost_end_time);
+		
+#ifndef CONFIG_CPU_FREQ_GOV_SMARTMAX_TEGRA
+		}
+#endif
+	}
+
+	pr_info("[smartmax]:" "%s boost_thread stopped\n", __func__);
+	return 0;
+}
+
+#ifdef CONFIG_INPUT_MEDIATOR
+
+static void smartmax_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value) {
+	if (touch_poke && type == EV_SYN && code == SYN_REPORT) {
+		// no need to bother if currently a boost is running anyway
+		if (boost_task_alive && boost_running)
+			return;
+
+		if (boost_task_alive) {
+			cur_boost_freq = touch_poke_freq;
+			cur_boost_duration = input_boost_duration;
+			wake_up_process(boost_task);
+		}
+	}
+}
+
+static struct input_mediator_handler smartmax_input_mediator_handler = {
+	.event = smartmax_input_event,
+	};
+
+#else
+
+static void dbs_input_event(struct input_handle *handle, unsigned int type,
+		unsigned int code, int value) {
+	if (touch_poke && type == EV_SYN && code == SYN_REPORT) {
+		// no need to bother if currently a boost is running anyway
+		if (boost_task_alive && boost_running)
+			return;
+
+		if (boost_task_alive) {
+			cur_boost_freq = touch_poke_freq;
+			cur_boost_duration = input_boost_duration;
+			wake_up_process(boost_task);
+		}
+	}
+}
+
+static int dbs_input_connect(struct input_handler *handler,
+		struct input_dev *dev, const struct input_device_id *id) {
+	struct input_handle *handle;
+	int error;
+
+	pr_info("[smartmax]:" "%s input connect to %s\n", __func__, dev->name);
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpufreq";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+	err1: input_unregister_handle(handle);
+	err2: kfree(handle);
+	pr_err("[smartmax]:" "%s faild to connect input handler %d\n", __func__, error);
+	return error;
+}
+
+static void dbs_input_disconnect(struct input_handle *handle) {
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id dbs_ids[] = {
+{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_KEYBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.keybit = { [BIT_WORD(BTN_TOUCH)] = BIT_MASK(BTN_TOUCH) },
+		.absbit = { [BIT_WORD(ABS_X)] =
+			    BIT_MASK(ABS_X) | BIT_MASK(ABS_Y) },
+	}, /* touchpad */
+	{ },
+};
+
+static struct input_handler dbs_input_handler = { 
+	.event = dbs_input_event,
+	.connect = dbs_input_connect, 
+	.disconnect = dbs_input_disconnect,
+	.name = "cpufreq_smartmax", 
+	.id_table = dbs_ids, 
+	};
+#endif
+
+static int cpufreq_governor_smartmax(struct cpufreq_policy *new_policy,
+		unsigned int event) {
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+	struct smartmax_info_s *this_smartmax = &per_cpu(smartmax_info, cpu);
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+    unsigned int latency;
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		mutex_lock(&dbs_mutex);
+
+		this_smartmax->cur_policy = new_policy;
+		this_smartmax->cpu = cpu;
+
+		smartmax_update_min_max(this_smartmax,new_policy);
+
+		this_smartmax->freq_table = cpufreq_frequency_get_table(cpu);
+		if (!this_smartmax->freq_table){
+			mutex_unlock(&dbs_mutex);
+			return -EINVAL;
+		}
+
+		update_idle_time(false);
+
+		dbs_enable++;
+		
+		if (dbs_enable == 1) {
+			if (!boost_task_alive) {
+				boost_task = kthread_create (
+						cpufreq_smartmax_boost_task,
+						NULL,
+						"smartmax_input_boost_task"
+				);
+
+				if (IS_ERR(boost_task)) {
+					dbs_enable--;
+					mutex_unlock(&dbs_mutex);
+					return PTR_ERR(boost_task);
+				}
+
+				pr_info("[smartmax]:" "%s input boost task created\n", __func__);
+				sched_setscheduler_nocheck(boost_task, SCHED_FIFO, &param);
+				get_task_struct(boost_task);
+				boost_task_alive = true;
+			}
+#ifdef CONFIG_INPUT_MEDIATOR
+			input_register_mediator_secondary(&smartmax_input_mediator_handler);
+#else
+			rc = input_register_handler(&dbs_input_handler);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+#endif
+			rc = sysfs_create_group(cpufreq_global_kobject,
+					&smartmax_attr_group);
+			if (rc) {
+				dbs_enable--;
+				mutex_unlock(&dbs_mutex);
+				return rc;
+			}
+			/* policy latency is in nS. Convert it to uS first */
+			latency = new_policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			/* Bring kernel and HW constraints together */
+			min_sampling_rate = max(min_sampling_rate, MIN_LATENCY_MULTIPLIER * latency);
+			sampling_rate = max(min_sampling_rate, sampling_rate);
+		}
+
+		mutex_unlock(&dbs_mutex);
+		dbs_timer_init(this_smartmax);
+
+		break;
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&this_smartmax->timer_mutex);
+		smartmax_update_min_max(this_smartmax,new_policy);
+
+		if (this_smartmax->cur_policy->cur > new_policy->max) {
+			dprintk(SMARTMAX_DEBUG_JUMPS,"CPUFREQ_GOV_LIMITS jumping to new max freq: %d\n",new_policy->max);
+			__cpufreq_driver_target(this_smartmax->cur_policy,
+					new_policy->max, CPUFREQ_RELATION_H);
+		}
+		else if (this_smartmax->cur_policy->cur < new_policy->min) {
+			dprintk(SMARTMAX_DEBUG_JUMPS,"CPUFREQ_GOV_LIMITS jumping to new min freq: %d\n",new_policy->min);
+			__cpufreq_driver_target(this_smartmax->cur_policy,
+					new_policy->min, CPUFREQ_RELATION_L);
+		}
+		mutex_unlock(&this_smartmax->timer_mutex);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		dbs_timer_exit(this_smartmax);
+
+		mutex_lock(&dbs_mutex);
+		this_smartmax->cur_policy = NULL;
+		dbs_enable--;
+
+		if (!dbs_enable){
+			if (boost_task_alive)
+				kthread_stop(boost_task);
+
+			sysfs_remove_group(cpufreq_global_kobject, &smartmax_attr_group);
+#ifdef CONFIG_INPUT_MEDIATOR
+			input_unregister_mediator_secondary(&smartmax_input_mediator_handler);
+#else
+			input_unregister_handler(&dbs_input_handler);
+#endif
+		}
+		
+		mutex_unlock(&dbs_mutex);
+		break;
+	}
+
+	return 0;
+}
+
+static int __init cpufreq_smartmax_init(void) {
+	unsigned int i;
+	struct smartmax_info_s *this_smartmax;
+	u64 wall;
+	u64 idle_time;
+	int cpu = get_cpu();
+
+	idle_time = get_cpu_idle_time_us(cpu, &wall);
+	put_cpu();
+	if (idle_time != -1ULL) {
+		/*
+		 * In no_hz/micro accounting case we set the minimum frequency
+		 * not depending on HZ, but fixed (very low). The deferred
+		 * timer might skip some samples if idle/sleeping as needed.
+		*/
+		min_sampling_rate = MICRO_FREQUENCY_MIN_SAMPLE_RATE;
+	} else {
+		/* For correct statistics, we need 10 ticks for each measure */
+		min_sampling_rate = MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);
+	}
+
+	smartmax_wq = alloc_workqueue("smartmax_wq", WQ_HIGHPRI, 0);
+	if (!smartmax_wq) {
+		printk(KERN_ERR "Failed to create smartmax_wq workqueue\n");
+		return -EFAULT;
+	}
+
+	up_rate = DEFAULT_UP_RATE;
+	down_rate = DEFAULT_DOWN_RATE;
+	suspend_ideal_freq = DEFAULT_SUSPEND_IDEAL_FREQ;
+	awake_ideal_freq = DEFAULT_AWAKE_IDEAL_FREQ;
+	ideal_freq = awake_ideal_freq;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	ramp_down_step = DEFAULT_RAMP_DOWN_STEP;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+	sampling_rate = DEFAULT_SAMPLING_RATE;
+	input_boost_duration = DEFAULT_INPUT_BOOST_DURATION;
+	io_is_busy = DEFAULT_IO_IS_BUSY;
+	ignore_nice = DEFAULT_IGNORE_NICE;
+	touch_poke_freq = DEFAULT_TOUCH_POKE_FREQ;
+	boost_freq = DEFAULT_BOOST_FREQ;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i)
+	{
+		this_smartmax = &per_cpu(smartmax_info, i);
+		this_smartmax->cur_policy = NULL;
+		this_smartmax->ramp_dir = 0;
+		this_smartmax->freq_change_time = 0;
+		this_smartmax->cur_cpu_load = 0;
+		mutex_init(&this_smartmax->timer_mutex);
+	}
+
+	return cpufreq_register_governor(&cpufreq_gov_smartmax);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX
+fs_initcall(cpufreq_smartmax_init);
+#else
+module_init(cpufreq_smartmax_init);
+#endif
+
+static void __exit cpufreq_smartmax_exit(void) {
+	unsigned int i;
+	struct smartmax_info_s *this_smartmax;
+
+	cpufreq_unregister_governor(&cpufreq_gov_smartmax);
+
+	for_each_possible_cpu(i)
+	{
+		this_smartmax = &per_cpu(smartmax_info, i);
+		mutex_destroy(&this_smartmax->timer_mutex);
+	}
+	destroy_workqueue(smartmax_wq);
+}
+
+module_exit(cpufreq_smartmax_exit);
+
+MODULE_AUTHOR("maxwen");
+MODULE_DESCRIPTION("'cpufreq_smartmax' - A smart cpufreq governor");
+MODULE_LICENSE("GPL");
diff --git a/drivers/input/touchscreen/synaptics/synaptics_i2c_rmi.c b/drivers/input/touchscreen/synaptics/synaptics_i2c_rmi.c
index 8816236f40b..9804dd987f7 100644
--- a/drivers/input/touchscreen/synaptics/synaptics_i2c_rmi.c
+++ b/drivers/input/touchscreen/synaptics/synaptics_i2c_rmi.c
@@ -165,8 +165,10 @@ static ssize_t synaptics_rmi4_0dbutton_show(struct device *dev,
 static ssize_t synaptics_rmi4_0dbutton_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count);
 
+#ifndef CONFIG_HAS_EARLYSUSPEND
 static ssize_t synaptics_rmi4_suspend_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count);
+#endif
 
 static struct device_attribute attrs[] = {
 	__ATTR(regval, (S_IRUGO | S_IWUSR | S_IWGRP),
@@ -214,9 +216,11 @@ static struct device_attribute attrs[] = {
 	__ATTR(0dbutton, (S_IRUGO | S_IWUSR | S_IWGRP),
 			synaptics_rmi4_0dbutton_show,
 			synaptics_rmi4_0dbutton_store),
+#ifndef CONFIG_HAS_EARLYSUSPEND
 	__ATTR(suspend, S_IWUSR | S_IWGRP,
 			synaptics_rmi4_show_error,
 			synaptics_rmi4_suspend_store),
+#endif
 };
 
 #ifdef READ_LCD_ID
@@ -1209,6 +1213,7 @@ static ssize_t synaptics_rmi4_0dbutton_store(struct device *dev,
 	return count;
 }
 
+#ifndef CONFIG_HAS_EARLYSUSPEND
 static ssize_t synaptics_rmi4_suspend_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
 {
@@ -1226,6 +1231,7 @@ static ssize_t synaptics_rmi4_suspend_store(struct device *dev,
 
 	return count;
 }
+#endif
 
 /**
  * synaptics_rmi4_set_page()
diff --git a/include/asm-generic/cputime.h b/include/asm-generic/cputime.h
index 9a62937c56c..7ea0bfd5fd0 100644
--- a/include/asm-generic/cputime.h
+++ b/include/asm-generic/cputime.h
@@ -15,6 +15,7 @@ typedef u64 __nocast cputime64_t;
 
 #define cputime64_to_jiffies64(__ct)	(__force u64)(__ct)
 #define jiffies64_to_cputime64(__jif)	(__force cputime64_t)(__jif)
+#define cputime64_sub(__a, __b)    ((__a) - (__b))
 
 #define nsecs_to_cputime64(__ct)	\
 	jiffies64_to_cputime64(nsecs_to_jiffies64(__ct))
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index 3c1394f57fd..9ba2969fa34 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -455,12 +455,51 @@ extern struct cpufreq_governor cpufreq_gov_userspace;
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND)
 extern struct cpufreq_governor cpufreq_gov_ondemand;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ondemand)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMANDPLUS)
+extern struct cpufreq_governor cpufreq_gov_ondemandplus;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_ondemandplus)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE)
 extern struct cpufreq_governor cpufreq_gov_conservative;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_conservative)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE)
 extern struct cpufreq_governor cpufreq_gov_interactive;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVEX)
+extern struct cpufreq_governor cpufreq_gov_interactivex;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactivex)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIACTIVE)
+extern struct cpufreq_governor cpufreq_gov_intelliactive;
+#define CPUFREQ_DEFAULT_GOVERNOR        (&cpufreq_gov_intelliactive)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_KTOONSERVATIVE)
+extern struct cpufreq_governor cpufreq_gov_ktoonservative;
+#define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_ktoonservative)
+##elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_NIGHTMARE)
+extern struct cpufreq_governor cpufreq_gov_nightmare;
+#define CPUFREQ_DEFAULT_GOVERNOR (&cpufreq_gov_nightmare)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS2)
+extern struct cpufreq_governor cpufreq_gov_smartass2;
+#define CPUFREQ_DEFAULT_GOVERNOR  (&cpufreq_gov_smartass2)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASSH3)
+extern struct cpufreq_governor cpufreq_gov_smartassH3;
+#define CPUFREQ_DEFAULT_GOVERNOR  (&cpufreq_gov_smartassH3)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTELLIDEMAND)
+extern struct cpufreq_governor cpufreq_gov_intellidemand;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_INTELLIDEMAND)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_ABYSSPLUG)
+extern struct cpufreq_governor cpufreq_gov_abyssplugv2;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_abyssplugv2)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_DANCEDANCE)
+extern struct cpufreq_governor cpufreq_gov_dancedance;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_DANCEDANCE)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_LIONHEART)
+extern struct cpufreq_governor cpufreq_gov_lionheart;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_LIONHEART)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTMAX)
+extern struct cpufreq_governor cpufreq_gov_smartmax;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_SMARTMAX)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_HYPER)
+extern struct cpufreq_governor cpufreq_gov_hyper;
+#define CPUFREQ_DEFAULT_GOVERNOR 	(&cpufreq_gov_hyper)
 #endif
 
 
diff --git a/include/trace/events/cpufreq_interactivex.h b/include/trace/events/cpufreq_interactivex.h
new file mode 100644
index 00000000000..11866468c4e
--- /dev/null
+++ b/include/trace/events/cpufreq_interactivex.h
@@ -0,0 +1,150 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_interactivex
+
+#if !defined(_TRACE_CPUFREQ_interactivex_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_interactivex_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_interactivex_setspeed,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+		    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curtarg   )
+		    __field(unsigned long, curactual )
+		    __field(unsigned long, newtarg   )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curtarg = curtarg;
+		    __entry->curactual = curactual;
+		    __entry->newtarg = newtarg;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu actual=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curtarg,
+		      __entry->curactual, __entry->newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactivex_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactivex_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_interactivex_notyet,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curtarg, unsigned long curactual,
+		     unsigned long newtarg),
+	    TP_ARGS(cpu_id, load, curtarg, curactual, newtarg)
+);
+
+DECLARE_EVENT_CLASS(modeeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+		    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id      )
+		    __field(unsigned long, total_load  )
+		    __field(unsigned long, single_enter)
+		    __field(unsigned long, multi_enter )
+		    __field(unsigned long, single_exit )
+		    __field(unsigned long, multi_exit  )
+		    __field(unsigned long, mode)
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->total_load = total_load;
+		    __entry->single_enter = single_enter;
+		    __entry->multi_enter = multi_enter;
+		    __entry->single_exit = single_exit;
+		    __entry->multi_exit = multi_exit;
+		    __entry->mode = mode ;
+	    ),
+
+	    TP_printk("cpu=%lu load=%3lu s_en=%6lu m_en=%6lu s_ex=%6lu m_ex=%6lu ret=%lu",
+		      __entry->cpu_id, __entry->total_load, __entry->single_enter,
+		      __entry->multi_enter, __entry->single_exit, __entry->multi_exit, __entry->mode)
+);
+
+DEFINE_EVENT(modeeval, cpufreq_interactivex_mode,
+	    TP_PROTO(unsigned long cpu_id, unsigned long total_load,
+		     unsigned long single_enter, unsigned long multi_enter,
+		     unsigned long single_exit, unsigned long multi_exit, unsigned long mode),
+	    TP_ARGS(cpu_id, total_load, single_enter, multi_enter, single_exit, multi_exit, mode)
+);
+
+TRACE_EVENT(cpufreq_interactivex_boost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+TRACE_EVENT(cpufreq_interactivex_unboost,
+	    TP_PROTO(const char *s),
+	    TP_ARGS(s),
+	    TP_STRUCT__entry(
+		    __string(s, s)
+	    ),
+	    TP_fast_assign(
+		    __assign_str(s, s);
+	    ),
+	    TP_printk("%s", __get_str(s))
+);
+
+#endif /* _TRACE_CPUFREQ_interactivex_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/include/trace/events/cpufreq_ondemandplus.h b/include/trace/events/cpufreq_ondemandplus.h
new file mode 100644
index 00000000000..ddde8a8a0ca
--- /dev/null
+++ b/include/trace/events/cpufreq_ondemandplus.h
@@ -0,0 +1,82 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM cpufreq_ondemandplus
+
+#if !defined(_TRACE_CPUFREQ_ONDEMANDPLUS_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_CPUFREQ_ONDEMANDPLUS_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(set,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	         unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq),
+
+	TP_STRUCT__entry(
+	    __field(          u32, cpu_id    )
+	    __field(unsigned long, targfreq   )
+	    __field(unsigned long, actualfreq )
+	   ),
+
+	TP_fast_assign(
+	    __entry->cpu_id = (u32) cpu_id;
+	    __entry->targfreq = targfreq;
+	    __entry->actualfreq = actualfreq;
+	),
+
+	TP_printk("cpu=%u targ=%lu actual=%lu",
+	      __entry->cpu_id, __entry->targfreq,
+	      __entry->actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_ondemandplus_up,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DEFINE_EVENT(set, cpufreq_ondemandplus_down,
+	TP_PROTO(u32 cpu_id, unsigned long targfreq,
+	     unsigned long actualfreq),
+	TP_ARGS(cpu_id, targfreq, actualfreq)
+);
+
+DECLARE_EVENT_CLASS(loadeval,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curfreq, unsigned long targfreq),
+	    TP_ARGS(cpu_id, load, curfreq, targfreq),
+
+	    TP_STRUCT__entry(
+		    __field(unsigned long, cpu_id    )
+		    __field(unsigned long, load      )
+		    __field(unsigned long, curfreq   )
+		    __field(unsigned long, targfreq  )
+	    ),
+
+	    TP_fast_assign(
+		    __entry->cpu_id = cpu_id;
+		    __entry->load = load;
+		    __entry->curfreq = curfreq;
+		    __entry->targfreq = targfreq;
+	    ),
+
+	    TP_printk("cpu=%lu load=%lu cur=%lu targ=%lu",
+		      __entry->cpu_id, __entry->load, __entry->curfreq,
+		      __entry->targfreq)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ondemandplus_target,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curfreq, unsigned long targfreq),
+	    TP_ARGS(cpu_id, load, curfreq, targfreq)
+);
+
+DEFINE_EVENT(loadeval, cpufreq_ondemandplus_already,
+	    TP_PROTO(unsigned long cpu_id, unsigned long load,
+		     unsigned long curfreq, unsigned long targfreq),
+	    TP_ARGS(cpu_id, load, curfreq, targfreq)
+);
+
+#endif /* _TRACE_CPUFREQ_ONDEMANDPLUS_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/kernel/power/Kconfig b/kernel/power/Kconfig
index 1051a06c12f..6153986d8b6 100644
--- a/kernel/power/Kconfig
+++ b/kernel/power/Kconfig
@@ -18,6 +18,11 @@ config SUSPEND_FREEZER
 
 	  Turning OFF this setting is NOT recommended! If in doubt, say Y.
 
+config HAS_EARLYSUSPEND
+        bool "Suport early suspend"
+        depends on SUSPEND
+        default n
+
 config CPU_FREQ_LIMIT_USERSPACE
 	bool "User space cpufreq limit interface"
 	depends on CPU_FREQ_LIMIT
diff --git a/kernel/power/Makefile b/kernel/power/Makefile
index 74c713ba61b..4745739123c 100644
--- a/kernel/power/Makefile
+++ b/kernel/power/Makefile
@@ -16,3 +16,4 @@ obj-$(CONFIG_SUSPEND_TIME)	+= suspend_time.o
 obj-$(CONFIG_MAGIC_SYSRQ)	+= poweroff.o
 
 obj-$(CONFIG_SUSPEND)	+= wakeup_reason.o
+obj-$(CONFIG_HAS_EARLYSUSPEND)	+= earlysuspend.o
diff --git a/kernel/power/earlysuspend.c b/kernel/power/earlysuspend.c
index b15f02eba45..6188366a8b7 100644
--- a/kernel/power/earlysuspend.c
+++ b/kernel/power/earlysuspend.c
@@ -23,6 +23,10 @@
 
 #include "power.h"
 
+struct wake_lock main_wake_lock;
+struct workqueue_struct *suspend_work_queue;
+suspend_state_t requested_suspend_state = PM_SUSPEND_MEM;
+
 enum {
 	DEBUG_USER_STATE = 1U << 0,
 	DEBUG_SUSPEND = 1U << 2,
-- 
2.15.1

