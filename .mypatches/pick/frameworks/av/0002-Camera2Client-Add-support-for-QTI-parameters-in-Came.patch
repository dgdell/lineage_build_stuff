From f71350e931e3f268bb02a6d6bb0de3f7a98cbf38 Mon Sep 17 00:00:00 2001
From: Sampath Vangaveti <sampathv@codeaurora.org>
Date: Wed, 25 Jan 2017 17:45:26 +0530
Subject: [PATCH 02/10] Camera2Client: Add support for QTI parameters in
 Camera2Client

Add support for QTI specific parameters in Camera2Client to
support Camera API1 QTI specific features with HAL3.

Change-Id: I9c88d7c15c91f829f1d1305c545e80d915e38fe5
---
 services/camera/libcameraservice/Android.mk   |   36 +-
 .../libcameraservice/api1/Camera2Client.cpp   |   12 +
 .../libcameraservice/api1/Camera2Client.h     |    5 +
 .../api1/qticlient2/CallbackProcessor.cpp     |  550 +++
 .../api1/qticlient2/CallbackProcessor.h       |   98 +
 .../api1/qticlient2/Camera2Heap.h             |   55 +
 .../api1/qticlient2/CaptureSequencer.cpp      |  742 ++++
 .../api1/qticlient2/CaptureSequencer.h        |  181 +
 .../api1/qticlient2/FrameProcessor.cpp        |  400 +++
 .../api1/qticlient2/FrameProcessor.h          |  118 +
 .../api1/qticlient2/JpegCompressor.cpp        |  221 ++
 .../api1/qticlient2/JpegCompressor.h          |  106 +
 .../api1/qticlient2/JpegProcessor.cpp         |  440 +++
 .../api1/qticlient2/JpegProcessor.h           |   93 +
 .../api1/qticlient2/Parameters.cpp            | 3177 +++++++++++++++++
 .../api1/qticlient2/Parameters.h              |  480 +++
 .../api1/qticlient2/QTIParameters.cpp         |  746 ++++
 .../api1/qticlient2/QTIParameters.h           |  120 +
 .../api1/qticlient2/StreamingProcessor.cpp    |  623 ++++
 .../api1/qticlient2/StreamingProcessor.h      |  116 +
 .../api1/qticlient2/ZslProcessor.cpp          |  905 +++++
 .../api1/qticlient2/ZslProcessor.h            |  170 +
 22 files changed, 9386 insertions(+), 8 deletions(-)
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/Camera2Heap.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/FrameProcessor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/FrameProcessor.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/JpegCompressor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/JpegCompressor.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/JpegProcessor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/JpegProcessor.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/Parameters.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/Parameters.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/QTIParameters.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/QTIParameters.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.h
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/ZslProcessor.cpp
 create mode 100644 services/camera/libcameraservice/api1/qticlient2/ZslProcessor.h

diff --git a/services/camera/libcameraservice/Android.mk b/services/camera/libcameraservice/Android.mk
index 762959430..d14992d9c 100644
--- a/services/camera/libcameraservice/Android.mk
+++ b/services/camera/libcameraservice/Android.mk
@@ -31,14 +31,6 @@ LOCAL_SRC_FILES :=  \
     common/FrameProcessorBase.cpp \
     api1/CameraClient.cpp \
     api1/Camera2Client.cpp \
-    api1/client2/Parameters.cpp \
-    api1/client2/FrameProcessor.cpp \
-    api1/client2/StreamingProcessor.cpp \
-    api1/client2/JpegProcessor.cpp \
-    api1/client2/CallbackProcessor.cpp \
-    api1/client2/JpegCompressor.cpp \
-    api1/client2/CaptureSequencer.cpp \
-    api1/client2/ZslProcessor.cpp \
     api2/CameraDeviceClient.cpp \
     device1/CameraHardwareInterface.cpp \
     device3/Camera3Device.cpp \
@@ -57,6 +49,30 @@ LOCAL_SRC_FILES :=  \
     utils/TagMonitor.cpp \
     utils/LatencyHistogram.cpp
 
+#use QTI Camera2Client layer, if TARGET_USES_QTI_CAMERA2CLIENT is enabled.
+ifeq ($(TARGET_USES_QTI_CAMERA2CLIENT),true)
+LOCAL_SRC_FILES +=  \
+    api1/qticlient2/Parameters.cpp \
+    api1/qticlient2/QTIParameters.cpp \
+    api1/qticlient2/FrameProcessor.cpp \
+    api1/qticlient2/StreamingProcessor.cpp \
+    api1/qticlient2/JpegProcessor.cpp \
+    api1/qticlient2/CallbackProcessor.cpp \
+    api1/qticlient2/JpegCompressor.cpp \
+    api1/qticlient2/CaptureSequencer.cpp \
+    api1/qticlient2/ZslProcessor.cpp
+else
+LOCAL_SRC_FILES +=  \
+    api1/client2/Parameters.cpp \
+    api1/client2/FrameProcessor.cpp \
+    api1/client2/StreamingProcessor.cpp \
+    api1/client2/JpegProcessor.cpp \
+    api1/client2/CallbackProcessor.cpp \
+    api1/client2/JpegCompressor.cpp \
+    api1/client2/CaptureSequencer.cpp \
+    api1/client2/ZslProcessor.cpp
+endif
+
 LOCAL_SHARED_LIBRARIES:= \
     libui \
     liblog \
@@ -104,6 +120,10 @@ ifeq ($(TARGET_HAS_LEGACY_CAMERA_HAL1),true)
     LOCAL_CFLAGS += -DNO_CAMERA_SERVER
 endif
 
+ifeq ($(TARGET_USES_QTI_CAMERA2CLIENT),true)
+LOCAL_CFLAGS += -DUSE_QTI_CAMERA2CLIENT
+endif
+
 LOCAL_MODULE:= libcameraservice
 
 include $(BUILD_SHARED_LIBRARY)
diff --git a/services/camera/libcameraservice/api1/Camera2Client.cpp b/services/camera/libcameraservice/api1/Camera2Client.cpp
index 585d2ebd5..743aa905c 100644
--- a/services/camera/libcameraservice/api1/Camera2Client.cpp
+++ b/services/camera/libcameraservice/api1/Camera2Client.cpp
@@ -28,11 +28,19 @@
 
 #include "api1/Camera2Client.h"
 
+#ifndef USE_QTI_CAMERA2CLIENT
 #include "api1/client2/StreamingProcessor.h"
 #include "api1/client2/JpegProcessor.h"
 #include "api1/client2/CaptureSequencer.h"
 #include "api1/client2/CallbackProcessor.h"
 #include "api1/client2/ZslProcessor.h"
+#else
+#include "api1/qticlient2/StreamingProcessor.h"
+#include "api1/qticlient2/JpegProcessor.h"
+#include "api1/qticlient2/CaptureSequencer.h"
+#include "api1/qticlient2/CallbackProcessor.h"
+#include "api1/qticlient2/ZslProcessor.h"
+#endif
 
 #define ALOG1(...) ALOGD_IF(gLogLevel >= 1, __VA_ARGS__);
 #define ALOG2(...) ALOGD_IF(gLogLevel >= 2, __VA_ARGS__);
@@ -101,7 +109,11 @@ status_t Camera2Client::initializeImpl(TProviderPtr providerPtr)
     {
         SharedParameters::Lock l(mParameters);
 
+#ifndef USE_QTI_CAMERA2CLIENT
         res = l.mParameters.initialize(&(mDevice->info()), mDeviceVersion);
+#else
+        res = l.mParameters.initialize(&(mDevice->info()), mDeviceVersion, providerPtr, mDevice);
+#endif
         if (res != OK) {
             ALOGE("%s: Camera %d: unable to build defaults: %s (%d)",
                     __FUNCTION__, mCameraId, strerror(-res), res);
diff --git a/services/camera/libcameraservice/api1/Camera2Client.h b/services/camera/libcameraservice/api1/Camera2Client.h
index 5af74eb15..905430e86 100644
--- a/services/camera/libcameraservice/api1/Camera2Client.h
+++ b/services/camera/libcameraservice/api1/Camera2Client.h
@@ -20,8 +20,13 @@
 #include "CameraService.h"
 #include "common/CameraDeviceBase.h"
 #include "common/Camera2ClientBase.h"
+#ifndef USE_QTI_CAMERA2CLIENT
 #include "api1/client2/Parameters.h"
 #include "api1/client2/FrameProcessor.h"
+#else
+#include "api1/qticlient2/Parameters.h"
+#include "api1/qticlient2/FrameProcessor.h"
+#endif
 //#include "api1/client2/StreamingProcessor.h"
 //#include "api1/client2/JpegProcessor.h"
 //#include "api1/client2/ZslProcessor.h"
diff --git a/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.cpp b/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.cpp
new file mode 100644
index 000000000..834abef92
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.cpp
@@ -0,0 +1,550 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-CallbackProcessor"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <gui/Surface.h>
+
+#include "common/CameraDeviceBase.h"
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/CallbackProcessor.h"
+
+#define ALIGN(x, mask) ( ((x) + (mask) - 1) & ~((mask) - 1) )
+
+namespace android {
+namespace camera2 {
+
+CallbackProcessor::CallbackProcessor(sp<Camera2Client> client):
+        Thread(false),
+        mClient(client),
+        mDevice(client->getCameraDevice()),
+        mId(client->getCameraId()),
+        mCallbackAvailable(false),
+        mCallbackToApp(false),
+        mCallbackStreamId(NO_STREAM) {
+}
+
+CallbackProcessor::~CallbackProcessor() {
+    ALOGV("%s: Exit", __FUNCTION__);
+    deleteStream();
+}
+
+void CallbackProcessor::onFrameAvailable(const BufferItem& /*item*/) {
+    Mutex::Autolock l(mInputMutex);
+    if (!mCallbackAvailable) {
+        mCallbackAvailable = true;
+        mCallbackAvailableSignal.signal();
+    }
+}
+
+status_t CallbackProcessor::setCallbackWindow(
+        const sp<Surface>& callbackWindow) {
+    ATRACE_CALL();
+    status_t res;
+
+    Mutex::Autolock l(mInputMutex);
+
+    sp<Camera2Client> client = mClient.promote();
+    if (client == 0) return OK;
+    sp<CameraDeviceBase> device = client->getCameraDevice();
+
+    // If the window is changing, clear out stream if it already exists
+    if (mCallbackWindow != callbackWindow && mCallbackStreamId != NO_STREAM) {
+        res = device->deleteStream(mCallbackStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to delete old stream "
+                    "for callbacks: %s (%d)", __FUNCTION__,
+                    client->getCameraId(), strerror(-res), res);
+            return res;
+        }
+        mCallbackStreamId = NO_STREAM;
+        mCallbackConsumer.clear();
+    }
+    mCallbackWindow = callbackWindow;
+    mCallbackToApp = (mCallbackWindow != NULL);
+
+    return OK;
+}
+
+status_t CallbackProcessor::updateStream(const Parameters &params) {
+    ATRACE_CALL();
+    status_t res;
+
+    Mutex::Autolock l(mInputMutex);
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    // If possible, use the flexible YUV format
+    int32_t callbackFormat = params.previewFormat;
+    if (mCallbackToApp) {
+        // TODO: etalvala: This should use the flexible YUV format as well, but
+        // need to reconcile HAL2/HAL3 requirements.
+        callbackFormat = HAL_PIXEL_FORMAT_YV12;
+    } else if(params.fastInfo.useFlexibleYuv &&
+            (params.previewFormat == HAL_PIXEL_FORMAT_YCrCb_420_SP ||
+             params.previewFormat == HAL_PIXEL_FORMAT_YV12) ) {
+        callbackFormat = HAL_PIXEL_FORMAT_YCbCr_420_888;
+    }
+
+    if (!mCallbackToApp && mCallbackConsumer == 0) {
+        // Create CPU buffer queue endpoint, since app hasn't given us one
+        // Make it async to avoid disconnect deadlocks
+        sp<IGraphicBufferProducer> producer;
+        sp<IGraphicBufferConsumer> consumer;
+        BufferQueue::createBufferQueue(&producer, &consumer);
+        mCallbackConsumer = new CpuConsumer(consumer, kCallbackHeapCount);
+        mCallbackConsumer->setFrameAvailableListener(this);
+        mCallbackConsumer->setName(String8("Camera2-CallbackConsumer"));
+        mCallbackWindow = new Surface(producer);
+    }
+
+    if (mCallbackStreamId != NO_STREAM) {
+        // Check if stream parameters have to change
+        uint32_t currentWidth, currentHeight, currentFormat;
+        res = device->getStreamInfo(mCallbackStreamId,
+                &currentWidth, &currentHeight, &currentFormat, 0);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Error querying callback output stream info: "
+                    "%s (%d)", __FUNCTION__, mId,
+                    strerror(-res), res);
+            return res;
+        }
+        if (currentWidth != (uint32_t)params.previewWidth ||
+                currentHeight != (uint32_t)params.previewHeight ||
+                currentFormat != (uint32_t)callbackFormat) {
+            // Since size should only change while preview is not running,
+            // assuming that all existing use of old callback stream is
+            // completed.
+            ALOGV("%s: Camera %d: Deleting stream %d since the buffer "
+                    "parameters changed", __FUNCTION__, mId, mCallbackStreamId);
+            res = device->deleteStream(mCallbackStreamId);
+            if (res != OK) {
+                ALOGE("%s: Camera %d: Unable to delete old output stream "
+                        "for callbacks: %s (%d)", __FUNCTION__,
+                        mId, strerror(-res), res);
+                return res;
+            }
+            mCallbackStreamId = NO_STREAM;
+        }
+    }
+
+    if (mCallbackStreamId == NO_STREAM) {
+        ALOGV("Creating callback stream: %d x %d, format 0x%x, API format 0x%x",
+                params.previewWidth, params.previewHeight,
+                callbackFormat, params.previewFormat);
+        res = device->createStream(mCallbackWindow,
+                params.previewWidth, params.previewHeight, callbackFormat,
+                HAL_DATASPACE_V0_JFIF, CAMERA3_STREAM_ROTATION_0, &mCallbackStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't create output stream for callbacks: "
+                    "%s (%d)", __FUNCTION__, mId,
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    return OK;
+}
+
+status_t CallbackProcessor::deleteStream() {
+    ATRACE_CALL();
+    sp<CameraDeviceBase> device;
+    status_t res;
+    {
+        Mutex::Autolock l(mInputMutex);
+
+        if (mCallbackStreamId == NO_STREAM) {
+            return OK;
+        }
+        device = mDevice.promote();
+        if (device == 0) {
+            ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+    }
+    res = device->waitUntilDrained();
+    if (res != OK) {
+        ALOGE("%s: Error waiting for HAL to drain: %s (%d)",
+                __FUNCTION__, strerror(-res), res);
+        return res;
+    }
+
+    res = device->deleteStream(mCallbackStreamId);
+    if (res != OK) {
+        ALOGE("%s: Unable to delete callback stream: %s (%d)",
+                __FUNCTION__, strerror(-res), res);
+        return res;
+    }
+
+    {
+        Mutex::Autolock l(mInputMutex);
+
+        mCallbackHeap.clear();
+        mCallbackWindow.clear();
+        mCallbackConsumer.clear();
+
+        mCallbackStreamId = NO_STREAM;
+    }
+    return OK;
+}
+
+int CallbackProcessor::getStreamId() const {
+    Mutex::Autolock l(mInputMutex);
+    return mCallbackStreamId;
+}
+
+void CallbackProcessor::dump(int /*fd*/, const Vector<String16>& /*args*/) const {
+}
+
+bool CallbackProcessor::threadLoop() {
+    status_t res;
+
+    {
+        Mutex::Autolock l(mInputMutex);
+        while (!mCallbackAvailable) {
+            res = mCallbackAvailableSignal.waitRelative(mInputMutex,
+                    kWaitDuration);
+            if (res == TIMED_OUT) return true;
+        }
+        mCallbackAvailable = false;
+    }
+
+    do {
+        sp<Camera2Client> client = mClient.promote();
+        if (client == 0) {
+            res = discardNewCallback();
+        } else {
+            res = processNewCallback(client);
+        }
+    } while (res == OK);
+
+    return true;
+}
+
+status_t CallbackProcessor::discardNewCallback() {
+    ATRACE_CALL();
+    status_t res;
+    CpuConsumer::LockedBuffer imgBuffer;
+    res = mCallbackConsumer->lockNextBuffer(&imgBuffer);
+    if (res != OK) {
+        if (res != BAD_VALUE) {
+            ALOGE("%s: Camera %d: Error receiving next callback buffer: "
+                    "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+        }
+        return res;
+    }
+    mCallbackConsumer->unlockBuffer(imgBuffer);
+    return OK;
+}
+
+status_t CallbackProcessor::processNewCallback(sp<Camera2Client> &client) {
+    ATRACE_CALL();
+    status_t res;
+
+    sp<Camera2Heap> callbackHeap;
+    bool useFlexibleYuv = false;
+    int32_t previewFormat = 0;
+    size_t heapIdx;
+
+    {
+        /* acquire SharedParameters before mMutex so we don't dead lock
+            with Camera2Client code calling into StreamingProcessor */
+        SharedParameters::Lock l(client->getParameters());
+        Mutex::Autolock m(mInputMutex);
+        CpuConsumer::LockedBuffer imgBuffer;
+        if (mCallbackStreamId == NO_STREAM) {
+            ALOGV("%s: Camera %d:No stream is available"
+                    , __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        ALOGV("%s: Getting buffer", __FUNCTION__);
+        res = mCallbackConsumer->lockNextBuffer(&imgBuffer);
+        if (res != OK) {
+            if (res != BAD_VALUE) {
+                ALOGE("%s: Camera %d: Error receiving next callback buffer: "
+                        "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+            }
+            return res;
+        }
+        ALOGV("%s: Camera %d: Preview callback available", __FUNCTION__,
+                mId);
+
+        if ( l.mParameters.state != Parameters::PREVIEW
+                && l.mParameters.state != Parameters::RECORD
+                && l.mParameters.state != Parameters::VIDEO_SNAPSHOT) {
+            ALOGV("%s: Camera %d: No longer streaming",
+                    __FUNCTION__, mId);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+
+        if (! (l.mParameters.previewCallbackFlags &
+                CAMERA_FRAME_CALLBACK_FLAG_ENABLE_MASK) ) {
+            ALOGV("%s: No longer enabled, dropping", __FUNCTION__);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+        if ((l.mParameters.previewCallbackFlags &
+                        CAMERA_FRAME_CALLBACK_FLAG_ONE_SHOT_MASK) &&
+                !l.mParameters.previewCallbackOneShot) {
+            ALOGV("%s: One shot mode, already sent, dropping", __FUNCTION__);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+
+        if (imgBuffer.width != static_cast<uint32_t>(l.mParameters.previewWidth) ||
+                imgBuffer.height != static_cast<uint32_t>(l.mParameters.previewHeight)) {
+            ALOGW("%s: The preview size has changed to %d x %d from %d x %d, this buffer is"
+                    " no longer valid, dropping",__FUNCTION__,
+                    l.mParameters.previewWidth, l.mParameters.previewHeight,
+                    imgBuffer.width, imgBuffer.height);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+
+        previewFormat = l.mParameters.previewFormat;
+        useFlexibleYuv = l.mParameters.fastInfo.useFlexibleYuv &&
+                (previewFormat == HAL_PIXEL_FORMAT_YCrCb_420_SP ||
+                 previewFormat == HAL_PIXEL_FORMAT_YV12);
+
+        int32_t expectedFormat = useFlexibleYuv ?
+                HAL_PIXEL_FORMAT_YCbCr_420_888 : previewFormat;
+
+        if (imgBuffer.format != expectedFormat) {
+            ALOGE("%s: Camera %d: Unexpected format for callback: "
+                    "0x%x, expected 0x%x", __FUNCTION__, mId,
+                    imgBuffer.format, expectedFormat);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return INVALID_OPERATION;
+        }
+
+        // In one-shot mode, stop sending callbacks after the first one
+        if (l.mParameters.previewCallbackFlags &
+                CAMERA_FRAME_CALLBACK_FLAG_ONE_SHOT_MASK) {
+            ALOGV("%s: clearing oneshot", __FUNCTION__);
+            l.mParameters.previewCallbackOneShot = false;
+        }
+
+        uint32_t destYStride = 0;
+        uint32_t destCStride = 0;
+        if (useFlexibleYuv) {
+            if (previewFormat == HAL_PIXEL_FORMAT_YV12) {
+                // Strides must align to 16 for YV12
+                destYStride = ALIGN(imgBuffer.width, 16);
+                destCStride = ALIGN(destYStride / 2, 16);
+            } else {
+                // No padding for NV21
+                ALOG_ASSERT(previewFormat == HAL_PIXEL_FORMAT_YCrCb_420_SP,
+                        "Unexpected preview format 0x%x", previewFormat);
+                destYStride = imgBuffer.width;
+                destCStride = destYStride / 2;
+            }
+        } else {
+            destYStride = imgBuffer.stride;
+            // don't care about cStride
+        }
+
+        size_t bufferSize = Camera2Client::calculateBufferSize(
+                imgBuffer.width, imgBuffer.height,
+                previewFormat, destYStride);
+        size_t currentBufferSize = (mCallbackHeap == 0) ?
+                0 : (mCallbackHeap->mHeap->getSize() / kCallbackHeapCount);
+        if (bufferSize != currentBufferSize) {
+            mCallbackHeap.clear();
+            mCallbackHeap = new Camera2Heap(bufferSize, kCallbackHeapCount,
+                    "Camera2Client::CallbackHeap");
+            if (mCallbackHeap->mHeap->getSize() == 0) {
+                ALOGE("%s: Camera %d: Unable to allocate memory for callbacks",
+                        __FUNCTION__, mId);
+                mCallbackConsumer->unlockBuffer(imgBuffer);
+                return INVALID_OPERATION;
+            }
+
+            mCallbackHeapHead = 0;
+            mCallbackHeapFree = kCallbackHeapCount;
+        }
+
+        if (mCallbackHeapFree == 0) {
+            ALOGE("%s: Camera %d: No free callback buffers, dropping frame",
+                    __FUNCTION__, mId);
+            mCallbackConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+
+        heapIdx = mCallbackHeapHead;
+
+        mCallbackHeapHead = (mCallbackHeapHead + 1) % kCallbackHeapCount;
+        mCallbackHeapFree--;
+
+        // TODO: Get rid of this copy by passing the gralloc queue all the way
+        // to app
+
+        ssize_t offset;
+        size_t size;
+        sp<IMemoryHeap> heap =
+                mCallbackHeap->mBuffers[heapIdx]->getMemory(&offset,
+                        &size);
+        uint8_t *data = (uint8_t*)heap->getBase() + offset;
+
+        if (!useFlexibleYuv) {
+            // Can just memcpy when HAL format matches API format
+            memcpy(data, imgBuffer.data, bufferSize);
+        } else {
+            res = convertFromFlexibleYuv(previewFormat, data, imgBuffer,
+                    destYStride, destCStride);
+            if (res != OK) {
+                ALOGE("%s: Camera %d: Can't convert between 0x%x and 0x%x formats!",
+                        __FUNCTION__, mId, imgBuffer.format, previewFormat);
+                mCallbackConsumer->unlockBuffer(imgBuffer);
+                return BAD_VALUE;
+            }
+        }
+
+        ALOGV("%s: Freeing buffer", __FUNCTION__);
+        mCallbackConsumer->unlockBuffer(imgBuffer);
+
+        // mCallbackHeap may get freed up once input mutex is released
+        callbackHeap = mCallbackHeap;
+    }
+
+    // Call outside parameter lock to allow re-entrancy from notification
+    {
+        Camera2Client::SharedCameraCallbacks::Lock
+            l(client->mSharedCameraCallbacks);
+        if (l.mRemoteCallback != 0) {
+            ALOGV("%s: Camera %d: Invoking client data callback",
+                    __FUNCTION__, mId);
+            l.mRemoteCallback->dataCallback(CAMERA_MSG_PREVIEW_FRAME,
+                    callbackHeap->mBuffers[heapIdx], NULL);
+        }
+    }
+
+    // Only increment free if we're still using the same heap
+    mCallbackHeapFree++;
+
+    ALOGV("%s: exit", __FUNCTION__);
+
+    return OK;
+}
+
+status_t CallbackProcessor::convertFromFlexibleYuv(int32_t previewFormat,
+        uint8_t *dst,
+        const CpuConsumer::LockedBuffer &src,
+        uint32_t dstYStride,
+        uint32_t dstCStride) const {
+
+    if (previewFormat != HAL_PIXEL_FORMAT_YCrCb_420_SP &&
+            previewFormat != HAL_PIXEL_FORMAT_YV12) {
+        ALOGE("%s: Camera %d: Unexpected preview format when using "
+                "flexible YUV: 0x%x", __FUNCTION__, mId, previewFormat);
+        return INVALID_OPERATION;
+    }
+
+    // Copy Y plane, adjusting for stride
+    const uint8_t *ySrc = src.data;
+    uint8_t *yDst = dst;
+    for (size_t row = 0; row < src.height; row++) {
+        memcpy(yDst, ySrc, src.width);
+        ySrc += src.stride;
+        yDst += dstYStride;
+    }
+
+    // Copy/swizzle chroma planes, 4:2:0 subsampling
+    const uint8_t *cbSrc = src.dataCb;
+    const uint8_t *crSrc = src.dataCr;
+    size_t chromaHeight = src.height / 2;
+    size_t chromaWidth = src.width / 2;
+    ssize_t chromaGap = src.chromaStride -
+            (chromaWidth * src.chromaStep);
+    size_t dstChromaGap = dstCStride - chromaWidth;
+
+    if (previewFormat == HAL_PIXEL_FORMAT_YCrCb_420_SP) {
+        // Flexible YUV chroma to NV21 chroma
+        uint8_t *crcbDst = yDst;
+        // Check for shortcuts
+        if (cbSrc == crSrc + 1 && src.chromaStep == 2) {
+            ALOGV("%s: Fast NV21->NV21", __FUNCTION__);
+            // Source has semiplanar CrCb chroma layout, can copy by rows
+            for (size_t row = 0; row < chromaHeight; row++) {
+                memcpy(crcbDst, crSrc, src.width);
+                crcbDst += src.width;
+                crSrc += src.chromaStride;
+            }
+        } else {
+            ALOGV("%s: Generic->NV21", __FUNCTION__);
+            // Generic copy, always works but not very efficient
+            for (size_t row = 0; row < chromaHeight; row++) {
+                for (size_t col = 0; col < chromaWidth; col++) {
+                    *(crcbDst++) = *crSrc;
+                    *(crcbDst++) = *cbSrc;
+                    crSrc += src.chromaStep;
+                    cbSrc += src.chromaStep;
+                }
+                crSrc += chromaGap;
+                cbSrc += chromaGap;
+            }
+        }
+    } else {
+        // flexible YUV chroma to YV12 chroma
+        ALOG_ASSERT(previewFormat == HAL_PIXEL_FORMAT_YV12,
+                "Unexpected preview format 0x%x", previewFormat);
+        uint8_t *crDst = yDst;
+        uint8_t *cbDst = yDst + chromaHeight * dstCStride;
+        if (src.chromaStep == 1) {
+            ALOGV("%s: Fast YV12->YV12", __FUNCTION__);
+            // Source has planar chroma layout, can copy by row
+            for (size_t row = 0; row < chromaHeight; row++) {
+                memcpy(crDst, crSrc, chromaWidth);
+                crDst += dstCStride;
+                crSrc += src.chromaStride;
+            }
+            for (size_t row = 0; row < chromaHeight; row++) {
+                memcpy(cbDst, cbSrc, chromaWidth);
+                cbDst += dstCStride;
+                cbSrc += src.chromaStride;
+            }
+        } else {
+            ALOGV("%s: Generic->YV12", __FUNCTION__);
+            // Generic copy, always works but not very efficient
+            for (size_t row = 0; row < chromaHeight; row++) {
+                for (size_t col = 0; col < chromaWidth; col++) {
+                    *(crDst++) = *crSrc;
+                    *(cbDst++) = *cbSrc;
+                    crSrc += src.chromaStep;
+                    cbSrc += src.chromaStep;
+                }
+                crSrc += chromaGap;
+                cbSrc += chromaGap;
+                crDst += dstChromaGap;
+                cbDst += dstChromaGap;
+            }
+        }
+    }
+
+    return OK;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.h b/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.h
new file mode 100644
index 000000000..5e9eb35bf
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/CallbackProcessor.h
@@ -0,0 +1,98 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_CALLBACKPROCESSOR_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_CALLBACKPROCESSOR_H
+
+#include <utils/Thread.h>
+#include <utils/String16.h>
+#include <utils/Vector.h>
+#include <utils/Mutex.h>
+#include <utils/Condition.h>
+#include <gui/CpuConsumer.h>
+
+#include "api1/qticlient2/Camera2Heap.h"
+
+namespace android {
+
+class Camera2Client;
+class CameraDeviceBase;
+
+namespace camera2 {
+
+struct Parameters;
+
+/***
+ * Still image capture output image processing
+ */
+class CallbackProcessor:
+            public Thread, public CpuConsumer::FrameAvailableListener {
+  public:
+    explicit CallbackProcessor(sp<Camera2Client> client);
+    ~CallbackProcessor();
+
+    void onFrameAvailable(const BufferItem& item);
+
+    // Set to NULL to disable the direct-to-app callback window
+    status_t setCallbackWindow(const sp<Surface>& callbackWindow);
+    status_t updateStream(const Parameters &params);
+    status_t deleteStream();
+    int getStreamId() const;
+
+    void dump(int fd, const Vector<String16>& args) const;
+  private:
+    static const nsecs_t kWaitDuration = 10000000; // 10 ms
+    wp<Camera2Client> mClient;
+    wp<CameraDeviceBase> mDevice;
+    int mId;
+
+    mutable Mutex mInputMutex;
+    bool mCallbackAvailable;
+    Condition mCallbackAvailableSignal;
+
+    enum {
+        NO_STREAM = -1
+    };
+
+    // True if mCallbackWindow is a remote consumer, false if just the local
+    // mCallbackConsumer
+    bool mCallbackToApp;
+    int mCallbackStreamId;
+    static const size_t kCallbackHeapCount = 6;
+    sp<CpuConsumer>    mCallbackConsumer;
+    sp<Surface>        mCallbackWindow;
+    sp<Camera2Heap>    mCallbackHeap;
+    size_t mCallbackHeapHead, mCallbackHeapFree;
+
+    virtual bool threadLoop();
+
+    status_t processNewCallback(sp<Camera2Client> &client);
+    // Used when shutting down
+    status_t discardNewCallback();
+
+    // Convert from flexible YUV to NV21 or YV12
+    status_t convertFromFlexibleYuv(int32_t previewFormat,
+            uint8_t *dst,
+            const CpuConsumer::LockedBuffer &src,
+            uint32_t dstYStride,
+            uint32_t dstCStride) const;
+};
+
+
+}; //namespace camera2
+}; //namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/Camera2Heap.h b/services/camera/libcameraservice/api1/qticlient2/Camera2Heap.h
new file mode 100644
index 000000000..fe3afcbf2
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/Camera2Heap.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROiD_SERVERS_CAMERA_CAMERA2HEAP_H
+#define ANDROiD_SERVERS_CAMERA_CAMERA2HEAP_H
+
+#include <binder/MemoryBase.h>
+#include <binder/MemoryHeapBase.h>
+
+namespace android {
+namespace camera2 {
+
+// Utility class for managing a set of IMemory blocks
+class Camera2Heap : public RefBase {
+  public:
+    explicit Camera2Heap(size_t buf_size, uint_t num_buffers = 1,
+            const char *name = NULL) :
+            mBufSize(buf_size),
+            mNumBufs(num_buffers) {
+        mHeap = new MemoryHeapBase(buf_size * num_buffers, 0, name);
+        mBuffers = new sp<MemoryBase>[mNumBufs];
+        for (uint_t i = 0; i < mNumBufs; i++)
+            mBuffers[i] = new MemoryBase(mHeap,
+                    i * mBufSize,
+                    mBufSize);
+    }
+
+    virtual ~Camera2Heap()
+    {
+        delete [] mBuffers;
+    }
+
+    size_t mBufSize;
+    uint_t mNumBufs;
+    sp<MemoryHeapBase> mHeap;
+    sp<MemoryBase> *mBuffers;
+};
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.cpp b/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.cpp
new file mode 100644
index 000000000..68988928e
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.cpp
@@ -0,0 +1,742 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-CaptureSequencer"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+
+#include <inttypes.h>
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <utils/Vector.h>
+
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/CaptureSequencer.h"
+#include "api1/qticlient2/Parameters.h"
+#include "api1/qticlient2/ZslProcessor.h"
+
+namespace android {
+namespace camera2 {
+
+/** Public members */
+
+CaptureSequencer::CaptureSequencer(wp<Camera2Client> client):
+        Thread(false),
+        mStartCapture(false),
+        mBusy(false),
+        mNewAEState(false),
+        mNewFrameReceived(false),
+        mNewCaptureReceived(false),
+        mNewCaptureErrorCnt(0),
+        mShutterNotified(false),
+        mHalNotifiedShutter(false),
+        mShutterCaptureId(-1),
+        mClient(client),
+        mCaptureState(IDLE),
+        mStateTransitionCount(0),
+        mTriggerId(0),
+        mTimeoutCount(0),
+        mCaptureId(Camera2Client::kCaptureRequestIdStart),
+        mMsgType(0) {
+    ALOGV("%s", __FUNCTION__);
+}
+
+CaptureSequencer::~CaptureSequencer() {
+    ALOGV("%s: Exit", __FUNCTION__);
+}
+
+void CaptureSequencer::setZslProcessor(const wp<ZslProcessor>& processor) {
+    Mutex::Autolock l(mInputMutex);
+    mZslProcessor = processor;
+}
+
+status_t CaptureSequencer::startCapture(int msgType) {
+    ALOGV("%s", __FUNCTION__);
+    ATRACE_CALL();
+    Mutex::Autolock l(mInputMutex);
+    if (mBusy) {
+        ALOGE("%s: Already busy capturing!", __FUNCTION__);
+        return INVALID_OPERATION;
+    }
+    if (!mStartCapture) {
+        mMsgType = msgType;
+        mStartCapture = true;
+        mStartCaptureSignal.signal();
+    }
+    return OK;
+}
+
+status_t CaptureSequencer::waitUntilIdle(nsecs_t timeout) {
+    ATRACE_CALL();
+    ALOGV("%s: Waiting for idle", __FUNCTION__);
+    Mutex::Autolock l(mStateMutex);
+    status_t res = -1;
+    while (mCaptureState != IDLE) {
+        nsecs_t startTime = systemTime();
+
+        res = mStateChanged.waitRelative(mStateMutex, timeout);
+        if (res != OK) return res;
+
+        timeout -= (systemTime() - startTime);
+    }
+    ALOGV("%s: Now idle", __FUNCTION__);
+    return OK;
+}
+
+void CaptureSequencer::notifyAutoExposure(uint8_t newState, int triggerId) {
+    ATRACE_CALL();
+    Mutex::Autolock l(mInputMutex);
+    mAEState = newState;
+    mAETriggerId = triggerId;
+    if (!mNewAEState) {
+        mNewAEState = true;
+        mNewNotifySignal.signal();
+    }
+}
+
+void CaptureSequencer::notifyShutter(const CaptureResultExtras& resultExtras,
+                                     nsecs_t timestamp) {
+    ATRACE_CALL();
+    (void) timestamp;
+    Mutex::Autolock l(mInputMutex);
+    if (!mHalNotifiedShutter && resultExtras.requestId == mShutterCaptureId) {
+        mHalNotifiedShutter = true;
+        mShutterNotifySignal.signal();
+    }
+}
+
+void CaptureSequencer::onResultAvailable(const CaptureResult &result) {
+    ATRACE_CALL();
+    ALOGV("%s: New result available.", __FUNCTION__);
+    Mutex::Autolock l(mInputMutex);
+    mNewFrameId = result.mResultExtras.requestId;
+    mNewFrame = result.mMetadata;
+    if (!mNewFrameReceived) {
+        mNewFrameReceived = true;
+        mNewFrameSignal.signal();
+    }
+}
+
+void CaptureSequencer::onCaptureAvailable(nsecs_t timestamp,
+        const sp<MemoryBase>& captureBuffer, bool captureError) {
+    ATRACE_CALL();
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock l(mInputMutex);
+    mCaptureTimestamp = timestamp;
+    mCaptureBuffer = captureBuffer;
+    if (!mNewCaptureReceived) {
+        mNewCaptureReceived = true;
+        if (captureError) {
+            mNewCaptureErrorCnt++;
+        } else {
+            mNewCaptureErrorCnt = 0;
+        }
+        mNewCaptureSignal.signal();
+    }
+}
+
+
+void CaptureSequencer::dump(int fd, const Vector<String16>& /*args*/) {
+    String8 result;
+    if (mCaptureRequest.entryCount() != 0) {
+        result = "    Capture request:\n";
+        write(fd, result.string(), result.size());
+        mCaptureRequest.dump(fd, 2, 6);
+    } else {
+        result = "    Capture request: undefined\n";
+        write(fd, result.string(), result.size());
+    }
+    result = String8::format("    Current capture state: %s\n",
+            kStateNames[mCaptureState]);
+    result.append("    Latest captured frame:\n");
+    write(fd, result.string(), result.size());
+    mNewFrame.dump(fd, 2, 6);
+}
+
+/** Private members */
+
+const char* CaptureSequencer::kStateNames[CaptureSequencer::NUM_CAPTURE_STATES+1] =
+{
+    "IDLE",
+    "START",
+    "ZSL_START",
+    "ZSL_WAITING",
+    "ZSL_REPROCESSING",
+    "STANDARD_START",
+    "STANDARD_PRECAPTURE_WAIT",
+    "STANDARD_CAPTURE",
+    "STANDARD_CAPTURE_WAIT",
+    "DONE",
+    "ERROR",
+    "UNKNOWN"
+};
+
+const CaptureSequencer::StateManager
+        CaptureSequencer::kStateManagers[CaptureSequencer::NUM_CAPTURE_STATES-1] = {
+    &CaptureSequencer::manageIdle,
+    &CaptureSequencer::manageStart,
+    &CaptureSequencer::manageZslStart,
+    &CaptureSequencer::manageZslWaiting,
+    &CaptureSequencer::manageZslReprocessing,
+    &CaptureSequencer::manageStandardStart,
+    &CaptureSequencer::manageStandardPrecaptureWait,
+    &CaptureSequencer::manageStandardCapture,
+    &CaptureSequencer::manageStandardCaptureWait,
+    &CaptureSequencer::manageDone,
+};
+
+bool CaptureSequencer::threadLoop() {
+
+    sp<Camera2Client> client = mClient.promote();
+    if (client == 0) return false;
+
+    CaptureState currentState;
+    {
+        Mutex::Autolock l(mStateMutex);
+        currentState = mCaptureState;
+    }
+
+    currentState = (this->*kStateManagers[currentState])(client);
+
+    Mutex::Autolock l(mStateMutex);
+    if (currentState != mCaptureState) {
+        if (mCaptureState != IDLE) {
+            ATRACE_ASYNC_END(kStateNames[mCaptureState], mStateTransitionCount);
+        }
+        mCaptureState = currentState;
+        mStateTransitionCount++;
+        if (mCaptureState != IDLE) {
+            ATRACE_ASYNC_BEGIN(kStateNames[mCaptureState], mStateTransitionCount);
+        }
+        ALOGV("Camera %d: New capture state %s",
+                client->getCameraId(), kStateNames[mCaptureState]);
+        mStateChanged.signal();
+    }
+
+    if (mCaptureState == ERROR) {
+        ALOGE("Camera %d: Stopping capture sequencer due to error",
+                client->getCameraId());
+        return false;
+    }
+
+    return true;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageIdle(
+        sp<Camera2Client> &/*client*/) {
+    status_t res;
+    Mutex::Autolock l(mInputMutex);
+    while (!mStartCapture) {
+        res = mStartCaptureSignal.waitRelative(mInputMutex,
+                kWaitDuration);
+        if (res == TIMED_OUT) break;
+    }
+    if (mStartCapture) {
+        mStartCapture = false;
+        mBusy = true;
+        return START;
+    }
+    return IDLE;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageDone(sp<Camera2Client> &client) {
+    status_t res = OK;
+    ATRACE_CALL();
+    mCaptureId++;
+    if (mCaptureId >= Camera2Client::kCaptureRequestIdEnd) {
+        mCaptureId = Camera2Client::kCaptureRequestIdStart;
+    }
+    {
+        Mutex::Autolock l(mInputMutex);
+        mBusy = false;
+    }
+
+    int takePictureCounter = 0;
+    {
+        SharedParameters::Lock l(client->getParameters());
+        switch (l.mParameters.state) {
+            case Parameters::DISCONNECTED:
+                ALOGW("%s: Camera %d: Discarding image data during shutdown ",
+                        __FUNCTION__, client->getCameraId());
+                res = INVALID_OPERATION;
+                break;
+            case Parameters::STILL_CAPTURE:
+                res = client->getCameraDevice()->waitUntilDrained();
+                if (res != OK) {
+                    ALOGE("%s: Camera %d: Can't idle after still capture: "
+                            "%s (%d)", __FUNCTION__, client->getCameraId(),
+                            strerror(-res), res);
+                }
+                l.mParameters.state = Parameters::STOPPED;
+                break;
+            case Parameters::VIDEO_SNAPSHOT:
+                l.mParameters.state = Parameters::RECORD;
+                break;
+            default:
+                ALOGE("%s: Camera %d: Still image produced unexpectedly "
+                        "in state %s!",
+                        __FUNCTION__, client->getCameraId(),
+                        Parameters::getStateName(l.mParameters.state));
+                res = INVALID_OPERATION;
+        }
+        takePictureCounter = l.mParameters.takePictureCounter;
+    }
+    sp<ZslProcessor> processor = mZslProcessor.promote();
+    if (processor != 0) {
+        ALOGV("%s: Memory optimization, clearing ZSL queue",
+              __FUNCTION__);
+        processor->clearZslQueue();
+    }
+
+    /**
+     * Fire the jpegCallback in Camera#takePicture(..., jpegCallback)
+     */
+    if (mCaptureBuffer != 0 && res == OK) {
+        ATRACE_ASYNC_END(Camera2Client::kTakepictureLabel, takePictureCounter);
+
+        Camera2Client::SharedCameraCallbacks::Lock
+            l(client->mSharedCameraCallbacks);
+        ALOGV("%s: Sending still image to client", __FUNCTION__);
+        if (l.mRemoteCallback != 0) {
+            l.mRemoteCallback->dataCallback(CAMERA_MSG_COMPRESSED_IMAGE,
+                    mCaptureBuffer, NULL);
+        } else {
+            ALOGV("%s: No client!", __FUNCTION__);
+        }
+    }
+    mCaptureBuffer.clear();
+
+    return IDLE;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageStart(
+        sp<Camera2Client> &client) {
+    ALOGV("%s", __FUNCTION__);
+    status_t res;
+    ATRACE_CALL();
+    SharedParameters::Lock l(client->getParameters());
+    CaptureState nextState = DONE;
+
+    res = updateCaptureRequest(l.mParameters, client);
+    if (res != OK ) {
+        ALOGE("%s: Camera %d: Can't update still image capture request: %s (%d)",
+                __FUNCTION__, client->getCameraId(), strerror(-res), res);
+        return DONE;
+    }
+
+    else if (l.mParameters.useZeroShutterLag() &&
+            l.mParameters.state == Parameters::STILL_CAPTURE &&
+            l.mParameters.flashMode != Parameters::FLASH_MODE_ON) {
+        nextState = ZSL_START;
+    } else {
+        nextState = STANDARD_START;
+    }
+    {
+        Mutex::Autolock l(mInputMutex);
+        mShutterCaptureId = mCaptureId;
+        mHalNotifiedShutter = false;
+    }
+    mShutterNotified = false;
+
+    return nextState;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageZslStart(
+        sp<Camera2Client> &client) {
+    ALOGV("%s", __FUNCTION__);
+    status_t res;
+    sp<ZslProcessor> processor = mZslProcessor.promote();
+    if (processor == 0) {
+        ALOGE("%s: No ZSL queue to use!", __FUNCTION__);
+        return DONE;
+    }
+
+    // We don't want to get partial results for ZSL capture.
+    client->registerFrameListener(mCaptureId, mCaptureId + 1,
+            this,
+            /*sendPartials*/false);
+
+    // TODO: Actually select the right thing here.
+    res = processor->pushToReprocess(mCaptureId);
+    if (res != OK) {
+        if (res == NOT_ENOUGH_DATA) {
+            ALOGV("%s: Camera %d: ZSL queue doesn't have good frame, "
+                    "falling back to normal capture", __FUNCTION__,
+                    client->getCameraId());
+        } else {
+            ALOGE("%s: Camera %d: Error in ZSL queue: %s (%d)",
+                    __FUNCTION__, client->getCameraId(), strerror(-res), res);
+        }
+        return STANDARD_START;
+    }
+
+    SharedParameters::Lock l(client->getParameters());
+    /* warning: this also locks a SharedCameraCallbacks */
+    shutterNotifyLocked(l.mParameters, client, mMsgType);
+    mShutterNotified = true;
+    mTimeoutCount = kMaxTimeoutsForCaptureEnd;
+    return STANDARD_CAPTURE_WAIT;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageZslWaiting(
+        sp<Camera2Client> &/*client*/) {
+    ALOGV("%s", __FUNCTION__);
+    return DONE;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageZslReprocessing(
+        sp<Camera2Client> &/*client*/) {
+    ALOGV("%s", __FUNCTION__);
+    return START;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageStandardStart(
+        sp<Camera2Client> &client) {
+    ATRACE_CALL();
+
+    bool isAeConverged = false;
+    // Get the onFrameAvailable callback when the requestID == mCaptureId
+    // We don't want to get partial results for normal capture, as we need
+    // Get ANDROID_SENSOR_TIMESTAMP from the capture result, but partial
+    // result doesn't have to have this metadata available.
+    // TODO: Update to use the HALv3 shutter notification for remove the
+    // need for this listener and make it faster. see bug 12530628.
+    client->registerFrameListener(mCaptureId, mCaptureId + 1,
+            this,
+            /*sendPartials*/false);
+
+    {
+        Mutex::Autolock l(mInputMutex);
+        isAeConverged = (mAEState == ANDROID_CONTROL_AE_STATE_CONVERGED);
+    }
+
+    {
+        SharedParameters::Lock l(client->getParameters());
+        // Skip AE precapture when it is already converged and not in force flash mode.
+        if (l.mParameters.flashMode != Parameters::FLASH_MODE_ON && isAeConverged) {
+            return STANDARD_CAPTURE;
+        }
+
+        mTriggerId = l.mParameters.precaptureTriggerCounter++;
+    }
+    client->getCameraDevice()->triggerPrecaptureMetering(mTriggerId);
+
+    mAeInPrecapture = false;
+    mTimeoutCount = kMaxTimeoutsForPrecaptureStart;
+    return STANDARD_PRECAPTURE_WAIT;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageStandardPrecaptureWait(
+        sp<Camera2Client> &/*client*/) {
+    status_t res;
+    ATRACE_CALL();
+    Mutex::Autolock l(mInputMutex);
+    while (!mNewAEState) {
+        res = mNewNotifySignal.waitRelative(mInputMutex, kWaitDuration);
+        if (res == TIMED_OUT) {
+            mTimeoutCount--;
+            break;
+        }
+    }
+    if (mTimeoutCount <= 0) {
+        ALOGW("Timed out waiting for precapture %s",
+                mAeInPrecapture ? "end" : "start");
+        return STANDARD_CAPTURE;
+    }
+    if (mNewAEState) {
+        if (!mAeInPrecapture) {
+            // Waiting to see PRECAPTURE state
+            if (mAETriggerId == mTriggerId) {
+                if (mAEState == ANDROID_CONTROL_AE_STATE_PRECAPTURE) {
+                    ALOGV("%s: Got precapture start", __FUNCTION__);
+                    mAeInPrecapture = true;
+                    mTimeoutCount = kMaxTimeoutsForPrecaptureEnd;
+                } else if (mAEState == ANDROID_CONTROL_AE_STATE_CONVERGED ||
+                        mAEState == ANDROID_CONTROL_AE_STATE_FLASH_REQUIRED) {
+                    // It is legal to transit to CONVERGED or FLASH_REQUIRED
+                    // directly after a trigger.
+                    ALOGV("%s: AE is already in good state, start capture", __FUNCTION__);
+                    return STANDARD_CAPTURE;
+                }
+            }
+        } else {
+            // Waiting to see PRECAPTURE state end
+            if (mAETriggerId == mTriggerId &&
+                    mAEState != ANDROID_CONTROL_AE_STATE_PRECAPTURE) {
+                ALOGV("%s: Got precapture end", __FUNCTION__);
+                return STANDARD_CAPTURE;
+            }
+        }
+        mNewAEState = false;
+    }
+    return STANDARD_PRECAPTURE_WAIT;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageStandardCapture(
+        sp<Camera2Client> &client) {
+    status_t res;
+    ATRACE_CALL();
+    SharedParameters::Lock l(client->getParameters());
+    Vector<int32_t> outputStreams;
+    uint8_t captureIntent = static_cast<uint8_t>(ANDROID_CONTROL_CAPTURE_INTENT_STILL_CAPTURE);
+
+    /**
+     * Set up output streams in the request
+     *  - preview
+     *  - capture/jpeg
+     *  - callback (if preview callbacks enabled)
+     *  - recording (if recording enabled)
+     */
+    outputStreams.push(client->getPreviewStreamId());
+
+    int captureStreamId = client->getCaptureStreamId();
+    if (captureStreamId == Camera2Client::NO_STREAM) {
+        res = client->createJpegStreamL(l.mParameters);
+        if (res != OK || client->getCaptureStreamId() == Camera2Client::NO_STREAM) {
+            ALOGE("%s: Camera %d: cannot create jpeg stream for slowJpeg mode: %s (%d)",
+                  __FUNCTION__, client->getCameraId(), strerror(-res), res);
+            return DONE;
+        }
+    }
+
+    outputStreams.push(client->getCaptureStreamId());
+
+    if (l.mParameters.previewCallbackFlags &
+            CAMERA_FRAME_CALLBACK_FLAG_ENABLE_MASK) {
+        outputStreams.push(client->getCallbackStreamId());
+    }
+
+    if (l.mParameters.state == Parameters::VIDEO_SNAPSHOT) {
+        outputStreams.push(client->getRecordingStreamId());
+        captureIntent = static_cast<uint8_t>(ANDROID_CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT);
+    }
+
+    res = mCaptureRequest.update(ANDROID_REQUEST_OUTPUT_STREAMS,
+            outputStreams);
+    if (res == OK) {
+        res = mCaptureRequest.update(ANDROID_REQUEST_ID,
+                &mCaptureId, 1);
+    }
+    if (res == OK) {
+        res = mCaptureRequest.update(ANDROID_CONTROL_CAPTURE_INTENT,
+                &captureIntent, 1);
+    }
+    if (res == OK) {
+        res = mCaptureRequest.sort();
+    }
+
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to set up still capture request: %s (%d)",
+                __FUNCTION__, client->getCameraId(), strerror(-res), res);
+        return DONE;
+    }
+
+    // Create a capture copy since CameraDeviceBase#capture takes ownership
+    CameraMetadata captureCopy = mCaptureRequest;
+    if (captureCopy.entryCount() == 0) {
+        ALOGE("%s: Camera %d: Unable to copy capture request for HAL device",
+                __FUNCTION__, client->getCameraId());
+        return DONE;
+    }
+
+    /**
+     * Clear the streaming request for still-capture pictures
+     *   (as opposed to i.e. video snapshots)
+     */
+    if (l.mParameters.state == Parameters::STILL_CAPTURE) {
+        // API definition of takePicture() - stop preview before taking pic
+        res = client->stopStream();
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to stop preview for still capture: "
+                    "%s (%d)",
+                    __FUNCTION__, client->getCameraId(), strerror(-res), res);
+            return DONE;
+        }
+    }
+
+    // TODO: Capture should be atomic with setStreamingRequest here
+    res = client->getCameraDevice()->capture(captureCopy);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to submit still image capture request: "
+                "%s (%d)",
+                __FUNCTION__, client->getCameraId(), strerror(-res), res);
+        return DONE;
+    }
+
+    mTimeoutCount = kMaxTimeoutsForCaptureEnd;
+    return STANDARD_CAPTURE_WAIT;
+}
+
+CaptureSequencer::CaptureState CaptureSequencer::manageStandardCaptureWait(
+        sp<Camera2Client> &client) {
+    status_t res;
+    ATRACE_CALL();
+    Mutex::Autolock l(mInputMutex);
+
+
+    // Wait for shutter callback
+    while (!mHalNotifiedShutter) {
+        if (mTimeoutCount <= 0) {
+            break;
+        }
+        res = mShutterNotifySignal.waitRelative(mInputMutex, kWaitDuration);
+        if (res == TIMED_OUT) {
+            mTimeoutCount--;
+            return STANDARD_CAPTURE_WAIT;
+        }
+    }
+
+    if (mHalNotifiedShutter) {
+        if (!mShutterNotified) {
+            SharedParameters::Lock l(client->getParameters());
+            /* warning: this also locks a SharedCameraCallbacks */
+            shutterNotifyLocked(l.mParameters, client, mMsgType);
+            mShutterNotified = true;
+        }
+    } else if (mTimeoutCount <= 0) {
+        ALOGW("Timed out waiting for shutter notification");
+        return DONE;
+    }
+
+    // Wait for new metadata result (mNewFrame)
+    while (!mNewFrameReceived) {
+        res = mNewFrameSignal.waitRelative(mInputMutex, kWaitDuration);
+        if (res == TIMED_OUT) {
+            mTimeoutCount--;
+            break;
+        }
+    }
+
+    // Wait until jpeg was captured by JpegProcessor
+    while (mNewFrameReceived && !mNewCaptureReceived) {
+        res = mNewCaptureSignal.waitRelative(mInputMutex, kWaitDuration);
+        if (res == TIMED_OUT) {
+            mTimeoutCount--;
+            break;
+        }
+    }
+    if (mNewCaptureReceived) {
+        if (mNewCaptureErrorCnt > kMaxRetryCount) {
+            ALOGW("Exceeding multiple retry limit of %d due to buffer drop", kMaxRetryCount);
+            return DONE;
+        } else if (mNewCaptureErrorCnt > 0) {
+            ALOGW("Capture error happened, retry %d...", mNewCaptureErrorCnt);
+            mNewCaptureReceived = false;
+            return STANDARD_CAPTURE;
+        }
+    }
+
+    if (mTimeoutCount <= 0) {
+        ALOGW("Timed out waiting for capture to complete");
+        return DONE;
+    }
+    if (mNewFrameReceived && mNewCaptureReceived) {
+
+        if (mNewFrameId != mCaptureId) {
+            ALOGW("Mismatched capture frame IDs: Expected %d, got %d",
+                    mCaptureId, mNewFrameId);
+        }
+        camera_metadata_entry_t entry;
+        entry = mNewFrame.find(ANDROID_SENSOR_TIMESTAMP);
+        if (entry.count == 0) {
+            ALOGE("No timestamp field in capture frame!");
+        } else if (entry.count == 1) {
+            if (entry.data.i64[0] != mCaptureTimestamp) {
+                ALOGW("Mismatched capture timestamps: Metadata frame %" PRId64 ","
+                        " captured buffer %" PRId64,
+                        entry.data.i64[0],
+                        mCaptureTimestamp);
+            }
+        } else {
+            ALOGE("Timestamp metadata is malformed!");
+        }
+        client->removeFrameListener(mCaptureId, mCaptureId + 1, this);
+
+        mNewFrameReceived = false;
+        mNewCaptureReceived = false;
+        return DONE;
+    }
+    return STANDARD_CAPTURE_WAIT;
+}
+
+status_t CaptureSequencer::updateCaptureRequest(const Parameters &params,
+        sp<Camera2Client> &client) {
+    ATRACE_CALL();
+    status_t res;
+    if (mCaptureRequest.entryCount() == 0) {
+        res = client->getCameraDevice()->createDefaultRequest(
+                CAMERA2_TEMPLATE_STILL_CAPTURE,
+                &mCaptureRequest);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to create default still image request:"
+                    " %s (%d)", __FUNCTION__, client->getCameraId(),
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    res = params.updateRequest(&mCaptureRequest);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update common entries of capture "
+                "request: %s (%d)", __FUNCTION__, client->getCameraId(),
+                strerror(-res), res);
+        return res;
+    }
+
+    res = params.updateRequestJpeg(&mCaptureRequest);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update JPEG entries of capture "
+                "request: %s (%d)", __FUNCTION__, client->getCameraId(),
+                strerror(-res), res);
+        return res;
+    }
+
+    return OK;
+}
+
+/*static*/ void CaptureSequencer::shutterNotifyLocked(const Parameters &params,
+            const sp<Camera2Client>& client, int msgType) {
+    ATRACE_CALL();
+
+    if (params.state == Parameters::STILL_CAPTURE
+        && params.playShutterSound
+        && (msgType & CAMERA_MSG_SHUTTER)) {
+        client->getCameraService()->playSound(CameraService::SOUND_SHUTTER);
+    }
+
+    {
+        Camera2Client::SharedCameraCallbacks::Lock
+            l(client->mSharedCameraCallbacks);
+
+        ALOGV("%s: Notifying of shutter close to client", __FUNCTION__);
+        if (l.mRemoteCallback != 0) {
+            // ShutterCallback
+            l.mRemoteCallback->notifyCallback(CAMERA_MSG_SHUTTER,
+                                            /*ext1*/0, /*ext2*/0);
+
+            // RawCallback with null buffer
+            l.mRemoteCallback->notifyCallback(CAMERA_MSG_RAW_IMAGE_NOTIFY,
+                                            /*ext1*/0, /*ext2*/0);
+        } else {
+            ALOGV("%s: No client!", __FUNCTION__);
+        }
+    }
+}
+
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.h b/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.h
new file mode 100644
index 000000000..f2e37508f
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/CaptureSequencer.h
@@ -0,0 +1,181 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_CAPTURESEQUENCER_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_CAPTURESEQUENCER_H
+
+#include <binder/MemoryBase.h>
+#include <utils/Thread.h>
+#include <utils/String16.h>
+#include <utils/Vector.h>
+#include <utils/Mutex.h>
+#include <utils/Condition.h>
+#include "camera/CameraMetadata.h"
+#include "camera/CaptureResult.h"
+#include "Parameters.h"
+#include "FrameProcessor.h"
+
+namespace android {
+
+class Camera2Client;
+
+namespace camera2 {
+
+class ZslProcessor;
+
+/**
+ * Manages the still image capture process for
+ * zero-shutter-lag, regular, and video snapshots.
+ */
+class CaptureSequencer:
+            virtual public Thread,
+            virtual public FrameProcessor::FilteredListener {
+  public:
+    explicit CaptureSequencer(wp<Camera2Client> client);
+    ~CaptureSequencer();
+
+    // Get reference to the ZslProcessor, which holds the ZSL buffers and frames
+    void setZslProcessor(const wp<ZslProcessor>& processor);
+
+    // Begin still image capture
+    status_t startCapture(int msgType);
+
+    // Wait until current image capture completes; returns immediately if no
+    // capture is active. Returns TIMED_OUT if capture does not complete during
+    // the specified duration.
+    status_t waitUntilIdle(nsecs_t timeout);
+
+    // Notifications about AE state changes
+    void notifyAutoExposure(uint8_t newState, int triggerId);
+
+    // Notifications about shutter (capture start)
+    void notifyShutter(const CaptureResultExtras& resultExtras,
+                       nsecs_t timestamp);
+
+    // Notification from the frame processor
+    virtual void onResultAvailable(const CaptureResult &result);
+
+    // Notifications from the JPEG processor
+    void onCaptureAvailable(nsecs_t timestamp, const sp<MemoryBase>& captureBuffer, bool captureError);
+
+    void dump(int fd, const Vector<String16>& args);
+
+  private:
+    /**
+     * Accessed by other threads
+     */
+    Mutex mInputMutex;
+
+    bool mStartCapture;
+    bool mBusy;
+    Condition mStartCaptureSignal;
+
+    bool mNewAEState;
+    uint8_t mAEState;
+    int mAETriggerId;
+    Condition mNewNotifySignal;
+
+    bool mNewFrameReceived;
+    int32_t mNewFrameId;
+    CameraMetadata mNewFrame;
+    Condition mNewFrameSignal;
+
+    bool mNewCaptureReceived;
+    int32_t mNewCaptureErrorCnt;
+    nsecs_t mCaptureTimestamp;
+    sp<MemoryBase> mCaptureBuffer;
+    Condition mNewCaptureSignal;
+
+    bool mShutterNotified; // Has CaptureSequencer sent shutter to Client
+    bool mHalNotifiedShutter; // Has HAL sent shutter to CaptureSequencer
+    int32_t mShutterCaptureId; // The captureId which is waiting for shutter notification
+    Condition mShutterNotifySignal;
+
+    /**
+     * Internal to CaptureSequencer
+     */
+    static const nsecs_t kWaitDuration = 100000000; // 100 ms
+    static const int kMaxTimeoutsForPrecaptureStart = 10; // 1 sec
+    static const int kMaxTimeoutsForPrecaptureEnd = 20;  // 2 sec
+    static const int kMaxTimeoutsForCaptureEnd    = 40;  // 4 sec
+    static const int kMaxRetryCount = 3; // 3 retries in case of buffer drop
+
+    wp<Camera2Client> mClient;
+    wp<ZslProcessor> mZslProcessor;
+
+    enum CaptureState {
+        IDLE,
+        START,
+        ZSL_START,
+        ZSL_WAITING,
+        ZSL_REPROCESSING,
+        STANDARD_START,
+        STANDARD_PRECAPTURE_WAIT,
+        STANDARD_CAPTURE,
+        STANDARD_CAPTURE_WAIT,
+        DONE,
+        ERROR,
+        NUM_CAPTURE_STATES
+    } mCaptureState;
+    static const char* kStateNames[];
+    int mStateTransitionCount;
+    Mutex mStateMutex; // Guards mCaptureState
+    Condition mStateChanged;
+
+    typedef CaptureState (CaptureSequencer::*StateManager)(sp<Camera2Client> &client);
+    static const StateManager kStateManagers[];
+
+    CameraMetadata mCaptureRequest;
+
+    int mTriggerId;
+    int mTimeoutCount;
+    bool mAeInPrecapture;
+
+    int32_t mCaptureId;
+    int mMsgType;
+
+    // Main internal methods
+
+    virtual bool threadLoop();
+
+    CaptureState manageIdle(sp<Camera2Client> &client);
+    CaptureState manageStart(sp<Camera2Client> &client);
+
+    CaptureState manageZslStart(sp<Camera2Client> &client);
+    CaptureState manageZslWaiting(sp<Camera2Client> &client);
+    CaptureState manageZslReprocessing(sp<Camera2Client> &client);
+
+    CaptureState manageStandardStart(sp<Camera2Client> &client);
+    CaptureState manageStandardPrecaptureWait(sp<Camera2Client> &client);
+    CaptureState manageStandardCapture(sp<Camera2Client> &client);
+    CaptureState manageStandardCaptureWait(sp<Camera2Client> &client);
+
+    CaptureState manageDone(sp<Camera2Client> &client);
+
+    // Utility methods
+
+    status_t updateCaptureRequest(const Parameters &params,
+            sp<Camera2Client> &client);
+
+    // Emit Shutter/Raw callback to java, and maybe play a shutter sound
+    static void shutterNotifyLocked(const Parameters &params,
+            const sp<Camera2Client>& client, int msgType);
+};
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.cpp b/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.cpp
new file mode 100644
index 000000000..7b9e98d3d
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.cpp
@@ -0,0 +1,400 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-FrameProcessor"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+
+#include "common/CameraDeviceBase.h"
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/FrameProcessor.h"
+
+namespace android {
+namespace camera2 {
+
+FrameProcessor::FrameProcessor(wp<CameraDeviceBase> device,
+                               sp<Camera2Client> client) :
+    FrameProcessorBase(device),
+    mClient(client),
+    mLastFrameNumberOfFaces(0),
+    mLast3AFrameNumber(-1) {
+
+    sp<CameraDeviceBase> d = device.promote();
+    mSynthesize3ANotify = !(d->willNotify3A());
+
+    {
+        SharedParameters::Lock l(client->getParameters());
+
+        mUsePartialResult = (mNumPartialResults > 1);
+
+        // Initialize starting 3A state
+        m3aState.afTriggerId = l.mParameters.afTriggerCounter;
+        m3aState.aeTriggerId = l.mParameters.precaptureTriggerCounter;
+        // Check if lens is fixed-focus
+        if (l.mParameters.focusMode == Parameters::FOCUS_MODE_FIXED) {
+            m3aState.afMode = ANDROID_CONTROL_AF_MODE_OFF;
+        } else {
+            m3aState.afMode = ANDROID_CONTROL_AF_MODE_AUTO;
+        }
+        m3aState.awbMode = ANDROID_CONTROL_AWB_MODE_AUTO;
+        m3aState.aeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+        m3aState.afState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+        m3aState.awbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+    }
+}
+
+FrameProcessor::~FrameProcessor() {
+}
+
+bool FrameProcessor::processSingleFrame(CaptureResult &frame,
+                                        const sp<CameraDeviceBase> &device) {
+
+    sp<Camera2Client> client = mClient.promote();
+    if (!client.get()) {
+        return false;
+    }
+
+    bool isPartialResult = false;
+    if (mUsePartialResult) {
+        isPartialResult = frame.mResultExtras.partialResultCount < mNumPartialResults;
+    }
+
+    if (!isPartialResult && processFaceDetect(frame.mMetadata, client) != OK) {
+        return false;
+    }
+
+    if (mSynthesize3ANotify) {
+        process3aState(frame, client);
+    }
+
+    return FrameProcessorBase::processSingleFrame(frame, device);
+}
+
+status_t FrameProcessor::processFaceDetect(const CameraMetadata &frame,
+        const sp<Camera2Client> &client) {
+    status_t res = BAD_VALUE;
+    ATRACE_CALL();
+    camera_metadata_ro_entry_t entry;
+    bool enableFaceDetect;
+
+    {
+        SharedParameters::Lock l(client->getParameters());
+        enableFaceDetect = l.mParameters.enableFaceDetect;
+    }
+    entry = frame.find(ANDROID_STATISTICS_FACE_DETECT_MODE);
+
+    // TODO: This should be an error once implementations are compliant
+    if (entry.count == 0) {
+        return OK;
+    }
+
+    uint8_t faceDetectMode = entry.data.u8[0];
+
+    camera_frame_metadata metadata;
+    Vector<camera_face_t> faces;
+    metadata.number_of_faces = 0;
+
+    if (enableFaceDetect &&
+        faceDetectMode != ANDROID_STATISTICS_FACE_DETECT_MODE_OFF) {
+
+        SharedParameters::Lock l(client->getParameters());
+        entry = frame.find(ANDROID_STATISTICS_FACE_RECTANGLES);
+        if (entry.count == 0) {
+            // No faces this frame
+            /* warning: locks SharedCameraCallbacks */
+            callbackFaceDetection(client, metadata);
+            return OK;
+        }
+        metadata.number_of_faces = entry.count / 4;
+        if (metadata.number_of_faces >
+                l.mParameters.fastInfo.maxFaces) {
+            ALOGE("%s: Camera %d: More faces than expected! (Got %d, max %d)",
+                    __FUNCTION__, client->getCameraId(),
+                    metadata.number_of_faces, l.mParameters.fastInfo.maxFaces);
+            return res;
+        }
+        const int32_t *faceRects = entry.data.i32;
+
+        entry = frame.find(ANDROID_STATISTICS_FACE_SCORES);
+        if (entry.count == 0) {
+            ALOGE("%s: Camera %d: Unable to read face scores",
+                    __FUNCTION__, client->getCameraId());
+            return res;
+        }
+        const uint8_t *faceScores = entry.data.u8;
+
+        const int32_t *faceLandmarks = NULL;
+        const int32_t *faceIds = NULL;
+
+        if (faceDetectMode == ANDROID_STATISTICS_FACE_DETECT_MODE_FULL) {
+            entry = frame.find(ANDROID_STATISTICS_FACE_LANDMARKS);
+            if (entry.count == 0) {
+                ALOGE("%s: Camera %d: Unable to read face landmarks",
+                        __FUNCTION__, client->getCameraId());
+                return res;
+            }
+            faceLandmarks = entry.data.i32;
+
+            entry = frame.find(ANDROID_STATISTICS_FACE_IDS);
+
+            if (entry.count == 0) {
+                ALOGE("%s: Camera %d: Unable to read face IDs",
+                        __FUNCTION__, client->getCameraId());
+                return res;
+            }
+            faceIds = entry.data.i32;
+        }
+
+        entry = frame.find(ANDROID_SCALER_CROP_REGION);
+        if (entry.count < 4) {
+            ALOGE("%s: Camera %d: Unable to read crop region (count = %zu)",
+                    __FUNCTION__, client->getCameraId(), entry.count);
+            return res;
+        }
+
+        Parameters::CropRegion scalerCrop = {
+            static_cast<float>(entry.data.i32[0]),
+            static_cast<float>(entry.data.i32[1]),
+            static_cast<float>(entry.data.i32[2]),
+            static_cast<float>(entry.data.i32[3])};
+
+        faces.setCapacity(metadata.number_of_faces);
+
+        size_t maxFaces = metadata.number_of_faces;
+        for (size_t i = 0; i < maxFaces; i++) {
+            if (faceScores[i] == 0) {
+                metadata.number_of_faces--;
+                continue;
+            }
+            if (faceScores[i] > 100) {
+                ALOGW("%s: Face index %zu with out of range score %d",
+                        __FUNCTION__, i, faceScores[i]);
+            }
+
+            camera_face_t face;
+
+            face.rect[0] = l.mParameters.arrayXToNormalizedWithCrop(
+                                faceRects[i*4 + 0], scalerCrop);
+            face.rect[1] = l.mParameters.arrayYToNormalizedWithCrop(
+                                faceRects[i*4 + 1], scalerCrop);
+            face.rect[2] = l.mParameters.arrayXToNormalizedWithCrop(
+                                faceRects[i*4 + 2], scalerCrop);
+            face.rect[3] = l.mParameters.arrayYToNormalizedWithCrop(
+                                faceRects[i*4 + 3], scalerCrop);
+
+            face.score = faceScores[i];
+            if (faceDetectMode == ANDROID_STATISTICS_FACE_DETECT_MODE_FULL) {
+                face.id = faceIds[i];
+                face.left_eye[0] = l.mParameters.arrayXToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 0], scalerCrop);
+                face.left_eye[1] = l.mParameters.arrayYToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 1], scalerCrop);
+                face.right_eye[0] = l.mParameters.arrayXToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 2], scalerCrop);
+                face.right_eye[1] = l.mParameters.arrayYToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 3], scalerCrop);
+                face.mouth[0] = l.mParameters.arrayXToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 4], scalerCrop);
+                face.mouth[1] = l.mParameters.arrayYToNormalizedWithCrop(
+                        faceLandmarks[i*6 + 5], scalerCrop);
+            } else {
+                face.id = 0;
+                face.left_eye[0] = face.left_eye[1] = -2000;
+                face.right_eye[0] = face.right_eye[1] = -2000;
+                face.mouth[0] = face.mouth[1] = -2000;
+            }
+            faces.push_back(face);
+        }
+
+        metadata.faces = faces.editArray();
+    }
+
+    /* warning: locks SharedCameraCallbacks */
+    callbackFaceDetection(client, metadata);
+
+    return OK;
+}
+
+status_t FrameProcessor::process3aState(const CaptureResult &frame,
+        const sp<Camera2Client> &client) {
+
+    ATRACE_CALL();
+    const CameraMetadata &metadata = frame.mMetadata;
+    camera_metadata_ro_entry_t entry;
+    int cameraId = client->getCameraId();
+
+    entry = metadata.find(ANDROID_REQUEST_FRAME_COUNT);
+    int32_t frameNumber = entry.data.i32[0];
+
+    // Don't send 3A notifications for the same frame number twice
+    if (frameNumber <= mLast3AFrameNumber) {
+        ALOGV("%s: Already sent 3A for frame number %d, skipping",
+                __FUNCTION__, frameNumber);
+
+        // Remove the entry if there is one for this frame number in mPending3AStates.
+        mPending3AStates.removeItem(frameNumber);
+        return OK;
+    }
+
+    AlgState pendingState;
+
+    ssize_t index = mPending3AStates.indexOfKey(frameNumber);
+    if (index != NAME_NOT_FOUND) {
+        pendingState = mPending3AStates.valueAt(index);
+    }
+
+    // Update 3A states from the result.
+    bool gotAllStates = true;
+
+    // TODO: Also use AE mode, AE trigger ID
+    gotAllStates &= updatePendingState<uint8_t>(metadata, ANDROID_CONTROL_AF_MODE,
+            &pendingState.afMode, frameNumber, cameraId);
+
+    gotAllStates &= updatePendingState<uint8_t>(metadata, ANDROID_CONTROL_AWB_MODE,
+            &pendingState.awbMode, frameNumber, cameraId);
+
+    gotAllStates &= updatePendingState<uint8_t>(metadata, ANDROID_CONTROL_AE_STATE,
+            &pendingState.aeState, frameNumber, cameraId);
+
+    gotAllStates &= updatePendingState<uint8_t>(metadata, ANDROID_CONTROL_AF_STATE,
+            &pendingState.afState, frameNumber, cameraId);
+
+    gotAllStates &= updatePendingState<uint8_t>(metadata, ANDROID_CONTROL_AWB_STATE,
+            &pendingState.awbState, frameNumber, cameraId);
+
+    pendingState.afTriggerId = frame.mResultExtras.afTriggerId;
+    pendingState.aeTriggerId = frame.mResultExtras.precaptureTriggerId;
+
+    if (!gotAllStates) {
+        // If not all states are received, put the pending state to mPending3AStates.
+        if (index == NAME_NOT_FOUND) {
+            mPending3AStates.add(frameNumber, pendingState);
+        } else {
+            mPending3AStates.replaceValueAt(index, pendingState);
+        }
+        return NOT_ENOUGH_DATA;
+    }
+
+    // Once all 3A states are received, notify the client about 3A changes.
+    if (pendingState.aeState != m3aState.aeState) {
+        ALOGV("%s: Camera %d: AE state %d->%d",
+                __FUNCTION__, cameraId,
+                m3aState.aeState, pendingState.aeState);
+        client->notifyAutoExposure(pendingState.aeState, pendingState.aeTriggerId);
+    }
+
+    if (pendingState.afState != m3aState.afState ||
+        pendingState.afMode != m3aState.afMode ||
+        pendingState.afTriggerId != m3aState.afTriggerId) {
+        ALOGV("%s: Camera %d: AF state %d->%d. AF mode %d->%d. Trigger %d->%d",
+                __FUNCTION__, cameraId,
+                m3aState.afState, pendingState.afState,
+                m3aState.afMode, pendingState.afMode,
+                m3aState.afTriggerId, pendingState.afTriggerId);
+        client->notifyAutoFocus(pendingState.afState, pendingState.afTriggerId);
+    }
+    if (pendingState.awbState != m3aState.awbState ||
+        pendingState.awbMode != m3aState.awbMode) {
+        ALOGV("%s: Camera %d: AWB state %d->%d. AWB mode %d->%d",
+                __FUNCTION__, cameraId,
+                m3aState.awbState, pendingState.awbState,
+                m3aState.awbMode, pendingState.awbMode);
+        client->notifyAutoWhitebalance(pendingState.awbState,
+                pendingState.aeTriggerId);
+    }
+
+    if (index != NAME_NOT_FOUND) {
+        mPending3AStates.removeItemsAt(index);
+    }
+
+    m3aState = pendingState;
+    mLast3AFrameNumber = frameNumber;
+
+    return OK;
+}
+
+template<typename Src, typename T>
+bool FrameProcessor::updatePendingState(const CameraMetadata& result, int32_t tag,
+        T* value, int32_t frameNumber, int cameraId) {
+    camera_metadata_ro_entry_t entry;
+    if (value == NULL) {
+        ALOGE("%s: Camera %d: Value to write to is NULL",
+                __FUNCTION__, cameraId);
+        return false;
+    }
+
+    // Already got the value for this tag.
+    if (*value != static_cast<T>(NOT_SET)) {
+        return true;
+    }
+
+    entry = result.find(tag);
+    if (entry.count == 0) {
+        const camera_metadata *metaBuffer = result.getAndLock();
+        ALOGV("%s: Camera %d: No %s provided by HAL for frame %d in this result!",
+                __FUNCTION__, cameraId,
+                get_local_camera_metadata_tag_name(tag, metaBuffer),
+                frameNumber);
+        result.unlock(metaBuffer);
+        return false;
+    } else {
+        switch(sizeof(Src)){
+            case sizeof(uint8_t):
+                *value = static_cast<T>(entry.data.u8[0]);
+                break;
+            case sizeof(int32_t):
+                *value = static_cast<T>(entry.data.i32[0]);
+                break;
+            default:
+                ALOGE("%s: Camera %d: Unsupported source",
+                        __FUNCTION__, cameraId);
+                return false;
+        }
+    }
+    return true;
+}
+
+
+void FrameProcessor::callbackFaceDetection(const sp<Camera2Client>& client,
+                                     const camera_frame_metadata &metadata) {
+
+    camera_frame_metadata *metadata_ptr =
+        const_cast<camera_frame_metadata*>(&metadata);
+
+    /**
+     * Filter out repeated 0-face callbacks,
+     * but not when the last frame was >0
+     */
+    if (metadata.number_of_faces != 0 ||
+        mLastFrameNumberOfFaces != metadata.number_of_faces) {
+
+        Camera2Client::SharedCameraCallbacks::Lock
+            l(client->mSharedCameraCallbacks);
+        if (l.mRemoteCallback != NULL) {
+            l.mRemoteCallback->dataCallback(CAMERA_MSG_PREVIEW_METADATA,
+                                            NULL,
+                                            metadata_ptr);
+        }
+    }
+
+    mLastFrameNumberOfFaces = metadata.number_of_faces;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.h b/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.h
new file mode 100644
index 000000000..62a4e91df
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/FrameProcessor.h
@@ -0,0 +1,118 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_FRAMEPROCESSOR_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_FRAMEPROCESSOR_H
+
+#include <utils/Thread.h>
+#include <utils/String16.h>
+#include <utils/Vector.h>
+#include <utils/KeyedVector.h>
+#include <utils/List.h>
+#include <camera/CameraMetadata.h>
+
+#include "common/FrameProcessorBase.h"
+
+struct camera_frame_metadata;
+
+namespace android {
+
+class Camera2Client;
+
+namespace camera2 {
+
+/* Output frame metadata processing thread.  This thread waits for new
+ * frames from the device, and analyzes them as necessary.
+ */
+class FrameProcessor : public FrameProcessorBase {
+  public:
+    FrameProcessor(wp<CameraDeviceBase> device, sp<Camera2Client> client);
+    ~FrameProcessor();
+
+  private:
+    static const int32_t NOT_SET = -1;
+
+    wp<Camera2Client> mClient;
+
+    bool mSynthesize3ANotify;
+
+    int mLastFrameNumberOfFaces;
+
+    void processNewFrames(const sp<Camera2Client> &client);
+
+    virtual bool processSingleFrame(CaptureResult &frame,
+                                    const sp<CameraDeviceBase> &device);
+
+    status_t processFaceDetect(const CameraMetadata &frame,
+            const sp<Camera2Client> &client);
+
+    // Send 3A state change notifications to client based on frame metadata
+    status_t process3aState(const CaptureResult &frame,
+            const sp<Camera2Client> &client);
+
+    // Helper for process3aState
+    template<typename Src, typename T>
+    bool updatePendingState(const CameraMetadata& result, int32_t tag, T* value,
+            int32_t frameNumber, int cameraId);
+
+
+    struct AlgState {
+        // TODO: also track AE mode
+        camera_metadata_enum_android_control_af_mode   afMode;
+        camera_metadata_enum_android_control_awb_mode  awbMode;
+
+        camera_metadata_enum_android_control_ae_state  aeState;
+        camera_metadata_enum_android_control_af_state  afState;
+        camera_metadata_enum_android_control_awb_state awbState;
+
+        int32_t                                        afTriggerId;
+        int32_t                                        aeTriggerId;
+
+        // These defaults need to match those in Parameters.cpp
+        AlgState() :
+                afMode((camera_metadata_enum_android_control_af_mode)NOT_SET),
+                awbMode((camera_metadata_enum_android_control_awb_mode)NOT_SET),
+                aeState((camera_metadata_enum_android_control_ae_state)NOT_SET),
+                afState((camera_metadata_enum_android_control_af_state)NOT_SET),
+                awbState((camera_metadata_enum_android_control_awb_state)NOT_SET),
+                afTriggerId(NOT_SET),
+                aeTriggerId(NOT_SET) {
+        }
+    };
+
+    AlgState m3aState;
+
+    // frame number -> pending 3A states that not all data are received yet.
+    KeyedVector<int32_t, AlgState> mPending3AStates;
+
+    // Whether the partial result is enabled for this device
+    bool mUsePartialResult;
+
+    // Track most recent frame number for which 3A notifications were sent for.
+    // Used to filter against sending 3A notifications for the same frame
+    // several times.
+    int32_t mLast3AFrameNumber;
+
+    // Emit FaceDetection event to java if faces changed
+    void callbackFaceDetection(const sp<Camera2Client>& client,
+                               const camera_frame_metadata &metadata);
+};
+
+
+}; //namespace camera2
+}; //namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.cpp b/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.cpp
new file mode 100644
index 000000000..01951a09b
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.cpp
@@ -0,0 +1,221 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+//#define LOG_NDEBUG 0
+#define LOG_TAG "Camera2-JpegCompressor"
+
+#include <utils/Log.h>
+#include <ui/GraphicBufferMapper.h>
+
+#include "JpegCompressor.h"
+
+namespace android {
+namespace camera2 {
+
+JpegCompressor::JpegCompressor():
+        Thread(false),
+        mIsBusy(false),
+        mCaptureTime(0) {
+}
+
+JpegCompressor::~JpegCompressor() {
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock lock(mMutex);
+}
+
+status_t JpegCompressor::start(const Vector<CpuConsumer::LockedBuffer*>& buffers,
+        nsecs_t captureTime) {
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock busyLock(mBusyMutex);
+
+    if (mIsBusy) {
+        ALOGE("%s: Already processing a buffer!", __FUNCTION__);
+        return INVALID_OPERATION;
+    }
+
+    mIsBusy = true;
+
+    mBuffers = buffers;
+    mCaptureTime = captureTime;
+
+    status_t res;
+    res = run("JpegCompressor");
+    if (res != OK) {
+        ALOGE("%s: Unable to start up compression thread: %s (%d)",
+                __FUNCTION__, strerror(-res), res);
+        //delete mBuffers;  // necessary?
+    }
+    return res;
+}
+
+status_t JpegCompressor::cancel() {
+    ALOGV("%s", __FUNCTION__);
+    requestExitAndWait();
+    return OK;
+}
+
+status_t JpegCompressor::readyToRun() {
+    ALOGV("%s", __FUNCTION__);
+    return OK;
+}
+
+bool JpegCompressor::threadLoop() {
+    ALOGV("%s", __FUNCTION__);
+
+    mAuxBuffer = mBuffers[0];    // input
+    mJpegBuffer = mBuffers[1];    // output
+
+    // Set up error management
+    mJpegErrorInfo = NULL;
+    JpegError error;
+    error.parent = this;
+
+    mCInfo.err = jpeg_std_error(&error);
+    mCInfo.err->error_exit = jpegErrorHandler;
+
+    jpeg_create_compress(&mCInfo);
+    if (checkError("Error initializing compression")) return false;
+
+    // Route compressed data straight to output stream buffer
+    JpegDestination jpegDestMgr;
+    jpegDestMgr.parent = this;
+    jpegDestMgr.init_destination = jpegInitDestination;
+    jpegDestMgr.empty_output_buffer = jpegEmptyOutputBuffer;
+    jpegDestMgr.term_destination = jpegTermDestination;
+
+    mCInfo.dest = &jpegDestMgr;
+
+    // Set up compression parameters
+    mCInfo.image_width = mAuxBuffer->width;
+    mCInfo.image_height = mAuxBuffer->height;
+    mCInfo.input_components = 1; // 3;
+    mCInfo.in_color_space = JCS_GRAYSCALE; // JCS_RGB
+
+    ALOGV("%s: image_width = %d, image_height = %d", __FUNCTION__, mCInfo.image_width, mCInfo.image_height);
+
+    jpeg_set_defaults(&mCInfo);
+    if (checkError("Error configuring defaults")) return false;
+
+    // Do compression
+    jpeg_start_compress(&mCInfo, TRUE);
+    if (checkError("Error starting compression")) return false;
+
+    size_t rowStride = mAuxBuffer->stride;// * 3;
+    const size_t kChunkSize = 32;
+    while (mCInfo.next_scanline < mCInfo.image_height) {
+        JSAMPROW chunk[kChunkSize];
+        for (size_t i = 0 ; i < kChunkSize; i++) {
+            chunk[i] = (JSAMPROW)
+                    (mAuxBuffer->data + (i + mCInfo.next_scanline) * rowStride);
+        }
+        jpeg_write_scanlines(&mCInfo, chunk, kChunkSize);
+        if (checkError("Error while compressing")) return false;
+        if (exitPending()) {
+            ALOGV("%s: Cancel called, exiting early", __FUNCTION__);
+            cleanUp();
+            return false;
+        }
+    }
+
+    jpeg_finish_compress(&mCInfo);
+    if (checkError("Error while finishing compression")) return false;
+
+    cleanUp();
+    return false;
+}
+
+bool JpegCompressor::isBusy() {
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock busyLock(mBusyMutex);
+    return mIsBusy;
+}
+
+// old function -- TODO: update for new buffer type
+bool JpegCompressor::isStreamInUse(uint32_t /*id*/) {
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock lock(mBusyMutex);
+
+    if (mBuffers.size() && mIsBusy) {
+        for (size_t i = 0; i < mBuffers.size(); i++) {
+//            if ( mBuffers[i].streamId == (int)id ) return true;
+        }
+    }
+    return false;
+}
+
+bool JpegCompressor::waitForDone(nsecs_t timeout) {
+    ALOGV("%s", __FUNCTION__);
+    Mutex::Autolock lock(mBusyMutex);
+    status_t res = OK;
+    if (mIsBusy) {
+        res = mDone.waitRelative(mBusyMutex, timeout);
+    }
+    return (res == OK);
+}
+
+bool JpegCompressor::checkError(const char *msg) {
+    ALOGV("%s", __FUNCTION__);
+    if (mJpegErrorInfo) {
+        char errBuffer[JMSG_LENGTH_MAX];
+        mJpegErrorInfo->err->format_message(mJpegErrorInfo, errBuffer);
+        ALOGE("%s: %s: %s",
+                __FUNCTION__, msg, errBuffer);
+        cleanUp();
+        mJpegErrorInfo = NULL;
+        return true;
+    }
+    return false;
+}
+
+void JpegCompressor::cleanUp() {
+    ALOGV("%s", __FUNCTION__);
+    jpeg_destroy_compress(&mCInfo);
+    Mutex::Autolock lock(mBusyMutex);
+    mIsBusy = false;
+    mDone.signal();
+}
+
+void JpegCompressor::jpegErrorHandler(j_common_ptr cinfo) {
+    ALOGV("%s", __FUNCTION__);
+    JpegError *error = static_cast<JpegError*>(cinfo->err);
+    error->parent->mJpegErrorInfo = cinfo;
+}
+
+void JpegCompressor::jpegInitDestination(j_compress_ptr cinfo) {
+    ALOGV("%s", __FUNCTION__);
+    JpegDestination *dest= static_cast<JpegDestination*>(cinfo->dest);
+    ALOGV("%s: Setting destination to %p, size %zu",
+            __FUNCTION__, dest->parent->mJpegBuffer->data, kMaxJpegSize);
+    dest->next_output_byte = (JOCTET*)(dest->parent->mJpegBuffer->data);
+    dest->free_in_buffer = kMaxJpegSize;
+}
+
+boolean JpegCompressor::jpegEmptyOutputBuffer(j_compress_ptr /*cinfo*/) {
+    ALOGV("%s", __FUNCTION__);
+    ALOGE("%s: JPEG destination buffer overflow!",
+            __FUNCTION__);
+    return true;
+}
+
+void JpegCompressor::jpegTermDestination(j_compress_ptr cinfo) {
+    (void) cinfo; // TODO: clean up
+    ALOGV("%s", __FUNCTION__);
+    ALOGV("%s: Done writing JPEG data. %zu bytes left in buffer",
+            __FUNCTION__, cinfo->dest->free_in_buffer);
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.h b/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.h
new file mode 100644
index 000000000..589a2fdc1
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/JpegCompressor.h
@@ -0,0 +1,106 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * This class simulates a hardware JPEG compressor.  It receives image buffers
+ * in RGBA_8888 format, processes them in a worker thread, and then pushes them
+ * out to their destination stream.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_JPEGCOMPRESSOR_H
+#define ANDROID_SERVERS_CAMERA_JPEGCOMPRESSOR_H
+
+#include "utils/Thread.h"
+#include "utils/Mutex.h"
+#include "utils/Timers.h"
+#include "utils/Vector.h"
+//#include "Base.h"
+#include <stdio.h>
+#include <gui/CpuConsumer.h>
+
+extern "C" {
+#include <jpeglib.h>
+}
+
+
+namespace android {
+namespace camera2 {
+
+class JpegCompressor: private Thread, public virtual RefBase {
+  public:
+
+    JpegCompressor();
+    ~JpegCompressor();
+
+    // Start compressing COMPRESSED format buffers; JpegCompressor takes
+    // ownership of the Buffers vector.
+    status_t start(const Vector<CpuConsumer::LockedBuffer*>& buffers,
+            nsecs_t captureTime);
+
+    status_t cancel();
+
+    bool isBusy();
+    bool isStreamInUse(uint32_t id);
+
+    bool waitForDone(nsecs_t timeout);
+
+    // TODO: Measure this
+    static const size_t kMaxJpegSize = 300000;
+
+  private:
+    Mutex mBusyMutex;
+    Mutex mMutex;
+    bool mIsBusy;
+    Condition mDone;
+    nsecs_t mCaptureTime;
+
+    Vector<CpuConsumer::LockedBuffer*> mBuffers;
+    CpuConsumer::LockedBuffer *mJpegBuffer;
+    CpuConsumer::LockedBuffer *mAuxBuffer;
+
+    jpeg_compress_struct mCInfo;
+
+    struct JpegError : public jpeg_error_mgr {
+        JpegCompressor *parent;
+    };
+    j_common_ptr mJpegErrorInfo;
+
+    struct JpegDestination : public jpeg_destination_mgr {
+        JpegCompressor *parent;
+    };
+
+    static void jpegErrorHandler(j_common_ptr cinfo);
+
+    static void jpegInitDestination(j_compress_ptr cinfo);
+    static boolean jpegEmptyOutputBuffer(j_compress_ptr cinfo);
+    static void jpegTermDestination(j_compress_ptr cinfo);
+
+    bool checkError(const char *msg);
+    void cleanUp();
+
+    /**
+     * Inherited Thread virtual overrides
+     */
+  private:
+    virtual status_t readyToRun();
+    virtual bool threadLoop();
+};
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.cpp b/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.cpp
new file mode 100644
index 000000000..1f6058a86
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.cpp
@@ -0,0 +1,440 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-JpegProcessor"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+
+#include <netinet/in.h>
+
+#include <binder/MemoryBase.h>
+#include <binder/MemoryHeapBase.h>
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <gui/Surface.h>
+
+#include "common/CameraDeviceBase.h"
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/Camera2Heap.h"
+#include "api1/qticlient2/CaptureSequencer.h"
+#include "api1/qticlient2/JpegProcessor.h"
+
+namespace android {
+namespace camera2 {
+
+JpegProcessor::JpegProcessor(
+    sp<Camera2Client> client,
+    wp<CaptureSequencer> sequencer):
+        Thread(false),
+        mDevice(client->getCameraDevice()),
+        mSequencer(sequencer),
+        mId(client->getCameraId()),
+        mCaptureDone(false),
+        mCaptureSuccess(false),
+        mCaptureStreamId(NO_STREAM) {
+}
+
+JpegProcessor::~JpegProcessor() {
+    ALOGV("%s: Exit", __FUNCTION__);
+    deleteStream();
+}
+
+void JpegProcessor::onFrameAvailable(const BufferItem& /*item*/) {
+    Mutex::Autolock l(mInputMutex);
+    ALOGV("%s", __FUNCTION__);
+    if (!mCaptureDone) {
+        mCaptureDone = true;
+        mCaptureSuccess = true;
+        mCaptureDoneSignal.signal();
+    }
+}
+
+void JpegProcessor::onBufferAcquired(const BufferInfo& /*bufferInfo*/) {
+    // Intentionally left empty
+}
+
+void JpegProcessor::onBufferReleased(const BufferInfo& bufferInfo) {
+    ALOGV("%s", __FUNCTION__);
+    if (bufferInfo.mError) {
+        // Only lock in case of error, since we get one of these for each
+        // onFrameAvailable as well, and scheduling may delay this call late
+        // enough to run into later preview restart operations, for non-error
+        // cases.
+        // b/29524651
+        ALOGV("%s: JPEG buffer lost", __FUNCTION__);
+        Mutex::Autolock l(mInputMutex);
+        mCaptureDone = true;
+        mCaptureSuccess = false;
+        mCaptureDoneSignal.signal();
+    }
+}
+
+status_t JpegProcessor::updateStream(const Parameters &params) {
+    ATRACE_CALL();
+    ALOGV("%s", __FUNCTION__);
+    status_t res;
+
+    Mutex::Autolock l(mInputMutex);
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    // Find out buffer size for JPEG
+    ssize_t maxJpegSize = device->getJpegBufferSize(params.pictureWidth, params.pictureHeight);
+    if (maxJpegSize <= 0) {
+        ALOGE("%s: Camera %d: Jpeg buffer size (%zu) is invalid ",
+                __FUNCTION__, mId, maxJpegSize);
+        return INVALID_OPERATION;
+    }
+
+    if (mCaptureConsumer == 0) {
+        // Create CPU buffer queue endpoint
+        sp<IGraphicBufferProducer> producer;
+        sp<IGraphicBufferConsumer> consumer;
+        BufferQueue::createBufferQueue(&producer, &consumer);
+        mCaptureConsumer = new CpuConsumer(consumer, 1);
+        mCaptureConsumer->setFrameAvailableListener(this);
+        mCaptureConsumer->setName(String8("Camera2-JpegConsumer"));
+        mCaptureWindow = new Surface(producer);
+    }
+
+    // Since ashmem heaps are rounded up to page size, don't reallocate if
+    // the capture heap isn't exactly the same size as the required JPEG buffer
+    const size_t HEAP_SLACK_FACTOR = 2;
+    if (mCaptureHeap == 0 ||
+            (mCaptureHeap->getSize() < static_cast<size_t>(maxJpegSize)) ||
+            (mCaptureHeap->getSize() >
+                    static_cast<size_t>(maxJpegSize) * HEAP_SLACK_FACTOR) ) {
+        // Create memory for API consumption
+        mCaptureHeap.clear();
+        mCaptureHeap =
+                new MemoryHeapBase(maxJpegSize, 0, "Camera2Client::CaptureHeap");
+        if (mCaptureHeap->getSize() == 0) {
+            ALOGE("%s: Camera %d: Unable to allocate memory for capture",
+                    __FUNCTION__, mId);
+            return NO_MEMORY;
+        }
+    }
+    ALOGV("%s: Camera %d: JPEG capture heap now %zu bytes; requested %zd bytes",
+            __FUNCTION__, mId, mCaptureHeap->getSize(), maxJpegSize);
+
+    if (mCaptureStreamId != NO_STREAM) {
+        // Check if stream parameters have to change
+        uint32_t currentWidth, currentHeight;
+        res = device->getStreamInfo(mCaptureStreamId,
+                &currentWidth, &currentHeight, 0, 0);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Error querying capture output stream info: "
+                    "%s (%d)", __FUNCTION__,
+                    mId, strerror(-res), res);
+            return res;
+        }
+        if (currentWidth != (uint32_t)params.pictureWidth ||
+                currentHeight != (uint32_t)params.pictureHeight) {
+            ALOGV("%s: Camera %d: Deleting stream %d since the buffer dimensions changed",
+                __FUNCTION__, mId, mCaptureStreamId);
+            res = device->deleteStream(mCaptureStreamId);
+            if (res == -EBUSY) {
+                ALOGV("%s: Camera %d: Device is busy, call updateStream again "
+                      " after it becomes idle", __FUNCTION__, mId);
+                return res;
+            } else if (res != OK) {
+                ALOGE("%s: Camera %d: Unable to delete old output stream "
+                        "for capture: %s (%d)", __FUNCTION__,
+                        mId, strerror(-res), res);
+                return res;
+            }
+            mCaptureStreamId = NO_STREAM;
+        }
+    }
+
+    if (mCaptureStreamId == NO_STREAM) {
+        // Create stream for HAL production
+        res = device->createStream(mCaptureWindow,
+                params.pictureWidth, params.pictureHeight,
+                HAL_PIXEL_FORMAT_BLOB, HAL_DATASPACE_V0_JFIF,
+                CAMERA3_STREAM_ROTATION_0, &mCaptureStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't create output stream for capture: "
+                    "%s (%d)", __FUNCTION__, mId,
+                    strerror(-res), res);
+            return res;
+        }
+
+        res = device->addBufferListenerForStream(mCaptureStreamId, this);
+        if (res != OK) {
+              ALOGE("%s: Camera %d: Can't add buffer listeneri: %s (%d)",
+                    __FUNCTION__, mId, strerror(-res), res);
+              return res;
+        }
+    }
+    return OK;
+}
+
+status_t JpegProcessor::deleteStream() {
+    ATRACE_CALL();
+
+    Mutex::Autolock l(mInputMutex);
+
+    if (mCaptureStreamId != NO_STREAM) {
+        sp<CameraDeviceBase> device = mDevice.promote();
+        if (device == 0) {
+            ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        device->deleteStream(mCaptureStreamId);
+
+        mCaptureHeap.clear();
+        mCaptureWindow.clear();
+        mCaptureConsumer.clear();
+
+        mCaptureStreamId = NO_STREAM;
+    }
+    return OK;
+}
+
+int JpegProcessor::getStreamId() const {
+    Mutex::Autolock l(mInputMutex);
+    return mCaptureStreamId;
+}
+
+void JpegProcessor::dump(int /*fd*/, const Vector<String16>& /*args*/) const {
+}
+
+bool JpegProcessor::threadLoop() {
+    status_t res;
+
+    bool captureSuccess = false;
+    {
+        Mutex::Autolock l(mInputMutex);
+
+        while (!mCaptureDone) {
+            res = mCaptureDoneSignal.waitRelative(mInputMutex,
+                    kWaitDuration);
+            if (res == TIMED_OUT) return true;
+        }
+
+        captureSuccess = mCaptureSuccess;
+        mCaptureDone = false;
+    }
+
+    res = processNewCapture(captureSuccess);
+
+    return true;
+}
+
+status_t JpegProcessor::processNewCapture(bool captureSuccess) {
+    ATRACE_CALL();
+    status_t res;
+    sp<Camera2Heap> captureHeap;
+    sp<MemoryBase> captureBuffer;
+
+    CpuConsumer::LockedBuffer imgBuffer;
+
+    if (captureSuccess) {
+        Mutex::Autolock l(mInputMutex);
+        if (mCaptureStreamId == NO_STREAM) {
+            ALOGW("%s: Camera %d: No stream is available", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        res = mCaptureConsumer->lockNextBuffer(&imgBuffer);
+        if (res != OK) {
+            if (res != BAD_VALUE) {
+                ALOGE("%s: Camera %d: Error receiving still image buffer: "
+                        "%s (%d)", __FUNCTION__,
+                        mId, strerror(-res), res);
+            }
+            return res;
+        }
+
+        ALOGV("%s: Camera %d: Still capture available", __FUNCTION__,
+                mId);
+
+        if (imgBuffer.format != HAL_PIXEL_FORMAT_BLOB) {
+            ALOGE("%s: Camera %d: Unexpected format for still image: "
+                    "%x, expected %x", __FUNCTION__, mId,
+                    imgBuffer.format,
+                    HAL_PIXEL_FORMAT_BLOB);
+            mCaptureConsumer->unlockBuffer(imgBuffer);
+            return OK;
+        }
+
+        // Find size of JPEG image
+        size_t jpegSize = findJpegSize(imgBuffer.data, imgBuffer.width);
+        if (jpegSize == 0) { // failed to find size, default to whole buffer
+            jpegSize = imgBuffer.width;
+        }
+        size_t heapSize = mCaptureHeap->getSize();
+        if (jpegSize > heapSize) {
+            ALOGW("%s: JPEG image is larger than expected, truncating "
+                    "(got %zu, expected at most %zu bytes)",
+                    __FUNCTION__, jpegSize, heapSize);
+            jpegSize = heapSize;
+        }
+
+        // TODO: Optimize this to avoid memcopy
+        captureBuffer = new MemoryBase(mCaptureHeap, 0, jpegSize);
+        void* captureMemory = mCaptureHeap->getBase();
+        memcpy(captureMemory, imgBuffer.data, jpegSize);
+
+        mCaptureConsumer->unlockBuffer(imgBuffer);
+    }
+
+    sp<CaptureSequencer> sequencer = mSequencer.promote();
+    if (sequencer != 0) {
+        sequencer->onCaptureAvailable(imgBuffer.timestamp, captureBuffer, !captureSuccess);
+    }
+
+    return OK;
+}
+
+/*
+ * JPEG FILE FORMAT OVERVIEW.
+ * http://www.jpeg.org/public/jfif.pdf
+ * (JPEG is the image compression algorithm, actual file format is called JFIF)
+ *
+ * "Markers" are 2-byte patterns used to distinguish parts of JFIF files.  The
+ * first byte is always 0xFF, and the second byte is between 0x01 and 0xFE
+ * (inclusive).  Because every marker begins with the same byte, they are
+ * referred to by the second byte's value.
+ *
+ * JFIF files all begin with the Start of Image (SOI) marker, which is 0xD8.
+ * Following it, "segment" sections begin with other markers, followed by a
+ * 2-byte length (in network byte order), then the segment data.
+ *
+ * For our purposes we will ignore the data, and just use the length to skip to
+ * the next segment.  This is necessary because the data inside segments are
+ * allowed to contain the End of Image marker (0xFF 0xD9), preventing us from
+ * naievely scanning until the end.
+ *
+ * After all the segments are processed, the jpeg compressed image stream begins.
+ * This can be considered an opaque format with one requirement: all 0xFF bytes
+ * in this stream must be followed with a 0x00 byte.  This prevents any of the
+ * image data to be interpreted as a segment.  The only exception to this is at
+ * the end of the image stream there is an End of Image (EOI) marker, which is
+ * 0xFF followed by a non-zero (0xD9) byte.
+ */
+
+const uint8_t MARK = 0xFF; // First byte of marker
+const uint8_t SOI = 0xD8; // Start of Image
+const uint8_t EOI = 0xD9; // End of Image
+const size_t MARKER_LENGTH = 2; // length of a marker
+
+#pragma pack(push)
+#pragma pack(1)
+typedef struct segment {
+    uint8_t marker[MARKER_LENGTH];
+    uint16_t length;
+} segment_t;
+#pragma pack(pop)
+
+/* HELPER FUNCTIONS */
+
+// check for Start of Image marker
+bool checkJpegStart(uint8_t* buf) {
+    return buf[0] == MARK && buf[1] == SOI;
+}
+// check for End of Image marker
+bool checkJpegEnd(uint8_t *buf) {
+    return buf[0] == MARK && buf[1] == EOI;
+}
+// check for arbitrary marker, returns marker type (second byte)
+// returns 0 if no marker found. Note: 0x00 is not a valid marker type
+uint8_t checkJpegMarker(uint8_t *buf) {
+    if (buf[0] == MARK && buf[1] > 0 && buf[1] < 0xFF) {
+        return buf[1];
+    }
+    return 0;
+}
+
+// Return the size of the JPEG, 0 indicates failure
+size_t JpegProcessor::findJpegSize(uint8_t* jpegBuffer, size_t maxSize) {
+    size_t size;
+
+    // First check for JPEG transport header at the end of the buffer
+    uint8_t *header = jpegBuffer + (maxSize - sizeof(struct camera2_jpeg_blob));
+    struct camera2_jpeg_blob *blob = (struct camera2_jpeg_blob*)(header);
+    if (blob->jpeg_blob_id == CAMERA2_JPEG_BLOB_ID) {
+        size = blob->jpeg_size;
+        if (size > 0 && size <= maxSize - sizeof(struct camera2_jpeg_blob)) {
+            // Verify SOI and EOI markers
+            size_t offset = size - MARKER_LENGTH;
+            uint8_t *end = jpegBuffer + offset;
+            if (checkJpegStart(jpegBuffer) && checkJpegEnd(end)) {
+                ALOGV("Found JPEG transport header, img size %zu", size);
+                return size;
+            } else {
+                ALOGW("Found JPEG transport header with bad Image Start/End");
+            }
+        } else {
+            ALOGW("Found JPEG transport header with bad size %zu", size);
+        }
+    }
+
+    // Check Start of Image
+    if ( !checkJpegStart(jpegBuffer) ) {
+        ALOGE("Could not find start of JPEG marker");
+        return 0;
+    }
+
+    // Read JFIF segment markers, skip over segment data
+    size = 0;
+    while (size <= maxSize - MARKER_LENGTH) {
+        segment_t *segment = (segment_t*)(jpegBuffer + size);
+        uint8_t type = checkJpegMarker(segment->marker);
+        if (type == 0) { // invalid marker, no more segments, begin JPEG data
+            ALOGV("JPEG stream found beginning at offset %zu", size);
+            break;
+        }
+        if (type == EOI || size > maxSize - sizeof(segment_t)) {
+            ALOGE("Got premature End before JPEG data, offset %zu", size);
+            return 0;
+        }
+        size_t length = ntohs(segment->length);
+        ALOGV("JFIF Segment, type %x length %zx", type, length);
+        size += length + MARKER_LENGTH;
+    }
+
+    // Find End of Image
+    // Scan JPEG buffer until End of Image (EOI)
+    bool foundEnd = false;
+    for ( ; size <= maxSize - MARKER_LENGTH; size++) {
+        if ( checkJpegEnd(jpegBuffer + size) ) {
+            foundEnd = true;
+            size += MARKER_LENGTH;
+            break;
+        }
+    }
+    if (!foundEnd) {
+        ALOGE("Could not find end of JPEG marker");
+        return 0;
+    }
+
+    if (size > maxSize) {
+        ALOGW("JPEG size %zu too large, reducing to maxSize %zu", size, maxSize);
+        size = maxSize;
+    }
+    ALOGV("Final JPEG size %zu", size);
+    return size;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.h b/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.h
new file mode 100644
index 000000000..7187ad94a
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/JpegProcessor.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_JPEGPROCESSOR_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_JPEGPROCESSOR_H
+
+#include <utils/Thread.h>
+#include <utils/String16.h>
+#include <utils/Vector.h>
+#include <utils/Mutex.h>
+#include <utils/Condition.h>
+#include <gui/CpuConsumer.h>
+
+#include "camera/CameraMetadata.h"
+
+namespace android {
+
+class Camera2Client;
+class CameraDeviceBase;
+class MemoryHeapBase;
+
+namespace camera2 {
+
+class CaptureSequencer;
+struct Parameters;
+
+/***
+ * Still image capture output image processing
+ */
+class JpegProcessor:
+            public Thread, public CpuConsumer::FrameAvailableListener,
+            public camera3::Camera3StreamBufferListener {
+  public:
+    JpegProcessor(sp<Camera2Client> client, wp<CaptureSequencer> sequencer);
+    ~JpegProcessor();
+
+    // CpuConsumer listener implementation
+    void onFrameAvailable(const BufferItem& item);
+
+    // Camera3StreamBufferListener implementation
+    void onBufferAcquired(const BufferInfo& bufferInfo) override;
+    void onBufferReleased(const BufferInfo& bufferInfo) override;
+
+    status_t updateStream(const Parameters &params);
+    status_t deleteStream();
+    int getStreamId() const;
+
+    void dump(int fd, const Vector<String16>& args) const;
+  private:
+    static const nsecs_t kWaitDuration = 10000000; // 10 ms
+    wp<CameraDeviceBase> mDevice;
+    wp<CaptureSequencer> mSequencer;
+    int mId;
+
+    mutable Mutex mInputMutex;
+    bool mCaptureDone;
+    bool mCaptureSuccess;
+    Condition mCaptureDoneSignal;
+
+    enum {
+        NO_STREAM = -1
+    };
+
+    int mCaptureStreamId;
+    sp<CpuConsumer>    mCaptureConsumer;
+    sp<Surface>        mCaptureWindow;
+    sp<MemoryHeapBase> mCaptureHeap;
+
+    virtual bool threadLoop();
+
+    status_t processNewCapture(bool captureSuccess);
+    size_t findJpegSize(uint8_t* jpegBuffer, size_t maxSize);
+
+};
+
+
+}; //namespace camera2
+}; //namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/Parameters.cpp b/services/camera/libcameraservice/api1/qticlient2/Parameters.cpp
new file mode 100644
index 000000000..1852de8e3
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/Parameters.cpp
@@ -0,0 +1,3177 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-Parameters"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+// #define LOG_NDEBUG 0
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <utils/Vector.h>
+#include <utils/SortedVector.h>
+
+#include <math.h>
+#include <stdlib.h>
+#include <cutils/properties.h>
+
+#include "Parameters.h"
+#include "system/camera.h"
+#include "hardware/camera_common.h"
+#include <android/hardware/ICamera.h>
+#include <media/MediaProfiles.h>
+#include <media/mediarecorder.h>
+
+namespace android {
+namespace camera2 {
+
+Parameters::Parameters(int cameraId,
+        int cameraFacing) :
+        cameraId(cameraId),
+        cameraFacing(cameraFacing),
+        info(NULL) {
+        qtiParams = new QTIParameters();
+}
+
+Parameters::~Parameters() {
+}
+
+status_t Parameters::initialize(const CameraMetadata *info, int deviceVersion,
+        sp<CameraProviderManager> manager, sp<CameraDeviceBase> mDevice) {
+    status_t res;
+
+    if (info->entryCount() == 0) {
+        ALOGE("%s: No static information provided!", __FUNCTION__);
+        return BAD_VALUE;
+    }
+    Parameters::info = info;
+    mDeviceVersion = deviceVersion;
+
+    res = buildFastInfo();
+    if (res != OK) return res;
+
+    res = buildQuirks();
+    if (res != OK) return res;
+
+    const Size MAX_PREVIEW_SIZE = { MAX_PREVIEW_WIDTH, MAX_PREVIEW_HEIGHT };
+    // Treat the H.264 max size as the max supported video size.
+    MediaProfiles *videoEncoderProfiles = MediaProfiles::getInstance();
+    Vector<video_encoder> encoders = videoEncoderProfiles->getVideoEncoders();
+    int32_t maxVideoWidth = 0;
+    int32_t maxVideoHeight = 0;
+    for (size_t i = 0; i < encoders.size(); i++) {
+        int width = videoEncoderProfiles->getVideoEncoderParamByName(
+                "enc.vid.width.max", encoders[i]);
+        int height = videoEncoderProfiles->getVideoEncoderParamByName(
+                "enc.vid.height.max", encoders[i]);
+        // Treat width/height separately here to handle the case where different
+        // profile might report max size of different aspect ratio
+        if (width > maxVideoWidth) {
+            maxVideoWidth = width;
+        }
+        if (height > maxVideoHeight) {
+            maxVideoHeight = height;
+        }
+    }
+    // This is just an upper bound and may not be an actually valid video size
+    const Size VIDEO_SIZE_UPPER_BOUND = {maxVideoWidth, maxVideoHeight};
+
+    res = getFilteredSizes(MAX_PREVIEW_SIZE, &availablePreviewSizes);
+    if (res != OK) return res;
+    res = getFilteredSizes(VIDEO_SIZE_UPPER_BOUND, &availableVideoSizes);
+    if (res != OK) return res;
+
+    // Select initial preview and video size that's under the initial bound and
+    // on the list of both preview and recording sizes
+    previewWidth = 0;
+    previewHeight = 0;
+    for (size_t i = 0 ; i < availablePreviewSizes.size(); i++) {
+        int newWidth = availablePreviewSizes[i].width;
+        int newHeight = availablePreviewSizes[i].height;
+        if (newWidth >= previewWidth && newHeight >= previewHeight &&
+                newWidth <= MAX_INITIAL_PREVIEW_WIDTH &&
+                newHeight <= MAX_INITIAL_PREVIEW_HEIGHT) {
+            for (size_t j = 0; j < availableVideoSizes.size(); j++) {
+                if (availableVideoSizes[j].width == newWidth &&
+                        availableVideoSizes[j].height == newHeight) {
+                    previewWidth = newWidth;
+                    previewHeight = newHeight;
+                }
+            }
+        }
+    }
+    if (previewWidth == 0) {
+        ALOGE("%s: No initial preview size can be found!", __FUNCTION__);
+        return BAD_VALUE;
+    }
+    videoWidth = previewWidth;
+    videoHeight = previewHeight;
+
+    params.setPreviewSize(previewWidth, previewHeight);
+    params.setVideoSize(videoWidth, videoHeight);
+    params.set(CameraParameters::KEY_PREFERRED_PREVIEW_SIZE_FOR_VIDEO,
+            String8::format("%dx%d",
+                    previewWidth, previewHeight));
+    {
+        String8 supportedPreviewSizes;
+        for (size_t i = 0; i < availablePreviewSizes.size(); i++) {
+            if (i != 0) supportedPreviewSizes += ",";
+            supportedPreviewSizes += String8::format("%dx%d",
+                    availablePreviewSizes[i].width,
+                    availablePreviewSizes[i].height);
+        }
+        ALOGV("Supported preview sizes are: %s", supportedPreviewSizes.string());
+        params.set(CameraParameters::KEY_SUPPORTED_PREVIEW_SIZES,
+                supportedPreviewSizes);
+
+        String8 supportedVideoSizes;
+        for (size_t i = 0; i < availableVideoSizes.size(); i++) {
+            if (i != 0) supportedVideoSizes += ",";
+            supportedVideoSizes += String8::format("%dx%d",
+                    availableVideoSizes[i].width,
+                    availableVideoSizes[i].height);
+        }
+        ALOGV("Supported video sizes are: %s", supportedVideoSizes.string());
+        params.set(CameraParameters::KEY_SUPPORTED_VIDEO_SIZES,
+                supportedVideoSizes);
+    }
+
+    camera_metadata_ro_entry_t availableFpsRanges =
+        staticInfo(ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES, 2);
+    if (!availableFpsRanges.count) return NO_INIT;
+
+    previewFormat = HAL_PIXEL_FORMAT_YCrCb_420_SP;
+    params.set(CameraParameters::KEY_PREVIEW_FORMAT,
+            formatEnumToString(previewFormat)); // NV21
+
+    previewTransform = degToTransform(0,
+            cameraFacing == CAMERA_FACING_FRONT);
+
+    {
+        String8 supportedPreviewFormats;
+        SortedVector<int32_t> outputFormats = getAvailableOutputFormats();
+        bool addComma = false;
+        for (size_t i=0; i < outputFormats.size(); i++) {
+            if (addComma) supportedPreviewFormats += ",";
+            addComma = true;
+            switch (outputFormats[i]) {
+            case HAL_PIXEL_FORMAT_YCbCr_422_SP:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV422SP;
+                break;
+            case HAL_PIXEL_FORMAT_YCrCb_420_SP:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV420SP;
+                break;
+            case HAL_PIXEL_FORMAT_YCbCr_422_I:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV422I;
+                break;
+            case HAL_PIXEL_FORMAT_YV12:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV420P;
+                break;
+            case HAL_PIXEL_FORMAT_RGB_565:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_RGB565;
+                break;
+            case HAL_PIXEL_FORMAT_RGBA_8888:
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_RGBA8888;
+                break;
+            case HAL_PIXEL_FORMAT_YCbCr_420_888:
+                // Flexible YUV allows both YV12 and NV21
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV420P;
+                supportedPreviewFormats += ",";
+                supportedPreviewFormats +=
+                    CameraParameters::PIXEL_FORMAT_YUV420SP;
+                break;
+            // Not advertizing JPEG, RAW16, etc, for preview formats
+            case HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED:
+            case HAL_PIXEL_FORMAT_RAW16:
+            case HAL_PIXEL_FORMAT_BLOB:
+                addComma = false;
+                break;
+
+            default:
+                ALOGW("%s: Camera %d: Unknown preview format: %x",
+                        __FUNCTION__, cameraId, outputFormats[i]);
+                addComma = false;
+                break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_PREVIEW_FORMATS,
+                supportedPreviewFormats);
+    }
+
+    previewFpsRange[0] = fastInfo.bestStillCaptureFpsRange[0];
+    previewFpsRange[1] = fastInfo.bestStillCaptureFpsRange[1];
+
+    // PREVIEW_FRAME_RATE / SUPPORTED_PREVIEW_FRAME_RATES are deprecated, but
+    // still have to do something sane for them
+
+    // NOTE: Not scaled like FPS range values are.
+    int previewFps = fpsFromRange(previewFpsRange[0], previewFpsRange[1]);
+    params.set(CameraParameters::KEY_PREVIEW_FRAME_RATE,
+            previewFps);
+
+    // PREVIEW_FPS_RANGE
+    // -- Order matters. Set range after single value to so that a roundtrip
+    //    of setParameters(getParameters()) would keep the FPS range in higher
+    //    order.
+    params.set(CameraParameters::KEY_PREVIEW_FPS_RANGE,
+            String8::format("%d,%d",
+                    previewFpsRange[0] * kFpsToApiScale,
+                    previewFpsRange[1] * kFpsToApiScale));
+
+    {
+        String8 supportedPreviewFpsRange;
+        for (size_t i=0; i < availableFpsRanges.count; i += 2) {
+            if (!isFpsSupported(availablePreviewSizes,
+                HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED, availableFpsRanges.data.i32[i+1])) {
+                continue;
+            }
+            if (i != 0) supportedPreviewFpsRange += ",";
+            supportedPreviewFpsRange += String8::format("(%d,%d)",
+                    availableFpsRanges.data.i32[i] * kFpsToApiScale,
+                    availableFpsRanges.data.i32[i+1] * kFpsToApiScale);
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_PREVIEW_FPS_RANGE,
+                supportedPreviewFpsRange);
+    }
+
+    {
+        SortedVector<int32_t> sortedPreviewFrameRates;
+
+        String8 supportedPreviewFrameRates;
+        for (size_t i=0; i < availableFpsRanges.count; i += 2) {
+            // from the [min, max] fps range use the max value
+            int fps = fpsFromRange(availableFpsRanges.data.i32[i],
+                                   availableFpsRanges.data.i32[i+1]);
+            if (!isFpsSupported(availablePreviewSizes,
+                    HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED, fps)) {
+                continue;
+            }
+            // de-dupe frame rates
+            if (sortedPreviewFrameRates.indexOf(fps) == NAME_NOT_FOUND) {
+                sortedPreviewFrameRates.add(fps);
+            }
+            else {
+                continue;
+            }
+
+            if (sortedPreviewFrameRates.size() > 1) {
+                supportedPreviewFrameRates += ",";
+            }
+
+            supportedPreviewFrameRates += String8::format("%d",
+                    fps);
+
+            ALOGV("%s: Supported preview frame rates: %s",
+                    __FUNCTION__, supportedPreviewFrameRates.string());
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_PREVIEW_FRAME_RATES,
+                supportedPreviewFrameRates);
+    }
+
+    Vector<Size> availableJpegSizes = getAvailableJpegSizes();
+    if (!availableJpegSizes.size()) return NO_INIT;
+
+    // TODO: Pick maximum
+    pictureWidth = availableJpegSizes[0].width;
+    pictureHeight = availableJpegSizes[0].height;
+    pictureWidthLastSet = pictureWidth;
+    pictureHeightLastSet = pictureHeight;
+    pictureSizeOverriden = false;
+
+    params.setPictureSize(pictureWidth,
+            pictureHeight);
+
+    {
+        String8 supportedPictureSizes;
+        for (size_t i=0; i < availableJpegSizes.size(); i++) {
+            if (i != 0) supportedPictureSizes += ",";
+            supportedPictureSizes += String8::format("%dx%d",
+                    availableJpegSizes[i].width,
+                    availableJpegSizes[i].height);
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_PICTURE_SIZES,
+                supportedPictureSizes);
+    }
+
+    params.setPictureFormat(CameraParameters::PIXEL_FORMAT_JPEG);
+    params.set(CameraParameters::KEY_SUPPORTED_PICTURE_FORMATS,
+            CameraParameters::PIXEL_FORMAT_JPEG);
+
+    camera_metadata_ro_entry_t availableJpegThumbnailSizes =
+        staticInfo(ANDROID_JPEG_AVAILABLE_THUMBNAIL_SIZES, 4);
+    if (!availableJpegThumbnailSizes.count) return NO_INIT;
+
+    // Pick the largest thumbnail size that matches still image aspect ratio.
+    ALOG_ASSERT(pictureWidth > 0 && pictureHeight > 0,
+            "Invalid picture size, %d x %d", pictureWidth, pictureHeight);
+    float picAspectRatio = static_cast<float>(pictureWidth) / pictureHeight;
+    Size thumbnailSize =
+            getMaxSizeForRatio(
+                    picAspectRatio,
+                    &availableJpegThumbnailSizes.data.i32[0],
+                    availableJpegThumbnailSizes.count);
+    jpegThumbSize[0] = thumbnailSize.width;
+    jpegThumbSize[1] = thumbnailSize.height;
+
+    params.set(CameraParameters::KEY_JPEG_THUMBNAIL_WIDTH,
+            jpegThumbSize[0]);
+    params.set(CameraParameters::KEY_JPEG_THUMBNAIL_HEIGHT,
+            jpegThumbSize[1]);
+
+    {
+        String8 supportedJpegThumbSizes;
+        for (size_t i=0; i < availableJpegThumbnailSizes.count; i += 2) {
+            if (i != 0) supportedJpegThumbSizes += ",";
+            supportedJpegThumbSizes += String8::format("%dx%d",
+                    availableJpegThumbnailSizes.data.i32[i],
+                    availableJpegThumbnailSizes.data.i32[i+1]);
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_JPEG_THUMBNAIL_SIZES,
+                supportedJpegThumbSizes);
+    }
+
+    jpegThumbQuality = 90;
+    params.set(CameraParameters::KEY_JPEG_THUMBNAIL_QUALITY,
+            jpegThumbQuality);
+    jpegQuality = 90;
+    params.set(CameraParameters::KEY_JPEG_QUALITY,
+            jpegQuality);
+    jpegRotation = 0;
+    params.set(CameraParameters::KEY_ROTATION,
+            jpegRotation);
+
+    gpsEnabled = false;
+    gpsCoordinates[0] = 0.0;
+    gpsCoordinates[1] = 0.0;
+    gpsCoordinates[2] = 0.0;
+    gpsTimestamp = 0;
+    gpsProcessingMethod = "unknown";
+    // GPS fields in CameraParameters are not set by implementation
+
+    wbMode = ANDROID_CONTROL_AWB_MODE_AUTO;
+    params.set(CameraParameters::KEY_WHITE_BALANCE,
+            CameraParameters::WHITE_BALANCE_AUTO);
+
+    camera_metadata_ro_entry_t availableWhiteBalanceModes =
+        staticInfo(ANDROID_CONTROL_AWB_AVAILABLE_MODES, 0, 0, false);
+    if (!availableWhiteBalanceModes.count) {
+        params.set(CameraParameters::KEY_SUPPORTED_WHITE_BALANCE,
+                CameraParameters::WHITE_BALANCE_AUTO);
+    } else {
+        String8 supportedWhiteBalance;
+        bool addComma = false;
+        for (size_t i=0; i < availableWhiteBalanceModes.count; i++) {
+            if (addComma) supportedWhiteBalance += ",";
+            addComma = true;
+            switch (availableWhiteBalanceModes.data.u8[i]) {
+            case ANDROID_CONTROL_AWB_MODE_AUTO:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_AUTO;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_INCANDESCENT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_INCANDESCENT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_FLUORESCENT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_FLUORESCENT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_WARM_FLUORESCENT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_WARM_FLUORESCENT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_DAYLIGHT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_DAYLIGHT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_CLOUDY_DAYLIGHT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_CLOUDY_DAYLIGHT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_TWILIGHT:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_TWILIGHT;
+                break;
+            case ANDROID_CONTROL_AWB_MODE_SHADE:
+                supportedWhiteBalance +=
+                    CameraParameters::WHITE_BALANCE_SHADE;
+                break;
+            // Skipping values not mappable to v1 API
+            case ANDROID_CONTROL_AWB_MODE_OFF:
+                addComma = false;
+                break;
+            default:
+                ALOGW("%s: Camera %d: Unknown white balance value: %d",
+                        __FUNCTION__, cameraId,
+                        availableWhiteBalanceModes.data.u8[i]);
+                addComma = false;
+                break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_WHITE_BALANCE,
+                supportedWhiteBalance);
+    }
+
+    effectMode = ANDROID_CONTROL_EFFECT_MODE_OFF;
+    params.set(CameraParameters::KEY_EFFECT,
+            CameraParameters::EFFECT_NONE);
+
+    camera_metadata_ro_entry_t availableEffects =
+        staticInfo(ANDROID_CONTROL_AVAILABLE_EFFECTS, 0, 0, false);
+    if (!availableEffects.count) {
+        params.set(CameraParameters::KEY_SUPPORTED_EFFECTS,
+                CameraParameters::EFFECT_NONE);
+    } else {
+        String8 supportedEffects;
+        bool addComma = false;
+        for (size_t i=0; i < availableEffects.count; i++) {
+            if (addComma) supportedEffects += ",";
+            addComma = true;
+            switch (availableEffects.data.u8[i]) {
+                case ANDROID_CONTROL_EFFECT_MODE_OFF:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_NONE;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_MONO:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_MONO;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_NEGATIVE:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_NEGATIVE;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_SOLARIZE:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_SOLARIZE;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_SEPIA:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_SEPIA;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_POSTERIZE:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_POSTERIZE;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_WHITEBOARD:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_WHITEBOARD;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_BLACKBOARD:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_BLACKBOARD;
+                    break;
+                case ANDROID_CONTROL_EFFECT_MODE_AQUA:
+                    supportedEffects +=
+                        CameraParameters::EFFECT_AQUA;
+                    break;
+                default:
+                    ALOGW("%s: Camera %d: Unknown effect value: %d",
+                        __FUNCTION__, cameraId, availableEffects.data.u8[i]);
+                    addComma = false;
+                    break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_EFFECTS, supportedEffects);
+    }
+
+    antibandingMode = ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO;
+    params.set(CameraParameters::KEY_ANTIBANDING,
+            CameraParameters::ANTIBANDING_AUTO);
+
+    camera_metadata_ro_entry_t availableAntibandingModes =
+        staticInfo(ANDROID_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES, 0, 0, false);
+    if (!availableAntibandingModes.count) {
+        params.set(CameraParameters::KEY_SUPPORTED_ANTIBANDING,
+                CameraParameters::ANTIBANDING_OFF);
+    } else {
+        String8 supportedAntibanding;
+        bool addComma = false;
+        for (size_t i=0; i < availableAntibandingModes.count; i++) {
+            if (addComma) supportedAntibanding += ",";
+            addComma = true;
+            switch (availableAntibandingModes.data.u8[i]) {
+                case ANDROID_CONTROL_AE_ANTIBANDING_MODE_OFF:
+                    supportedAntibanding +=
+                        CameraParameters::ANTIBANDING_OFF;
+                    break;
+                case ANDROID_CONTROL_AE_ANTIBANDING_MODE_50HZ:
+                    supportedAntibanding +=
+                        CameraParameters::ANTIBANDING_50HZ;
+                    break;
+                case ANDROID_CONTROL_AE_ANTIBANDING_MODE_60HZ:
+                    supportedAntibanding +=
+                        CameraParameters::ANTIBANDING_60HZ;
+                    break;
+                case ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO:
+                    supportedAntibanding +=
+                        CameraParameters::ANTIBANDING_AUTO;
+                    break;
+                default:
+                    ALOGW("%s: Camera %d: Unknown antibanding value: %d",
+                        __FUNCTION__, cameraId,
+                            availableAntibandingModes.data.u8[i]);
+                    addComma = false;
+                    break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_ANTIBANDING,
+                supportedAntibanding);
+    }
+
+    sceneMode = ANDROID_CONTROL_SCENE_MODE_DISABLED;
+    params.set(CameraParameters::KEY_SCENE_MODE,
+            CameraParameters::SCENE_MODE_AUTO);
+
+    camera_metadata_ro_entry_t availableSceneModes =
+        staticInfo(ANDROID_CONTROL_AVAILABLE_SCENE_MODES, 0, 0, false);
+    if (!availableSceneModes.count) {
+        params.remove(CameraParameters::KEY_SCENE_MODE);
+    } else {
+        String8 supportedSceneModes(CameraParameters::SCENE_MODE_AUTO);
+        bool addComma = true;
+        bool noSceneModes = false;
+        for (size_t i=0; i < availableSceneModes.count; i++) {
+            if (addComma) supportedSceneModes += ",";
+            addComma = true;
+            switch (availableSceneModes.data.u8[i]) {
+                case ANDROID_CONTROL_SCENE_MODE_DISABLED:
+                    noSceneModes = true;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_FACE_PRIORITY:
+                    // Not in old API
+                    addComma = false;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_ACTION:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_ACTION;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_PORTRAIT:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_PORTRAIT;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_LANDSCAPE:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_LANDSCAPE;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_NIGHT:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_NIGHT;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_NIGHT_PORTRAIT:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_NIGHT_PORTRAIT;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_THEATRE:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_THEATRE;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_BEACH:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_BEACH;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_SNOW:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_SNOW;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_SUNSET:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_SUNSET;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_STEADYPHOTO:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_STEADYPHOTO;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_FIREWORKS:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_FIREWORKS;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_SPORTS:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_SPORTS;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_PARTY:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_PARTY;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_CANDLELIGHT:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_CANDLELIGHT;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_BARCODE:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_BARCODE;
+                    break;
+                case ANDROID_CONTROL_SCENE_MODE_HDR:
+                    supportedSceneModes +=
+                        CameraParameters::SCENE_MODE_HDR;
+                    break;
+                default:
+                    ALOGW("%s: Camera %d: Unknown scene mode value: %d",
+                        __FUNCTION__, cameraId,
+                            availableSceneModes.data.u8[i]);
+                    addComma = false;
+                    break;
+            }
+        }
+        if (!noSceneModes) {
+            params.set(CameraParameters::KEY_SUPPORTED_SCENE_MODES,
+                    supportedSceneModes);
+        } else {
+            params.remove(CameraParameters::KEY_SCENE_MODE);
+        }
+    }
+
+    bool isFlashAvailable = false;
+    camera_metadata_ro_entry_t flashAvailable =
+        staticInfo(ANDROID_FLASH_INFO_AVAILABLE, 0, 1, false);
+    if (flashAvailable.count) {
+        isFlashAvailable = flashAvailable.data.u8[0];
+    }
+
+    camera_metadata_ro_entry_t availableAeModes =
+        staticInfo(ANDROID_CONTROL_AE_AVAILABLE_MODES, 0, 0, false);
+
+    flashMode = Parameters::FLASH_MODE_OFF;
+    if (isFlashAvailable) {
+        params.set(CameraParameters::KEY_FLASH_MODE,
+                CameraParameters::FLASH_MODE_OFF);
+
+        String8 supportedFlashModes(CameraParameters::FLASH_MODE_OFF);
+        supportedFlashModes = supportedFlashModes +
+            "," + CameraParameters::FLASH_MODE_AUTO +
+            "," + CameraParameters::FLASH_MODE_ON +
+            "," + CameraParameters::FLASH_MODE_TORCH;
+        for (size_t i=0; i < availableAeModes.count; i++) {
+            if (availableAeModes.data.u8[i] ==
+                    ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE) {
+                supportedFlashModes = supportedFlashModes + "," +
+                    CameraParameters::FLASH_MODE_RED_EYE;
+                break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_FLASH_MODES,
+                supportedFlashModes);
+    } else {
+        // No flash means null flash mode and supported flash modes keys, so
+        // remove them just to be safe
+        params.remove(CameraParameters::KEY_FLASH_MODE);
+        params.remove(CameraParameters::KEY_SUPPORTED_FLASH_MODES);
+    }
+
+    camera_metadata_ro_entry_t minFocusDistance =
+        staticInfo(ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE, 0, 1, false);
+
+    camera_metadata_ro_entry_t availableAfModes =
+        staticInfo(ANDROID_CONTROL_AF_AVAILABLE_MODES, 0, 0, false);
+
+    if (!minFocusDistance.count || minFocusDistance.data.f[0] == 0) {
+        // Fixed-focus lens
+        focusMode = Parameters::FOCUS_MODE_FIXED;
+        params.set(CameraParameters::KEY_FOCUS_MODE,
+                CameraParameters::FOCUS_MODE_FIXED);
+        params.set(CameraParameters::KEY_SUPPORTED_FOCUS_MODES,
+                CameraParameters::FOCUS_MODE_FIXED);
+    } else {
+        focusMode = Parameters::FOCUS_MODE_AUTO;
+        params.set(CameraParameters::KEY_FOCUS_MODE,
+                CameraParameters::FOCUS_MODE_AUTO);
+        String8 supportedFocusModes;
+        bool addComma = false;
+        camera_metadata_ro_entry_t focusDistanceCalibration =
+            staticInfo(ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION, 0, 0, false);
+
+        if (focusDistanceCalibration.count &&
+                focusDistanceCalibration.data.u8[0] !=
+                ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED) {
+            supportedFocusModes += CameraParameters::FOCUS_MODE_INFINITY;
+            addComma = true;
+        }
+
+        for (size_t i=0; i < availableAfModes.count; i++) {
+            if (addComma) supportedFocusModes += ",";
+            addComma = true;
+            switch (availableAfModes.data.u8[i]) {
+                case ANDROID_CONTROL_AF_MODE_AUTO:
+                    supportedFocusModes +=
+                        CameraParameters::FOCUS_MODE_AUTO;
+                    break;
+                case ANDROID_CONTROL_AF_MODE_MACRO:
+                    supportedFocusModes +=
+                        CameraParameters::FOCUS_MODE_MACRO;
+                    break;
+                case ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO:
+                    supportedFocusModes +=
+                        CameraParameters::FOCUS_MODE_CONTINUOUS_VIDEO;
+                    break;
+                case ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE:
+                    supportedFocusModes +=
+                        CameraParameters::FOCUS_MODE_CONTINUOUS_PICTURE;
+                    break;
+                case ANDROID_CONTROL_AF_MODE_EDOF:
+                    supportedFocusModes +=
+                        CameraParameters::FOCUS_MODE_EDOF;
+                    break;
+                // Not supported in old API
+                case ANDROID_CONTROL_AF_MODE_OFF:
+                    addComma = false;
+                    break;
+                default:
+                    ALOGW("%s: Camera %d: Unknown AF mode value: %d",
+                        __FUNCTION__, cameraId, availableAfModes.data.u8[i]);
+                    addComma = false;
+                    break;
+            }
+        }
+        params.set(CameraParameters::KEY_SUPPORTED_FOCUS_MODES,
+                supportedFocusModes);
+    }
+    focusState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+    shadowFocusMode = FOCUS_MODE_INVALID;
+
+    camera_metadata_ro_entry_t max3aRegions = staticInfo(ANDROID_CONTROL_MAX_REGIONS,
+            Parameters::NUM_REGION, Parameters::NUM_REGION);
+    if (max3aRegions.count != Parameters::NUM_REGION) return NO_INIT;
+
+    int32_t maxNumFocusAreas = 0;
+    if (focusMode != Parameters::FOCUS_MODE_FIXED) {
+        maxNumFocusAreas = max3aRegions.data.i32[Parameters::REGION_AF];
+    }
+    params.set(CameraParameters::KEY_MAX_NUM_FOCUS_AREAS, maxNumFocusAreas);
+    params.set(CameraParameters::KEY_FOCUS_AREAS,
+            "(0,0,0,0,0)");
+    focusingAreas.clear();
+    focusingAreas.add(Parameters::Area(0,0,0,0,0));
+
+    camera_metadata_ro_entry_t availableFocalLengths =
+        staticInfo(ANDROID_LENS_INFO_AVAILABLE_FOCAL_LENGTHS, 0, 0, false);
+    if (!availableFocalLengths.count) return NO_INIT;
+
+    float minFocalLength = availableFocalLengths.data.f[0];
+    params.setFloat(CameraParameters::KEY_FOCAL_LENGTH, minFocalLength);
+
+    float horizFov, vertFov;
+    res = calculatePictureFovs(&horizFov, &vertFov);
+    if (res != OK) {
+        ALOGE("%s: Can't calculate field of views!", __FUNCTION__);
+        return res;
+    }
+
+    params.setFloat(CameraParameters::KEY_HORIZONTAL_VIEW_ANGLE, horizFov);
+    params.setFloat(CameraParameters::KEY_VERTICAL_VIEW_ANGLE, vertFov);
+
+    exposureCompensation = 0;
+    params.set(CameraParameters::KEY_EXPOSURE_COMPENSATION,
+                exposureCompensation);
+
+    camera_metadata_ro_entry_t exposureCompensationRange =
+        staticInfo(ANDROID_CONTROL_AE_COMPENSATION_RANGE, 2, 2);
+    if (!exposureCompensationRange.count) return NO_INIT;
+
+    params.set(CameraParameters::KEY_MAX_EXPOSURE_COMPENSATION,
+            exposureCompensationRange.data.i32[1]);
+    params.set(CameraParameters::KEY_MIN_EXPOSURE_COMPENSATION,
+            exposureCompensationRange.data.i32[0]);
+
+    camera_metadata_ro_entry_t exposureCompensationStep =
+        staticInfo(ANDROID_CONTROL_AE_COMPENSATION_STEP, 1, 1);
+    if (!exposureCompensationStep.count) return NO_INIT;
+
+    params.setFloat(CameraParameters::KEY_EXPOSURE_COMPENSATION_STEP,
+            (float)exposureCompensationStep.data.r[0].numerator /
+            exposureCompensationStep.data.r[0].denominator);
+
+    autoExposureLock = false;
+    autoExposureLockAvailable = false;
+    camera_metadata_ro_entry_t exposureLockAvailable =
+        staticInfo(ANDROID_CONTROL_AE_LOCK_AVAILABLE, 1, 1);
+    if ((0 < exposureLockAvailable.count) &&
+            (ANDROID_CONTROL_AE_LOCK_AVAILABLE_TRUE ==
+                    exposureLockAvailable.data.u8[0])) {
+        params.set(CameraParameters::KEY_AUTO_EXPOSURE_LOCK,
+                CameraParameters::FALSE);
+        params.set(CameraParameters::KEY_AUTO_EXPOSURE_LOCK_SUPPORTED,
+                   CameraParameters::TRUE);
+        autoExposureLockAvailable = true;
+    } else {
+        params.set(CameraParameters::KEY_AUTO_EXPOSURE_LOCK_SUPPORTED,
+                   CameraParameters::FALSE);
+    }
+
+    autoWhiteBalanceLock = false;
+    autoWhiteBalanceLockAvailable = false;
+    camera_metadata_ro_entry_t whitebalanceLockAvailable =
+        staticInfo(ANDROID_CONTROL_AWB_LOCK_AVAILABLE, 1, 1);
+    if ((0 < whitebalanceLockAvailable.count) &&
+            (ANDROID_CONTROL_AWB_LOCK_AVAILABLE_TRUE ==
+                    whitebalanceLockAvailable.data.u8[0])) {
+        params.set(CameraParameters::KEY_AUTO_WHITEBALANCE_LOCK,
+                CameraParameters::FALSE);
+        params.set(CameraParameters::KEY_AUTO_WHITEBALANCE_LOCK_SUPPORTED,
+                CameraParameters::TRUE);
+        autoWhiteBalanceLockAvailable = true;
+    } else {
+        params.set(CameraParameters::KEY_AUTO_WHITEBALANCE_LOCK_SUPPORTED,
+                CameraParameters::FALSE);
+    }
+
+    meteringAreas.add(Parameters::Area(0, 0, 0, 0, 0));
+    params.set(CameraParameters::KEY_MAX_NUM_METERING_AREAS,
+            max3aRegions.data.i32[Parameters::REGION_AE]);
+    params.set(CameraParameters::KEY_METERING_AREAS,
+            "(0,0,0,0,0)");
+
+    zoom = 0;
+    zoomAvailable = false;
+    camera_metadata_ro_entry_t maxDigitalZoom =
+        staticInfo(ANDROID_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM, /*minCount*/1, /*maxCount*/1);
+    if (!maxDigitalZoom.count) return NO_INIT;
+
+    if (fabs(maxDigitalZoom.data.f[0] - 1.f) > 0.00001f) {
+        params.set(CameraParameters::KEY_ZOOM, zoom);
+        params.set(CameraParameters::KEY_MAX_ZOOM, NUM_ZOOM_STEPS - 1);
+
+        {
+            String8 zoomRatios;
+            float zoom = 1.f;
+            float zoomIncrement = (maxDigitalZoom.data.f[0] - zoom) /
+                    (NUM_ZOOM_STEPS-1);
+            bool addComma = false;
+            for (size_t i=0; i < NUM_ZOOM_STEPS; i++) {
+                if (addComma) zoomRatios += ",";
+                addComma = true;
+                zoomRatios += String8::format("%d", static_cast<int>(zoom * 100));
+                zoom += zoomIncrement;
+            }
+            params.set(CameraParameters::KEY_ZOOM_RATIOS, zoomRatios);
+        }
+
+        params.set(CameraParameters::KEY_ZOOM_SUPPORTED,
+                CameraParameters::TRUE);
+        zoomAvailable = true;
+    } else {
+        params.set(CameraParameters::KEY_ZOOM_SUPPORTED,
+                CameraParameters::FALSE);
+    }
+    params.set(CameraParameters::KEY_SMOOTH_ZOOM_SUPPORTED,
+            CameraParameters::FALSE);
+
+    params.set(CameraParameters::KEY_FOCUS_DISTANCES,
+            "Infinity,Infinity,Infinity");
+
+    params.set(CameraParameters::KEY_MAX_NUM_DETECTED_FACES_HW,
+            fastInfo.maxFaces);
+    params.set(CameraParameters::KEY_MAX_NUM_DETECTED_FACES_SW,
+            0);
+
+    params.set(CameraParameters::KEY_VIDEO_FRAME_FORMAT,
+            CameraParameters::PIXEL_FORMAT_ANDROID_OPAQUE);
+
+    recordingHint = false;
+    params.set(CameraParameters::KEY_RECORDING_HINT,
+            CameraParameters::FALSE);
+
+    params.set(CameraParameters::KEY_VIDEO_SNAPSHOT_SUPPORTED,
+            CameraParameters::TRUE);
+
+    videoStabilization = false;
+    params.set(CameraParameters::KEY_VIDEO_STABILIZATION,
+            CameraParameters::FALSE);
+
+    camera_metadata_ro_entry_t availableVideoStabilizationModes =
+        staticInfo(ANDROID_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES, 0, 0,
+                false);
+
+    if (availableVideoStabilizationModes.count > 1) {
+        params.set(CameraParameters::KEY_VIDEO_STABILIZATION_SUPPORTED,
+                CameraParameters::TRUE);
+    } else {
+        params.set(CameraParameters::KEY_VIDEO_STABILIZATION_SUPPORTED,
+                CameraParameters::FALSE);
+    }
+
+    // Set up initial state for non-Camera.Parameters state variables
+    videoFormat = HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED;
+    videoDataSpace = HAL_DATASPACE_V0_BT709;
+    videoBufferMode = hardware::ICamera::VIDEO_BUFFER_MODE_DATA_CALLBACK_YUV;
+    playShutterSound = true;
+    enableFaceDetect = false;
+
+    enableFocusMoveMessages = false;
+    afTriggerCounter = 1;
+    afStateCounter = 0;
+    currentAfTriggerId = -1;
+    afInMotion = false;
+
+    precaptureTriggerCounter = 1;
+
+    takePictureCounter = 0;
+
+    previewCallbackFlags = 0;
+    previewCallbackOneShot = false;
+    previewCallbackSurface = false;
+
+    Size maxJpegSize = getMaxSize(getAvailableJpegSizes());
+    int64_t minFrameDurationNs = getJpegStreamMinFrameDurationNs(maxJpegSize);
+
+    slowJpegMode = false;
+    if (minFrameDurationNs > kSlowJpegModeThreshold) {
+        slowJpegMode = true;
+        // Slow jpeg devices does not support video snapshot without
+        // slowing down preview.
+        // TODO: support video size video snapshot only?
+        params.set(CameraParameters::KEY_VIDEO_SNAPSHOT_SUPPORTED,
+            CameraParameters::FALSE);
+    }
+
+    isZslReprocessPresent = false;
+    camera_metadata_ro_entry_t availableCapabilities =
+        staticInfo(ANDROID_REQUEST_AVAILABLE_CAPABILITIES);
+    if (0 < availableCapabilities.count) {
+        const uint8_t *caps = availableCapabilities.data.u8;
+        for (size_t i = 0; i < availableCapabilities.count; i++) {
+            if (ANDROID_REQUEST_AVAILABLE_CAPABILITIES_PRIVATE_REPROCESSING ==
+                caps[i]) {
+                isZslReprocessPresent = true;
+                break;
+            }
+        }
+    }
+
+    if (slowJpegMode || property_get_bool("camera.disable_zsl_mode", false)) {
+        ALOGI("Camera %d: Disabling ZSL mode", cameraId);
+        allowZslMode = false;
+    } else {
+        allowZslMode = isZslReprocessPresent;
+    }
+
+    ALOGI("%s: allowZslMode: %d slowJpegMode %d", __FUNCTION__, allowZslMode, slowJpegMode);
+
+    res = qtiParams->initialize((void*)this, mDevice, manager);
+    if(res != OK)
+        return res;
+
+    state = STOPPED;
+
+    paramsFlattened = params.flatten();
+
+    return OK;
+}
+
+String8 Parameters::get() const {
+    return paramsFlattened;
+}
+
+status_t Parameters::buildFastInfo() {
+
+    camera_metadata_ro_entry_t activeArraySize =
+        staticInfo(ANDROID_SENSOR_INFO_ACTIVE_ARRAY_SIZE, 2, 4);
+    if (!activeArraySize.count) return NO_INIT;
+    int32_t arrayWidth;
+    int32_t arrayHeight;
+    if (activeArraySize.count == 2) {
+        ALOGW("%s: Camera %d: activeArraySize is missing xmin/ymin!",
+                __FUNCTION__, cameraId);
+        arrayWidth = activeArraySize.data.i32[0];
+        arrayHeight = activeArraySize.data.i32[1];
+    } else if (activeArraySize.count == 4) {
+        arrayWidth = activeArraySize.data.i32[2];
+        arrayHeight = activeArraySize.data.i32[3];
+    } else return NO_INIT;
+
+    // We'll set the target FPS range for still captures to be as wide
+    // as possible to give the HAL maximum latitude for exposure selection
+    camera_metadata_ro_entry_t availableFpsRanges =
+        staticInfo(ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES, 2);
+    if (availableFpsRanges.count < 2 || availableFpsRanges.count % 2 != 0) {
+        return NO_INIT;
+    }
+
+    // Get supported preview fps ranges, up to default maximum.
+    Vector<Size> supportedPreviewSizes;
+    Vector<FpsRange> supportedPreviewFpsRanges;
+    const Size PREVIEW_SIZE_BOUND = { MAX_PREVIEW_WIDTH, MAX_PREVIEW_HEIGHT };
+    status_t res = getFilteredSizes(PREVIEW_SIZE_BOUND, &supportedPreviewSizes);
+    if (res != OK) return res;
+    for (size_t i=0; i < availableFpsRanges.count; i += 2) {
+        if (!isFpsSupported(supportedPreviewSizes,
+                HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED, availableFpsRanges.data.i32[i+1]) ||
+                availableFpsRanges.data.i32[i+1] > MAX_DEFAULT_FPS) {
+            continue;
+        }
+        FpsRange fpsRange = {availableFpsRanges.data.i32[i], availableFpsRanges.data.i32[i+1]};
+        supportedPreviewFpsRanges.add(fpsRange);
+    }
+    if (supportedPreviewFpsRanges.size() == 0) {
+        ALOGE("Supported preview fps range is empty");
+        return NO_INIT;
+    }
+
+    int32_t bestStillCaptureFpsRange[2] = {
+        supportedPreviewFpsRanges[0].low, supportedPreviewFpsRanges[0].high
+    };
+    int32_t curRange =
+            bestStillCaptureFpsRange[1] - bestStillCaptureFpsRange[0];
+    for (size_t i = 1; i < supportedPreviewFpsRanges.size(); i ++) {
+        int32_t nextRange =
+                supportedPreviewFpsRanges[i].high -
+                supportedPreviewFpsRanges[i].low;
+        if ( (nextRange > curRange) ||       // Maximize size of FPS range first
+                (nextRange == curRange &&    // Then minimize low-end FPS
+                 bestStillCaptureFpsRange[0] > supportedPreviewFpsRanges[i].low)) {
+
+            bestStillCaptureFpsRange[0] = supportedPreviewFpsRanges[i].low;
+            bestStillCaptureFpsRange[1] = supportedPreviewFpsRanges[i].high;
+            curRange = nextRange;
+        }
+    }
+
+    camera_metadata_ro_entry_t availableFaceDetectModes =
+        staticInfo(ANDROID_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES, 0, 0,
+                false);
+
+    uint8_t bestFaceDetectMode =
+        ANDROID_STATISTICS_FACE_DETECT_MODE_OFF;
+    for (size_t i = 0 ; i < availableFaceDetectModes.count; i++) {
+        switch (availableFaceDetectModes.data.u8[i]) {
+            case ANDROID_STATISTICS_FACE_DETECT_MODE_OFF:
+                break;
+            case ANDROID_STATISTICS_FACE_DETECT_MODE_SIMPLE:
+                if (bestFaceDetectMode !=
+                        ANDROID_STATISTICS_FACE_DETECT_MODE_FULL) {
+                    bestFaceDetectMode =
+                        ANDROID_STATISTICS_FACE_DETECT_MODE_SIMPLE;
+                }
+                break;
+            case ANDROID_STATISTICS_FACE_DETECT_MODE_FULL:
+                bestFaceDetectMode =
+                    ANDROID_STATISTICS_FACE_DETECT_MODE_FULL;
+                break;
+            default:
+                ALOGE("%s: Camera %d: Unknown face detect mode %d:",
+                        __FUNCTION__, cameraId,
+                        availableFaceDetectModes.data.u8[i]);
+                return NO_INIT;
+        }
+    }
+
+    int32_t maxFaces = 0;
+    camera_metadata_ro_entry_t maxFacesDetected =
+        staticInfo(ANDROID_STATISTICS_INFO_MAX_FACE_COUNT, 0, 1, false);
+    if (maxFacesDetected.count) {
+        maxFaces = maxFacesDetected.data.i32[0];
+    }
+
+    camera_metadata_ro_entry_t availableSceneModes =
+        staticInfo(ANDROID_CONTROL_AVAILABLE_SCENE_MODES, 0, 0, false);
+    camera_metadata_ro_entry_t sceneModeOverrides =
+        staticInfo(ANDROID_CONTROL_SCENE_MODE_OVERRIDES, 0, 0, false);
+    camera_metadata_ro_entry_t minFocusDistance =
+        staticInfo(ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE, 0, 0, false);
+    bool fixedLens = minFocusDistance.count == 0 ||
+        minFocusDistance.data.f[0] == 0;
+
+    camera_metadata_ro_entry_t focusDistanceCalibration =
+            staticInfo(ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION, 0, 0,
+                    false);
+    bool canFocusInfinity = (focusDistanceCalibration.count &&
+            focusDistanceCalibration.data.u8[0] !=
+            ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED);
+
+    camera_metadata_ro_entry_t availableFocalLengths =
+        staticInfo(ANDROID_LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
+    if (!availableFocalLengths.count) return NO_INIT;
+
+    SortedVector<int32_t> availableFormats = getAvailableOutputFormats();
+    if (!availableFormats.size()) return NO_INIT;
+
+
+    if (sceneModeOverrides.count > 0) {
+        // sceneModeOverrides is defined to have 3 entries for each scene mode,
+        // which are AE, AWB, and AF override modes the HAL wants for that scene
+        // mode.
+        const size_t kModesPerSceneMode = 3;
+        if (sceneModeOverrides.count !=
+                availableSceneModes.count * kModesPerSceneMode) {
+            ALOGE("%s: Camera %d: Scene mode override list is an "
+                    "unexpected size: %zu (expected %zu)", __FUNCTION__,
+                    cameraId, sceneModeOverrides.count,
+                    availableSceneModes.count * kModesPerSceneMode);
+            return NO_INIT;
+        }
+        for (size_t i = 0; i < availableSceneModes.count; i++) {
+            DeviceInfo::OverrideModes modes;
+            uint8_t aeMode =
+                    sceneModeOverrides.data.u8[i * kModesPerSceneMode + 0];
+            switch(aeMode) {
+                case ANDROID_CONTROL_AE_MODE_ON:
+                    modes.flashMode = FLASH_MODE_OFF;
+                    break;
+                case ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH:
+                    modes.flashMode = FLASH_MODE_AUTO;
+                    break;
+                case ANDROID_CONTROL_AE_MODE_ON_ALWAYS_FLASH:
+                    modes.flashMode = FLASH_MODE_ON;
+                    break;
+                case ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE:
+                    modes.flashMode = FLASH_MODE_RED_EYE;
+                    break;
+                default:
+                    ALOGE("%s: Unknown override AE mode: %d", __FUNCTION__,
+                            aeMode);
+                    modes.flashMode = FLASH_MODE_INVALID;
+                    break;
+            }
+            modes.wbMode =
+                    sceneModeOverrides.data.u8[i * kModesPerSceneMode + 1];
+            uint8_t afMode =
+                    sceneModeOverrides.data.u8[i * kModesPerSceneMode + 2];
+            switch(afMode) {
+                case ANDROID_CONTROL_AF_MODE_OFF:
+                    if (!fixedLens && !canFocusInfinity) {
+                        ALOGE("%s: Camera %d: Scene mode override lists asks for"
+                                " fixed focus on a device with focuser but not"
+                                " calibrated for infinity focus", __FUNCTION__,
+                                cameraId);
+                        return NO_INIT;
+                    }
+                    modes.focusMode = fixedLens ?
+                            FOCUS_MODE_FIXED : FOCUS_MODE_INFINITY;
+                    break;
+                case ANDROID_CONTROL_AF_MODE_AUTO:
+                case ANDROID_CONTROL_AF_MODE_MACRO:
+                case ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO:
+                case ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE:
+                case ANDROID_CONTROL_AF_MODE_EDOF:
+                    modes.focusMode = static_cast<focusMode_t>(afMode);
+                    break;
+                default:
+                    ALOGE("%s: Unknown override AF mode: %d", __FUNCTION__,
+                            afMode);
+                    modes.focusMode = FOCUS_MODE_INVALID;
+                    break;
+            }
+            fastInfo.sceneModeOverrides.add(availableSceneModes.data.u8[i],
+                    modes);
+        }
+    }
+
+    fastInfo.arrayWidth = arrayWidth;
+    fastInfo.arrayHeight = arrayHeight;
+    fastInfo.bestStillCaptureFpsRange[0] = bestStillCaptureFpsRange[0];
+    fastInfo.bestStillCaptureFpsRange[1] = bestStillCaptureFpsRange[1];
+    fastInfo.bestFaceDetectMode = bestFaceDetectMode;
+    fastInfo.maxFaces = maxFaces;
+
+    // Find smallest (widest-angle) focal length to use as basis of still
+    // picture FOV reporting.
+    fastInfo.minFocalLength = availableFocalLengths.data.f[0];
+    for (size_t i = 1; i < availableFocalLengths.count; i++) {
+        if (fastInfo.minFocalLength > availableFocalLengths.data.f[i]) {
+            fastInfo.minFocalLength = availableFocalLengths.data.f[i];
+        }
+    }
+
+    // Check if the HAL supports HAL_PIXEL_FORMAT_YCbCr_420_888
+    fastInfo.useFlexibleYuv = false;
+    for (size_t i = 0; i < availableFormats.size(); i++) {
+        if (availableFormats[i] == HAL_PIXEL_FORMAT_YCbCr_420_888) {
+            fastInfo.useFlexibleYuv = true;
+            break;
+        }
+    }
+    ALOGV("Camera %d: Flexible YUV %s supported",
+            cameraId, fastInfo.useFlexibleYuv ? "is" : "is not");
+
+    fastInfo.maxJpegSize = getMaxSize(getAvailableJpegSizes());
+
+    return OK;
+}
+
+status_t Parameters::buildQuirks() {
+    camera_metadata_ro_entry_t entry;
+    entry = info->find(ANDROID_QUIRKS_TRIGGER_AF_WITH_AUTO);
+    quirks.triggerAfWithAuto = (entry.count != 0 && entry.data.u8[0] == 1);
+    ALOGV_IF(quirks.triggerAfWithAuto, "Camera %d: Quirk triggerAfWithAuto enabled",
+            cameraId);
+
+    entry = info->find(ANDROID_QUIRKS_USE_ZSL_FORMAT);
+    quirks.useZslFormat = (entry.count != 0 && entry.data.u8[0] == 1);
+    ALOGV_IF(quirks.useZslFormat, "Camera %d: Quirk useZslFormat enabled",
+            cameraId);
+
+    entry = info->find(ANDROID_QUIRKS_METERING_CROP_REGION);
+    quirks.meteringCropRegion = (entry.count != 0 && entry.data.u8[0] == 1);
+    ALOGV_IF(quirks.meteringCropRegion, "Camera %d: Quirk meteringCropRegion"
+                " enabled", cameraId);
+
+    entry = info->find(ANDROID_QUIRKS_USE_PARTIAL_RESULT);
+    quirks.partialResults = (entry.count != 0 && entry.data.u8[0] == 1);
+    ALOGV_IF(quirks.partialResults, "Camera %d: Quirk usePartialResult"
+                " enabled", cameraId);
+
+    return OK;
+}
+
+camera_metadata_ro_entry_t Parameters::staticInfo(uint32_t tag,
+        size_t minCount, size_t maxCount, bool required) const {
+    camera_metadata_ro_entry_t entry = info->find(tag);
+    const camera_metadata_t *metaBuffer = info->getAndLock();
+
+    if (CC_UNLIKELY( entry.count == 0 ) && required) {
+        const char* tagSection = get_local_camera_metadata_section_name(tag,
+                metaBuffer);
+        if (tagSection == NULL) tagSection = "<unknown>";
+        const char* tagName = get_local_camera_metadata_tag_name(tag,
+                metaBuffer);
+        if (tagName == NULL) tagName = "<unknown>";
+
+        ALOGE("Error finding static metadata entry '%s.%s' (%x)",
+                tagSection, tagName, tag);
+    } else if (CC_UNLIKELY(
+            (minCount != 0 && entry.count < minCount) ||
+            (maxCount != 0 && entry.count > maxCount) ) ) {
+        const char* tagSection = get_local_camera_metadata_section_name(tag,
+                metaBuffer);
+        if (tagSection == NULL) tagSection = "<unknown>";
+        const char* tagName = get_local_camera_metadata_tag_name(tag,
+                metaBuffer);
+        if (tagName == NULL) tagName = "<unknown>";
+        ALOGE("Malformed static metadata entry '%s.%s' (%x):"
+                "Expected between %zu and %zu values, but got %zu values",
+                tagSection, tagName, tag, minCount, maxCount, entry.count);
+    }
+    info->unlock(metaBuffer);
+
+    return entry;
+}
+
+status_t Parameters::set(const String8& paramString) {
+    status_t res;
+
+    CameraParameters2 newParams(paramString);
+
+    // TODO: Currently ignoring any changes to supposedly read-only parameters
+    // such as supported preview sizes, etc. Should probably produce an error if
+    // they're changed.
+
+    /** Extract and verify new parameters */
+
+    size_t i;
+
+    Parameters validatedParams(*this);
+
+    // PREVIEW_SIZE
+    newParams.getPreviewSize(&validatedParams.previewWidth,
+            &validatedParams.previewHeight);
+
+    if (validatedParams.previewWidth != previewWidth ||
+            validatedParams.previewHeight != previewHeight) {
+        if (state >= PREVIEW) {
+            ALOGE("%s: Preview size cannot be updated when preview "
+                    "is active! (Currently %d x %d, requested %d x %d",
+                    __FUNCTION__,
+                    previewWidth, previewHeight,
+                    validatedParams.previewWidth, validatedParams.previewHeight);
+            return BAD_VALUE;
+        }
+        for (i = 0; i < availablePreviewSizes.size(); i++) {
+            if ((availablePreviewSizes[i].width ==
+                    validatedParams.previewWidth) &&
+                (availablePreviewSizes[i].height ==
+                    validatedParams.previewHeight)) break;
+        }
+        if (i == availablePreviewSizes.size()) {
+            ALOGE("%s: Requested preview size %d x %d is not supported",
+                    __FUNCTION__, validatedParams.previewWidth,
+                    validatedParams.previewHeight);
+            return BAD_VALUE;
+        }
+    }
+
+    // RECORDING_HINT (always supported)
+    validatedParams.recordingHint = boolFromString(
+        newParams.get(CameraParameters::KEY_RECORDING_HINT) );
+    IF_ALOGV() { // Avoid unused variable warning
+        bool recordingHintChanged =
+                validatedParams.recordingHint != recordingHint;
+        if (recordingHintChanged) {
+            ALOGV("%s: Recording hint changed to %d",
+                  __FUNCTION__, validatedParams.recordingHint);
+        }
+    }
+
+    // PREVIEW_FPS_RANGE
+
+    /**
+     * Use the single FPS value if it was set later than the range.
+     * Otherwise, use the range value.
+     */
+    bool fpsUseSingleValue;
+    {
+        const char *fpsRange, *fpsSingle;
+
+        fpsRange = newParams.get(CameraParameters::KEY_PREVIEW_FRAME_RATE);
+        fpsSingle = newParams.get(CameraParameters::KEY_PREVIEW_FPS_RANGE);
+
+        /**
+         * Pick either the range or the single key if only one was set.
+         *
+         * If both are set, pick the one that has greater set order.
+         */
+        if (fpsRange == NULL && fpsSingle == NULL) {
+            ALOGE("%s: FPS was not set. One of %s or %s must be set.",
+                  __FUNCTION__, CameraParameters::KEY_PREVIEW_FRAME_RATE,
+                  CameraParameters::KEY_PREVIEW_FPS_RANGE);
+            return BAD_VALUE;
+        } else if (fpsRange == NULL) {
+            fpsUseSingleValue = true;
+            ALOGV("%s: FPS range not set, using FPS single value",
+                  __FUNCTION__);
+        } else if (fpsSingle == NULL) {
+            fpsUseSingleValue = false;
+            ALOGV("%s: FPS single not set, using FPS range value",
+                  __FUNCTION__);
+        } else {
+            int fpsKeyOrder;
+            res = newParams.compareSetOrder(
+                    CameraParameters::KEY_PREVIEW_FRAME_RATE,
+                    CameraParameters::KEY_PREVIEW_FPS_RANGE,
+                    &fpsKeyOrder);
+            LOG_ALWAYS_FATAL_IF(res != OK, "Impossibly bad FPS keys");
+
+            fpsUseSingleValue = (fpsKeyOrder > 0);
+
+        }
+
+        ALOGV("%s: Preview FPS value is used from '%s'",
+              __FUNCTION__, fpsUseSingleValue ? "single" : "range");
+    }
+    newParams.getPreviewFpsRange(&validatedParams.previewFpsRange[0],
+            &validatedParams.previewFpsRange[1]);
+
+    validatedParams.previewFpsRange[0] /= kFpsToApiScale;
+    validatedParams.previewFpsRange[1] /= kFpsToApiScale;
+
+    // Ignore the FPS range if the FPS single has higher precedence
+    if (!fpsUseSingleValue) {
+        ALOGV("%s: Preview FPS range (%d, %d)", __FUNCTION__,
+                validatedParams.previewFpsRange[0],
+                validatedParams.previewFpsRange[1]);
+
+        camera_metadata_ro_entry_t availablePreviewFpsRanges =
+            staticInfo(ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES, 2);
+        for (i = 0; i < availablePreviewFpsRanges.count; i += 2) {
+            if ((availablePreviewFpsRanges.data.i32[i] ==
+                    validatedParams.previewFpsRange[0]) &&
+                (availablePreviewFpsRanges.data.i32[i+1] ==
+                    validatedParams.previewFpsRange[1]) ) {
+                break;
+            }
+        }
+        if (i == availablePreviewFpsRanges.count) {
+            ALOGE("%s: Requested preview FPS range %d - %d is not supported",
+                __FUNCTION__, validatedParams.previewFpsRange[0],
+                    validatedParams.previewFpsRange[1]);
+            return BAD_VALUE;
+        }
+    }
+
+    // PREVIEW_FORMAT
+    validatedParams.previewFormat =
+            formatStringToEnum(newParams.getPreviewFormat());
+    if (validatedParams.previewFormat != previewFormat) {
+        if (state >= PREVIEW) {
+            ALOGE("%s: Preview format cannot be updated when preview "
+                    "is active!", __FUNCTION__);
+            return BAD_VALUE;
+        }
+        SortedVector<int32_t> availableFormats = getAvailableOutputFormats();
+        // If using flexible YUV, always support NV21/YV12. Otherwise, check
+        // HAL's list.
+        if (! (fastInfo.useFlexibleYuv &&
+                (validatedParams.previewFormat ==
+                        HAL_PIXEL_FORMAT_YCrCb_420_SP ||
+                 validatedParams.previewFormat ==
+                        HAL_PIXEL_FORMAT_YV12) ) ) {
+            // Not using flexible YUV format, so check explicitly
+            for (i = 0; i < availableFormats.size(); i++) {
+                if (availableFormats[i] == validatedParams.previewFormat) break;
+            }
+            if (i == availableFormats.size()) {
+                ALOGE("%s: Requested preview format %s (0x%x) is not supported",
+                        __FUNCTION__, newParams.getPreviewFormat(),
+                        validatedParams.previewFormat);
+                return BAD_VALUE;
+            }
+        }
+    }
+
+    // PREVIEW_FRAME_RATE Deprecated
+    // - Use only if the single FPS value was set later than the FPS range
+    if (fpsUseSingleValue) {
+        int previewFps = newParams.getPreviewFrameRate();
+        ALOGV("%s: Preview FPS single value requested: %d",
+              __FUNCTION__, previewFps);
+        {
+            camera_metadata_ro_entry_t availableFrameRates =
+                staticInfo(ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES);
+            /**
+              * If recording hint is set, find the range that encompasses
+              * previewFps with the largest min index.
+              *
+              * If recording hint is not set, find the range with previewFps
+              * with the smallest min index.
+              *
+              * Either way, in case of multiple ranges, break the tie by
+              * selecting the smaller range.
+              *
+              * Always select range within 30fps if one exists.
+              */
+
+            // all ranges which have previewFps
+            Vector<Range> candidateRanges;
+            Vector<Range> candidateFastRanges;
+            for (i = 0; i < availableFrameRates.count; i+=2) {
+                Range r = {
+                            availableFrameRates.data.i32[i],
+                            availableFrameRates.data.i32[i+1]
+                };
+                if (!isFpsSupported(availablePreviewSizes,
+                        HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED, r.max)) {
+                    continue;
+                }
+
+                if (r.min <= previewFps && previewFps <= r.max) {
+                    if (r.max <= MAX_DEFAULT_FPS) {
+                        candidateRanges.push(r);
+                    } else {
+                        candidateFastRanges.push(r);
+                    }
+                }
+            }
+            if (candidateRanges.isEmpty() && candidateFastRanges.isEmpty()) {
+                ALOGE("%s: Requested preview frame rate %d is not supported",
+                        __FUNCTION__, previewFps);
+                return BAD_VALUE;
+            }
+
+            // most applicable range with targetFps
+            Vector<Range>& ranges =
+                    candidateRanges.size() > 0 ? candidateRanges : candidateFastRanges;
+            Range bestRange = ranges[0];
+            for (i = 1; i < ranges.size(); ++i) {
+                Range r = ranges[i];
+                // Find by largest minIndex in recording mode
+                if (validatedParams.recordingHint) {
+                    if (r.min > bestRange.min) {
+                        bestRange = r;
+                    }
+                    else if (r.min == bestRange.min && r.max < bestRange.max) {
+                        bestRange = r;
+                    }
+                }
+                // Find by smallest minIndex in preview mode
+                else {
+                    if (r.min < bestRange.min) {
+                        bestRange = r;
+                    }
+                    else if (r.min == bestRange.min && r.max < bestRange.max) {
+                        bestRange = r;
+                    }
+                }
+            }
+
+            validatedParams.previewFpsRange[0] =
+                    bestRange.min;
+            validatedParams.previewFpsRange[1] =
+                    bestRange.max;
+
+            ALOGV("%s: New preview FPS range: %d, %d, recordingHint = %d",
+                __FUNCTION__,
+                validatedParams.previewFpsRange[0],
+                validatedParams.previewFpsRange[1],
+                validatedParams.recordingHint);
+        }
+    }
+
+    /**
+     * Update Preview FPS and Preview FPS ranges based on
+     * what we actually set.
+     *
+     * This updates the API-visible (Camera.Parameters#getParameters) values of
+     * the FPS fields, not only the internal versions.
+     *
+     * Order matters: The value that was set last takes precedence.
+     * - If the client does a setParameters(getParameters()) we retain
+     *   the same order for preview FPS.
+     */
+    if (!fpsUseSingleValue) {
+        // Set fps single, then fps range (range wins)
+        newParams.setPreviewFrameRate(
+                fpsFromRange(/*min*/validatedParams.previewFpsRange[0],
+                             /*max*/validatedParams.previewFpsRange[1]));
+        newParams.setPreviewFpsRange(
+                validatedParams.previewFpsRange[0] * kFpsToApiScale,
+                validatedParams.previewFpsRange[1] * kFpsToApiScale);
+    } else {
+        // Set fps range, then fps single (single wins)
+        newParams.setPreviewFpsRange(
+                validatedParams.previewFpsRange[0] * kFpsToApiScale,
+                validatedParams.previewFpsRange[1] * kFpsToApiScale);
+        // Set this to the same value, but with higher priority
+        newParams.setPreviewFrameRate(
+                newParams.getPreviewFrameRate());
+    }
+
+    // PICTURE_SIZE
+    newParams.getPictureSize(&validatedParams.pictureWidth,
+            &validatedParams.pictureHeight);
+    if (validatedParams.pictureWidth != pictureWidth ||
+            validatedParams.pictureHeight != pictureHeight) {
+        Vector<Size> availablePictureSizes = getAvailableJpegSizes();
+        for (i = 0; i < availablePictureSizes.size(); i++) {
+            if ((availablePictureSizes[i].width ==
+                    validatedParams.pictureWidth) &&
+                (availablePictureSizes[i].height ==
+                    validatedParams.pictureHeight)) break;
+        }
+        if (i == availablePictureSizes.size()) {
+            ALOGE("%s: Requested picture size %d x %d is not supported",
+                    __FUNCTION__, validatedParams.pictureWidth,
+                    validatedParams.pictureHeight);
+            return BAD_VALUE;
+        }
+    }
+
+    // JPEG_THUMBNAIL_WIDTH/HEIGHT
+    validatedParams.jpegThumbSize[0] =
+            newParams.getInt(CameraParameters::KEY_JPEG_THUMBNAIL_WIDTH);
+    validatedParams.jpegThumbSize[1] =
+            newParams.getInt(CameraParameters::KEY_JPEG_THUMBNAIL_HEIGHT);
+    if (validatedParams.jpegThumbSize[0] != jpegThumbSize[0] ||
+            validatedParams.jpegThumbSize[1] != jpegThumbSize[1]) {
+        camera_metadata_ro_entry_t availableJpegThumbSizes =
+            staticInfo(ANDROID_JPEG_AVAILABLE_THUMBNAIL_SIZES);
+        for (i = 0; i < availableJpegThumbSizes.count; i+=2) {
+            if ((availableJpegThumbSizes.data.i32[i] ==
+                    validatedParams.jpegThumbSize[0]) &&
+                (availableJpegThumbSizes.data.i32[i+1] ==
+                    validatedParams.jpegThumbSize[1])) break;
+        }
+        if (i == availableJpegThumbSizes.count) {
+            ALOGE("%s: Requested JPEG thumbnail size %d x %d is not supported",
+                    __FUNCTION__, validatedParams.jpegThumbSize[0],
+                    validatedParams.jpegThumbSize[1]);
+            return BAD_VALUE;
+        }
+    }
+
+    // JPEG_THUMBNAIL_QUALITY
+    int quality = newParams.getInt(CameraParameters::KEY_JPEG_THUMBNAIL_QUALITY);
+    // also makes sure quality fits in uint8_t
+    if (quality < 0 || quality > 100) {
+        ALOGE("%s: Requested JPEG thumbnail quality %d is not supported",
+                __FUNCTION__, quality);
+        return BAD_VALUE;
+    }
+    validatedParams.jpegThumbQuality = quality;
+
+    // JPEG_QUALITY
+    quality = newParams.getInt(CameraParameters::KEY_JPEG_QUALITY);
+    // also makes sure quality fits in uint8_t
+    if (quality < 0 || quality > 100) {
+        ALOGE("%s: Requested JPEG quality %d is not supported",
+                __FUNCTION__, quality);
+        return BAD_VALUE;
+    }
+    validatedParams.jpegQuality = quality;
+
+    // ROTATION
+    validatedParams.jpegRotation =
+            newParams.getInt(CameraParameters::KEY_ROTATION);
+    if (validatedParams.jpegRotation != 0 &&
+            validatedParams.jpegRotation != 90 &&
+            validatedParams.jpegRotation != 180 &&
+            validatedParams.jpegRotation != 270) {
+        ALOGE("%s: Requested picture rotation angle %d is not supported",
+                __FUNCTION__, validatedParams.jpegRotation);
+        return BAD_VALUE;
+    }
+
+    // GPS
+
+    const char *gpsLatStr =
+            newParams.get(CameraParameters::KEY_GPS_LATITUDE);
+    if (gpsLatStr != NULL) {
+        const char *gpsLongStr =
+                newParams.get(CameraParameters::KEY_GPS_LONGITUDE);
+        const char *gpsAltitudeStr =
+                newParams.get(CameraParameters::KEY_GPS_ALTITUDE);
+        const char *gpsTimeStr =
+                newParams.get(CameraParameters::KEY_GPS_TIMESTAMP);
+        const char *gpsProcMethodStr =
+                newParams.get(CameraParameters::KEY_GPS_PROCESSING_METHOD);
+        if (gpsLongStr == NULL ||
+                gpsAltitudeStr == NULL ||
+                gpsTimeStr == NULL ||
+                gpsProcMethodStr == NULL) {
+            ALOGE("%s: Incomplete set of GPS parameters provided",
+                    __FUNCTION__);
+            return BAD_VALUE;
+        }
+        char *endPtr;
+        errno = 0;
+        validatedParams.gpsCoordinates[0] = strtod(gpsLatStr, &endPtr);
+        if (errno || endPtr == gpsLatStr) {
+            ALOGE("%s: Malformed GPS latitude: %s", __FUNCTION__, gpsLatStr);
+            return BAD_VALUE;
+        }
+        errno = 0;
+        validatedParams.gpsCoordinates[1] = strtod(gpsLongStr, &endPtr);
+        if (errno || endPtr == gpsLongStr) {
+            ALOGE("%s: Malformed GPS longitude: %s", __FUNCTION__, gpsLongStr);
+            return BAD_VALUE;
+        }
+        errno = 0;
+        validatedParams.gpsCoordinates[2] = strtod(gpsAltitudeStr, &endPtr);
+        if (errno || endPtr == gpsAltitudeStr) {
+            ALOGE("%s: Malformed GPS altitude: %s", __FUNCTION__,
+                    gpsAltitudeStr);
+            return BAD_VALUE;
+        }
+        errno = 0;
+        validatedParams.gpsTimestamp = strtoll(gpsTimeStr, &endPtr, 10);
+        if (errno || endPtr == gpsTimeStr) {
+            ALOGE("%s: Malformed GPS timestamp: %s", __FUNCTION__, gpsTimeStr);
+            return BAD_VALUE;
+        }
+        validatedParams.gpsProcessingMethod = gpsProcMethodStr;
+
+        validatedParams.gpsEnabled = true;
+    } else {
+        validatedParams.gpsEnabled = false;
+    }
+
+    // EFFECT
+    validatedParams.effectMode = effectModeStringToEnum(
+        newParams.get(CameraParameters::KEY_EFFECT) );
+    if (validatedParams.effectMode != effectMode) {
+        camera_metadata_ro_entry_t availableEffectModes =
+            staticInfo(ANDROID_CONTROL_AVAILABLE_EFFECTS);
+        for (i = 0; i < availableEffectModes.count; i++) {
+            if (validatedParams.effectMode == availableEffectModes.data.u8[i]) break;
+        }
+        if (i == availableEffectModes.count) {
+            ALOGE("%s: Requested effect mode \"%s\" is not supported",
+                    __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_EFFECT) );
+            return BAD_VALUE;
+        }
+    }
+
+    // ANTIBANDING
+    validatedParams.antibandingMode = abModeStringToEnum(
+        newParams.get(CameraParameters::KEY_ANTIBANDING) );
+    if (validatedParams.antibandingMode != antibandingMode) {
+        camera_metadata_ro_entry_t availableAbModes =
+            staticInfo(ANDROID_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES);
+        for (i = 0; i < availableAbModes.count; i++) {
+            if (validatedParams.antibandingMode == availableAbModes.data.u8[i])
+                break;
+        }
+        if (i == availableAbModes.count) {
+            ALOGE("%s: Requested antibanding mode \"%s\" is not supported",
+                    __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_ANTIBANDING));
+            return BAD_VALUE;
+        }
+    }
+
+    // SCENE_MODE
+    validatedParams.sceneMode = sceneModeStringToEnum(
+        newParams.get(CameraParameters::KEY_SCENE_MODE) );
+    if (validatedParams.sceneMode != sceneMode &&
+            validatedParams.sceneMode !=
+            ANDROID_CONTROL_SCENE_MODE_DISABLED) {
+        camera_metadata_ro_entry_t availableSceneModes =
+            staticInfo(ANDROID_CONTROL_AVAILABLE_SCENE_MODES);
+        for (i = 0; i < availableSceneModes.count; i++) {
+            if (validatedParams.sceneMode == availableSceneModes.data.u8[i])
+                break;
+        }
+        if (i == availableSceneModes.count) {
+            ALOGE("%s: Requested scene mode \"%s\" is not supported",
+                    __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_SCENE_MODE));
+            return BAD_VALUE;
+        }
+    }
+    bool sceneModeSet =
+            validatedParams.sceneMode != ANDROID_CONTROL_SCENE_MODE_DISABLED;
+
+    // FLASH_MODE
+    if (sceneModeSet) {
+        validatedParams.flashMode =
+                fastInfo.sceneModeOverrides.
+                        valueFor(validatedParams.sceneMode).flashMode;
+    } else {
+        validatedParams.flashMode = FLASH_MODE_INVALID;
+    }
+    if (validatedParams.flashMode == FLASH_MODE_INVALID) {
+        validatedParams.flashMode = flashModeStringToEnum(
+            newParams.get(CameraParameters::KEY_FLASH_MODE) );
+    }
+
+    if (validatedParams.flashMode != flashMode) {
+        camera_metadata_ro_entry_t flashAvailable =
+            staticInfo(ANDROID_FLASH_INFO_AVAILABLE, 1, 1);
+        bool isFlashAvailable =
+                flashAvailable.data.u8[0] == ANDROID_FLASH_INFO_AVAILABLE_TRUE;
+        if (!isFlashAvailable &&
+                validatedParams.flashMode != Parameters::FLASH_MODE_OFF) {
+            ALOGE("%s: Requested flash mode \"%s\" is not supported: "
+                    "No flash on device", __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_FLASH_MODE));
+            return BAD_VALUE;
+        } else if (validatedParams.flashMode == Parameters::FLASH_MODE_RED_EYE) {
+            camera_metadata_ro_entry_t availableAeModes =
+                staticInfo(ANDROID_CONTROL_AE_AVAILABLE_MODES);
+            for (i = 0; i < availableAeModes.count; i++) {
+                if (validatedParams.flashMode == availableAeModes.data.u8[i])
+                    break;
+            }
+            if (i == availableAeModes.count) {
+                ALOGE("%s: Requested flash mode \"%s\" is not supported",
+                        __FUNCTION__,
+                        newParams.get(CameraParameters::KEY_FLASH_MODE));
+                return BAD_VALUE;
+            }
+        } else if (validatedParams.flashMode == -1) {
+            ALOGE("%s: Requested flash mode \"%s\" is unknown",
+                    __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_FLASH_MODE));
+            return BAD_VALUE;
+        }
+        // Update in case of override, but only if flash is supported
+        if (isFlashAvailable) {
+            newParams.set(CameraParameters::KEY_FLASH_MODE,
+                    flashModeEnumToString(validatedParams.flashMode));
+        }
+    }
+
+    // WHITE_BALANCE
+    if (sceneModeSet) {
+        validatedParams.wbMode =
+                fastInfo.sceneModeOverrides.
+                        valueFor(validatedParams.sceneMode).wbMode;
+    } else {
+        validatedParams.wbMode = ANDROID_CONTROL_AWB_MODE_OFF;
+    }
+    if (validatedParams.wbMode == ANDROID_CONTROL_AWB_MODE_OFF) {
+        validatedParams.wbMode = wbModeStringToEnum(
+            newParams.get(CameraParameters::KEY_WHITE_BALANCE) );
+    }
+    if (validatedParams.wbMode != wbMode) {
+        camera_metadata_ro_entry_t availableWbModes =
+            staticInfo(ANDROID_CONTROL_AWB_AVAILABLE_MODES, 0, 0, false);
+        for (i = 0; i < availableWbModes.count; i++) {
+            if (validatedParams.wbMode == availableWbModes.data.u8[i]) break;
+        }
+        if (i == availableWbModes.count) {
+            ALOGE("%s: Requested white balance mode %s is not supported",
+                    __FUNCTION__,
+                    newParams.get(CameraParameters::KEY_WHITE_BALANCE));
+            return BAD_VALUE;
+        }
+        // Update in case of override
+        newParams.set(CameraParameters::KEY_WHITE_BALANCE,
+                wbModeEnumToString(validatedParams.wbMode));
+    }
+
+    // FOCUS_MODE
+    if (sceneModeSet) {
+        validatedParams.focusMode =
+                fastInfo.sceneModeOverrides.
+                        valueFor(validatedParams.sceneMode).focusMode;
+    } else {
+        validatedParams.focusMode = FOCUS_MODE_INVALID;
+    }
+    if (validatedParams.focusMode == FOCUS_MODE_INVALID) {
+        validatedParams.focusMode = focusModeStringToEnum(
+                newParams.get(CameraParameters::KEY_FOCUS_MODE) );
+    }
+    if (validatedParams.focusMode != focusMode) {
+        validatedParams.currentAfTriggerId = -1;
+        if (validatedParams.focusMode != Parameters::FOCUS_MODE_FIXED) {
+            camera_metadata_ro_entry_t minFocusDistance =
+                staticInfo(ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE, 0, 0,
+                        false);
+            if (minFocusDistance.count && minFocusDistance.data.f[0] == 0) {
+                ALOGE("%s: Requested focus mode \"%s\" is not available: "
+                        "fixed focus lens",
+                        __FUNCTION__,
+                        newParams.get(CameraParameters::KEY_FOCUS_MODE));
+                return BAD_VALUE;
+            } else if (validatedParams.focusMode !=
+                    Parameters::FOCUS_MODE_INFINITY) {
+                camera_metadata_ro_entry_t availableFocusModes =
+                    staticInfo(ANDROID_CONTROL_AF_AVAILABLE_MODES);
+                for (i = 0; i < availableFocusModes.count; i++) {
+                    if (validatedParams.focusMode ==
+                            availableFocusModes.data.u8[i]) break;
+                }
+                if (i == availableFocusModes.count) {
+                    ALOGE("%s: Requested focus mode \"%s\" is not supported",
+                            __FUNCTION__,
+                            newParams.get(CameraParameters::KEY_FOCUS_MODE));
+                    return BAD_VALUE;
+                }
+            }
+        }
+        validatedParams.focusState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+        // Always reset shadow focus mode to avoid reverting settings
+        validatedParams.shadowFocusMode = FOCUS_MODE_INVALID;
+        // Update in case of override
+        newParams.set(CameraParameters::KEY_FOCUS_MODE,
+                focusModeEnumToString(validatedParams.focusMode));
+    } else {
+        validatedParams.currentAfTriggerId = currentAfTriggerId;
+    }
+
+    // FOCUS_AREAS
+    res = parseAreas(newParams.get(CameraParameters::KEY_FOCUS_AREAS),
+            &validatedParams.focusingAreas);
+    size_t maxAfRegions = (size_t)staticInfo(ANDROID_CONTROL_MAX_REGIONS,
+              Parameters::NUM_REGION, Parameters::NUM_REGION).
+              data.i32[Parameters::REGION_AF];
+    if (res == OK) res = validateAreas(validatedParams.focusingAreas,
+            maxAfRegions, AREA_KIND_FOCUS);
+    if (res != OK) {
+        ALOGE("%s: Requested focus areas are malformed: %s",
+                __FUNCTION__, newParams.get(CameraParameters::KEY_FOCUS_AREAS));
+        return BAD_VALUE;
+    }
+
+    // EXPOSURE_COMPENSATION
+    validatedParams.exposureCompensation =
+        newParams.getInt(CameraParameters::KEY_EXPOSURE_COMPENSATION);
+    camera_metadata_ro_entry_t exposureCompensationRange =
+        staticInfo(ANDROID_CONTROL_AE_COMPENSATION_RANGE);
+    if ((validatedParams.exposureCompensation <
+            exposureCompensationRange.data.i32[0]) ||
+        (validatedParams.exposureCompensation >
+            exposureCompensationRange.data.i32[1])) {
+        ALOGE("%s: Requested exposure compensation index is out of bounds: %d",
+                __FUNCTION__, validatedParams.exposureCompensation);
+        return BAD_VALUE;
+    }
+
+    if (autoExposureLockAvailable) {
+        validatedParams.autoExposureLock = boolFromString(
+            newParams.get(CameraParameters::KEY_AUTO_EXPOSURE_LOCK));
+    } else if (nullptr !=
+            newParams.get(CameraParameters::KEY_AUTO_EXPOSURE_LOCK)){
+        ALOGE("%s: Requested auto exposure lock is not supported",
+              __FUNCTION__);
+        return BAD_VALUE;
+    }
+
+    if (autoWhiteBalanceLockAvailable) {
+        validatedParams.autoWhiteBalanceLock = boolFromString(
+                newParams.get(CameraParameters::KEY_AUTO_WHITEBALANCE_LOCK));
+    } else if (nullptr !=
+           newParams.get(CameraParameters::KEY_AUTO_WHITEBALANCE_LOCK)) {
+        ALOGE("%s: Requested auto whitebalance lock is not supported",
+              __FUNCTION__);
+        return BAD_VALUE;
+    }
+
+    // METERING_AREAS
+    size_t maxAeRegions = (size_t)staticInfo(ANDROID_CONTROL_MAX_REGIONS,
+            Parameters::NUM_REGION, Parameters::NUM_REGION).
+            data.i32[Parameters::REGION_AE];
+    res = parseAreas(newParams.get(CameraParameters::KEY_METERING_AREAS),
+            &validatedParams.meteringAreas);
+    if (res == OK) {
+        res = validateAreas(validatedParams.meteringAreas, maxAeRegions,
+                            AREA_KIND_METERING);
+    }
+    if (res != OK) {
+        ALOGE("%s: Requested metering areas are malformed: %s",
+                __FUNCTION__,
+                newParams.get(CameraParameters::KEY_METERING_AREAS));
+        return BAD_VALUE;
+    }
+
+    // ZOOM
+    if (zoomAvailable) {
+        validatedParams.zoom = newParams.getInt(CameraParameters::KEY_ZOOM);
+        if (validatedParams.zoom < 0
+                    || validatedParams.zoom >= (int)NUM_ZOOM_STEPS) {
+            ALOGE("%s: Requested zoom level %d is not supported",
+                    __FUNCTION__, validatedParams.zoom);
+            return BAD_VALUE;
+        }
+    }
+
+    // VIDEO_SIZE
+    newParams.getVideoSize(&validatedParams.videoWidth,
+            &validatedParams.videoHeight);
+    if (validatedParams.videoWidth != videoWidth ||
+            validatedParams.videoHeight != videoHeight) {
+        if (state == RECORD) {
+            ALOGW("%s: Video size cannot be updated (from %d x %d to %d x %d)"
+                    " when recording is active! Ignore the size update!",
+                    __FUNCTION__, videoWidth, videoHeight, validatedParams.videoWidth,
+                    validatedParams.videoHeight);
+            validatedParams.videoWidth = videoWidth;
+            validatedParams.videoHeight = videoHeight;
+            newParams.setVideoSize(videoWidth, videoHeight);
+        } else {
+            for (i = 0; i < availableVideoSizes.size(); i++) {
+                if ((availableVideoSizes[i].width ==
+                        validatedParams.videoWidth) &&
+                    (availableVideoSizes[i].height ==
+                        validatedParams.videoHeight)) break;
+            }
+            if (i == availableVideoSizes.size()) {
+                ALOGE("%s: Requested video size %d x %d is not supported",
+                        __FUNCTION__, validatedParams.videoWidth,
+                        validatedParams.videoHeight);
+                return BAD_VALUE;
+            }
+        }
+    }
+
+    // VIDEO_STABILIZATION
+    validatedParams.videoStabilization = boolFromString(
+        newParams.get(CameraParameters::KEY_VIDEO_STABILIZATION) );
+    camera_metadata_ro_entry_t availableVideoStabilizationModes =
+        staticInfo(ANDROID_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES, 0, 0,
+                false);
+    if (validatedParams.videoStabilization &&
+            availableVideoStabilizationModes.count == 1) {
+        ALOGE("%s: Video stabilization not supported", __FUNCTION__);
+    }
+
+    // QTI Parameters
+    qtiParams->set(newParams);
+
+    /** Update internal parameters */
+
+    *this = validatedParams;
+    updateOverriddenJpegSize();
+
+    /** Update external parameters calculated from the internal ones */
+
+    // HORIZONTAL/VERTICAL FIELD OF VIEW
+    float horizFov, vertFov;
+    res = calculatePictureFovs(&horizFov, &vertFov);
+    if (res != OK) {
+        ALOGE("%s: Can't calculate FOVs", __FUNCTION__);
+        // continue so parameters are at least consistent
+    }
+    newParams.setFloat(CameraParameters::KEY_HORIZONTAL_VIEW_ANGLE,
+            horizFov);
+    newParams.setFloat(CameraParameters::KEY_VERTICAL_VIEW_ANGLE,
+            vertFov);
+    ALOGV("Current still picture FOV: %f x %f deg", horizFov, vertFov);
+
+    // Need to flatten again in case of overrides
+    paramsFlattened = newParams.flatten();
+    params = newParams;
+
+    slowJpegMode = false;
+    Size pictureSize = { pictureWidth, pictureHeight };
+    int64_t minFrameDurationNs = getJpegStreamMinFrameDurationNs(pictureSize);
+    if (previewFpsRange[1] > 1e9/minFrameDurationNs + FPS_MARGIN) {
+        slowJpegMode = true;
+    }
+    if (slowJpegMode || property_get_bool("camera.disable_zsl_mode", false)) {
+        allowZslMode = false;
+    } else {
+        allowZslMode = isZslReprocessPresent;
+    }
+    ALOGV("%s: allowZslMode: %d slowJpegMode %d", __FUNCTION__, allowZslMode, slowJpegMode);
+
+    return OK;
+}
+
+status_t Parameters::updateRequest(CameraMetadata *request) const {
+    ATRACE_CALL();
+    status_t res;
+
+    /**
+     * Mixin default important security values
+     * - android.led.transmit = defaulted ON
+     */
+    camera_metadata_ro_entry_t entry = staticInfo(ANDROID_LED_AVAILABLE_LEDS,
+                                                  /*minimumCount*/0,
+                                                  /*maximumCount*/0,
+                                                  /*required*/false);
+    for(size_t i = 0; i < entry.count; ++i) {
+        uint8_t led = entry.data.u8[i];
+
+        switch(led) {
+            // Transmit LED is unconditionally on when using
+            // the android.hardware.Camera API
+            case ANDROID_LED_AVAILABLE_LEDS_TRANSMIT: {
+                uint8_t transmitDefault = ANDROID_LED_TRANSMIT_ON;
+                res = request->update(ANDROID_LED_TRANSMIT,
+                                      &transmitDefault, 1);
+                if (res != OK) return res;
+                break;
+            }
+        }
+    }
+
+    /**
+     * Construct metadata from parameters
+     */
+
+    uint8_t metadataMode = ANDROID_REQUEST_METADATA_MODE_FULL;
+    res = request->update(ANDROID_REQUEST_METADATA_MODE,
+            &metadataMode, 1);
+    if (res != OK) return res;
+
+    camera_metadata_entry_t intent =
+            request->find(ANDROID_CONTROL_CAPTURE_INTENT);
+
+    if (intent.count == 0) return BAD_VALUE;
+
+    if (intent.data.u8[0] == ANDROID_CONTROL_CAPTURE_INTENT_STILL_CAPTURE) {
+        res = request->update(ANDROID_CONTROL_AE_TARGET_FPS_RANGE,
+                fastInfo.bestStillCaptureFpsRange, 2);
+    } else {
+        res = request->update(ANDROID_CONTROL_AE_TARGET_FPS_RANGE,
+                previewFpsRange, 2);
+    }
+    if (res != OK) return res;
+
+    if (autoWhiteBalanceLockAvailable) {
+        uint8_t reqWbLock = autoWhiteBalanceLock ?
+                ANDROID_CONTROL_AWB_LOCK_ON : ANDROID_CONTROL_AWB_LOCK_OFF;
+        res = request->update(ANDROID_CONTROL_AWB_LOCK,
+                &reqWbLock, 1);
+    }
+
+    res = request->update(ANDROID_CONTROL_EFFECT_MODE,
+            &effectMode, 1);
+    if (res != OK) return res;
+    res = request->update(ANDROID_CONTROL_AE_ANTIBANDING_MODE,
+            &antibandingMode, 1);
+    if (res != OK) return res;
+
+    // android.hardware.Camera requires that when face detect is enabled, the
+    // camera is in a face-priority mode. HAL3.x splits this into separate parts
+    // (face detection statistics and face priority scene mode). Map from other
+    // to the other.
+    bool sceneModeActive =
+            sceneMode != (uint8_t)ANDROID_CONTROL_SCENE_MODE_DISABLED;
+    uint8_t reqControlMode = ANDROID_CONTROL_MODE_AUTO;
+    if (enableFaceDetect || sceneModeActive) {
+        reqControlMode = ANDROID_CONTROL_MODE_USE_SCENE_MODE;
+    }
+    res = request->update(ANDROID_CONTROL_MODE,
+            &reqControlMode, 1);
+    if (res != OK) return res;
+
+    uint8_t reqSceneMode =
+            sceneModeActive ? sceneMode :
+            enableFaceDetect ? (uint8_t)ANDROID_CONTROL_SCENE_MODE_FACE_PRIORITY :
+            (uint8_t)ANDROID_CONTROL_SCENE_MODE_DISABLED;
+    res = request->update(ANDROID_CONTROL_SCENE_MODE,
+            &reqSceneMode, 1);
+    if (res != OK) return res;
+
+    uint8_t reqFlashMode = ANDROID_FLASH_MODE_OFF;
+    uint8_t reqAeMode = ANDROID_CONTROL_AE_MODE_OFF;
+    switch (flashMode) {
+        case Parameters::FLASH_MODE_OFF:
+            reqAeMode = ANDROID_CONTROL_AE_MODE_ON; break;
+        case Parameters::FLASH_MODE_AUTO:
+            reqAeMode = ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH; break;
+        case Parameters::FLASH_MODE_ON:
+            reqAeMode = ANDROID_CONTROL_AE_MODE_ON_ALWAYS_FLASH; break;
+        case Parameters::FLASH_MODE_TORCH:
+            reqAeMode = ANDROID_CONTROL_AE_MODE_ON;
+            reqFlashMode = ANDROID_FLASH_MODE_TORCH;
+            break;
+        case Parameters::FLASH_MODE_RED_EYE:
+            reqAeMode = ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE; break;
+        default:
+            ALOGE("%s: Camera %d: Unknown flash mode %d", __FUNCTION__,
+                    cameraId, flashMode);
+                return BAD_VALUE;
+    }
+    res = request->update(ANDROID_FLASH_MODE,
+            &reqFlashMode, 1);
+    if (res != OK) return res;
+    res = request->update(ANDROID_CONTROL_AE_MODE,
+            &reqAeMode, 1);
+    if (res != OK) return res;
+
+    if (autoExposureLockAvailable) {
+        uint8_t reqAeLock = autoExposureLock ?
+                ANDROID_CONTROL_AE_LOCK_ON : ANDROID_CONTROL_AE_LOCK_OFF;
+        res = request->update(ANDROID_CONTROL_AE_LOCK,
+                &reqAeLock, 1);
+        if (res != OK) return res;
+    }
+
+    res = request->update(ANDROID_CONTROL_AWB_MODE,
+            &wbMode, 1);
+    if (res != OK) return res;
+
+    float reqFocusDistance = 0; // infinity focus in diopters
+    uint8_t reqFocusMode = ANDROID_CONTROL_AF_MODE_OFF;
+    switch (focusMode) {
+        case Parameters::FOCUS_MODE_AUTO:
+        case Parameters::FOCUS_MODE_MACRO:
+        case Parameters::FOCUS_MODE_CONTINUOUS_VIDEO:
+        case Parameters::FOCUS_MODE_CONTINUOUS_PICTURE:
+        case Parameters::FOCUS_MODE_EDOF:
+            reqFocusMode = focusMode;
+            break;
+        case Parameters::FOCUS_MODE_INFINITY:
+        case Parameters::FOCUS_MODE_FIXED:
+            reqFocusMode = ANDROID_CONTROL_AF_MODE_OFF;
+            break;
+        default:
+                ALOGE("%s: Camera %d: Unknown focus mode %d", __FUNCTION__,
+                        cameraId, focusMode);
+                return BAD_VALUE;
+    }
+    res = request->update(ANDROID_LENS_FOCUS_DISTANCE,
+            &reqFocusDistance, 1);
+    if (res != OK) return res;
+    res = request->update(ANDROID_CONTROL_AF_MODE,
+            &reqFocusMode, 1);
+    if (res != OK) return res;
+
+    size_t reqFocusingAreasSize = focusingAreas.size() * 5;
+    int32_t *reqFocusingAreas = new int32_t[reqFocusingAreasSize];
+    for (size_t i = 0, j = 0; i < reqFocusingAreasSize; i += 5, j++) {
+        if (focusingAreas[j].weight != 0) {
+            reqFocusingAreas[i + 0] =
+                    normalizedXToArray(focusingAreas[j].left);
+            reqFocusingAreas[i + 1] =
+                    normalizedYToArray(focusingAreas[j].top);
+            reqFocusingAreas[i + 2] =
+                    normalizedXToArray(focusingAreas[j].right);
+            reqFocusingAreas[i + 3] =
+                    normalizedYToArray(focusingAreas[j].bottom);
+        } else {
+            reqFocusingAreas[i + 0] = 0;
+            reqFocusingAreas[i + 1] = 0;
+            reqFocusingAreas[i + 2] = 0;
+            reqFocusingAreas[i + 3] = 0;
+        }
+        reqFocusingAreas[i + 4] = focusingAreas[j].weight;
+    }
+    res = request->update(ANDROID_CONTROL_AF_REGIONS,
+            reqFocusingAreas, reqFocusingAreasSize);
+    if (res != OK) return res;
+    delete[] reqFocusingAreas;
+
+    res = request->update(ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION,
+            &exposureCompensation, 1);
+    if (res != OK) return res;
+
+    size_t reqMeteringAreasSize = meteringAreas.size() * 5;
+    int32_t *reqMeteringAreas = new int32_t[reqMeteringAreasSize];
+    for (size_t i = 0, j = 0; i < reqMeteringAreasSize; i += 5, j++) {
+        if (meteringAreas[j].weight != 0) {
+            reqMeteringAreas[i + 0] =
+                normalizedXToArray(meteringAreas[j].left);
+            reqMeteringAreas[i + 1] =
+                normalizedYToArray(meteringAreas[j].top);
+            reqMeteringAreas[i + 2] =
+                normalizedXToArray(meteringAreas[j].right);
+            reqMeteringAreas[i + 3] =
+                normalizedYToArray(meteringAreas[j].bottom);
+        } else {
+            reqMeteringAreas[i + 0] = 0;
+            reqMeteringAreas[i + 1] = 0;
+            reqMeteringAreas[i + 2] = 0;
+            reqMeteringAreas[i + 3] = 0;
+        }
+        reqMeteringAreas[i + 4] = meteringAreas[j].weight;
+    }
+    res = request->update(ANDROID_CONTROL_AE_REGIONS,
+            reqMeteringAreas, reqMeteringAreasSize);
+    if (res != OK) return res;
+
+    // Set awb regions to be the same as the metering regions if allowed
+    size_t maxAwbRegions = (size_t)staticInfo(ANDROID_CONTROL_MAX_REGIONS,
+            Parameters::NUM_REGION, Parameters::NUM_REGION).
+            data.i32[Parameters::REGION_AWB];
+    if (maxAwbRegions > 0) {
+        if (maxAwbRegions >= meteringAreas.size()) {
+            res = request->update(ANDROID_CONTROL_AWB_REGIONS,
+                    reqMeteringAreas, reqMeteringAreasSize);
+        } else {
+            // Ensure the awb regions are zeroed if the region count is too high.
+            int32_t zeroedAwbAreas[5] = {0, 0, 0, 0, 0};
+            res = request->update(ANDROID_CONTROL_AWB_REGIONS,
+                    zeroedAwbAreas, sizeof(zeroedAwbAreas)/sizeof(int32_t));
+        }
+        if (res != OK) return res;
+    }
+
+    delete[] reqMeteringAreas;
+
+    CropRegion crop = calculateCropRegion(/*previewOnly*/ false);
+    int32_t reqCropRegion[4] = {
+        static_cast<int32_t>(crop.left),
+        static_cast<int32_t>(crop.top),
+        static_cast<int32_t>(crop.width),
+        static_cast<int32_t>(crop.height)
+    };
+    res = request->update(ANDROID_SCALER_CROP_REGION,
+            reqCropRegion, 4);
+    if (res != OK) return res;
+
+    uint8_t reqVstabMode = videoStabilization ?
+            ANDROID_CONTROL_VIDEO_STABILIZATION_MODE_ON :
+            ANDROID_CONTROL_VIDEO_STABILIZATION_MODE_OFF;
+    res = request->update(ANDROID_CONTROL_VIDEO_STABILIZATION_MODE,
+            &reqVstabMode, 1);
+    if (res != OK) return res;
+
+    uint8_t reqFaceDetectMode = enableFaceDetect ?
+            fastInfo.bestFaceDetectMode :
+            (uint8_t)ANDROID_STATISTICS_FACE_DETECT_MODE_OFF;
+    res = request->update(ANDROID_STATISTICS_FACE_DETECT_MODE,
+            &reqFaceDetectMode, 1);
+    if (res != OK) return res;
+
+    qtiParams->updateRequest(request);
+
+    return OK;
+}
+
+status_t Parameters::updateRequestJpeg(CameraMetadata *request) const {
+    status_t res;
+
+    res = request->update(ANDROID_JPEG_THUMBNAIL_SIZE,
+            jpegThumbSize, 2);
+    if (res != OK) return res;
+    res = request->update(ANDROID_JPEG_THUMBNAIL_QUALITY,
+            &jpegThumbQuality, 1);
+    if (res != OK) return res;
+    res = request->update(ANDROID_JPEG_QUALITY,
+            &jpegQuality, 1);
+    if (res != OK) return res;
+    res = request->update(
+            ANDROID_JPEG_ORIENTATION,
+            &jpegRotation, 1);
+    if (res != OK) return res;
+
+    if (gpsEnabled) {
+        res = request->update(
+                ANDROID_JPEG_GPS_COORDINATES,
+                gpsCoordinates, 3);
+        if (res != OK) return res;
+        res = request->update(
+                ANDROID_JPEG_GPS_TIMESTAMP,
+                &gpsTimestamp, 1);
+        if (res != OK) return res;
+        res = request->update(
+                ANDROID_JPEG_GPS_PROCESSING_METHOD,
+                gpsProcessingMethod);
+        if (res != OK) return res;
+    } else {
+        res = request->erase(ANDROID_JPEG_GPS_COORDINATES);
+        if (res != OK) return res;
+        res = request->erase(ANDROID_JPEG_GPS_TIMESTAMP);
+        if (res != OK) return res;
+        res = request->erase(ANDROID_JPEG_GPS_PROCESSING_METHOD);
+        if (res != OK) return res;
+    }
+    return OK;
+}
+
+status_t Parameters::overrideJpegSizeByVideoSize() {
+    if (pictureSizeOverriden) {
+        ALOGV("Picture size has been overridden. Skip overriding");
+        return OK;
+    }
+
+    pictureSizeOverriden = true;
+    pictureWidthLastSet = pictureWidth;
+    pictureHeightLastSet = pictureHeight;
+    pictureWidth = videoWidth;
+    pictureHeight = videoHeight;
+    // This change of picture size is invisible to app layer.
+    // Do not update app visible params
+    return OK;
+}
+
+status_t Parameters::updateOverriddenJpegSize() {
+    if (!pictureSizeOverriden) {
+        ALOGV("Picture size has not been overridden. Skip checking");
+        return OK;
+    }
+
+    pictureWidthLastSet = pictureWidth;
+    pictureHeightLastSet = pictureHeight;
+
+    if (pictureWidth <= videoWidth && pictureHeight <= videoHeight) {
+        // Picture size is now smaller than video size. No need to override anymore
+        return recoverOverriddenJpegSize();
+    }
+
+    pictureWidth = videoWidth;
+    pictureHeight = videoHeight;
+
+    return OK;
+}
+
+status_t Parameters::recoverOverriddenJpegSize() {
+    if (!pictureSizeOverriden) {
+        ALOGV("Picture size has not been overridden. Skip recovering");
+        return OK;
+    }
+    pictureSizeOverriden = false;
+    pictureWidth = pictureWidthLastSet;
+    pictureHeight = pictureHeightLastSet;
+    return OK;
+}
+
+bool Parameters::isJpegSizeOverridden() {
+    return pictureSizeOverriden;
+}
+
+bool Parameters::useZeroShutterLag() const {
+    // If ZSL mode is disabled, don't use it
+    if (!allowZslMode) return false;
+    // If recording hint is enabled, don't do ZSL
+    if (recordingHint) return false;
+    // If still capture size is no bigger than preview or video size,
+    // don't do ZSL
+    if (pictureWidth <= previewWidth || pictureHeight <= previewHeight ||
+            pictureWidth <= videoWidth || pictureHeight <= videoHeight) {
+        return false;
+    }
+    // If still capture size is less than quarter of max, don't do ZSL
+    if ((pictureWidth * pictureHeight) <
+            (fastInfo.maxJpegSize.width * fastInfo.maxJpegSize.height / 4) ) {
+        return false;
+    }
+    return true;
+}
+
+const char* Parameters::getStateName(State state) {
+#define CASE_ENUM_TO_CHAR(x) case x: return(#x); break;
+    switch(state) {
+        CASE_ENUM_TO_CHAR(DISCONNECTED)
+        CASE_ENUM_TO_CHAR(STOPPED)
+        CASE_ENUM_TO_CHAR(WAITING_FOR_PREVIEW_WINDOW)
+        CASE_ENUM_TO_CHAR(PREVIEW)
+        CASE_ENUM_TO_CHAR(RECORD)
+        CASE_ENUM_TO_CHAR(STILL_CAPTURE)
+        CASE_ENUM_TO_CHAR(VIDEO_SNAPSHOT)
+        default:
+            return "Unknown state!";
+            break;
+    }
+#undef CASE_ENUM_TO_CHAR
+}
+
+int Parameters::formatStringToEnum(const char *format) {
+    return CameraParameters::previewFormatToEnum(format);
+}
+
+const char* Parameters::formatEnumToString(int format) {
+    const char *fmt;
+    switch(format) {
+        case HAL_PIXEL_FORMAT_YCbCr_422_SP: // NV16
+            fmt = CameraParameters::PIXEL_FORMAT_YUV422SP;
+            break;
+        case HAL_PIXEL_FORMAT_YCrCb_420_SP: // NV21
+            fmt = CameraParameters::PIXEL_FORMAT_YUV420SP;
+            break;
+        case HAL_PIXEL_FORMAT_YCbCr_422_I: // YUY2
+            fmt = CameraParameters::PIXEL_FORMAT_YUV422I;
+            break;
+        case HAL_PIXEL_FORMAT_YV12:        // YV12
+            fmt = CameraParameters::PIXEL_FORMAT_YUV420P;
+            break;
+        case HAL_PIXEL_FORMAT_RGB_565:     // RGB565
+            fmt = CameraParameters::PIXEL_FORMAT_RGB565;
+            break;
+        case HAL_PIXEL_FORMAT_RGBA_8888:   // RGBA8888
+            fmt = CameraParameters::PIXEL_FORMAT_RGBA8888;
+            break;
+        case HAL_PIXEL_FORMAT_RAW16:
+            ALOGW("Raw sensor preview format requested.");
+            fmt = CameraParameters::PIXEL_FORMAT_BAYER_RGGB;
+            break;
+        default:
+            ALOGE("%s: Unknown preview format: %x",
+                    __FUNCTION__,  format);
+            fmt = NULL;
+            break;
+    }
+    return fmt;
+}
+
+int Parameters::wbModeStringToEnum(const char *wbMode) {
+    return
+        !wbMode ?
+            ANDROID_CONTROL_AWB_MODE_AUTO :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_AUTO) ?
+            ANDROID_CONTROL_AWB_MODE_AUTO :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_INCANDESCENT) ?
+            ANDROID_CONTROL_AWB_MODE_INCANDESCENT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_FLUORESCENT) ?
+            ANDROID_CONTROL_AWB_MODE_FLUORESCENT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_WARM_FLUORESCENT) ?
+            ANDROID_CONTROL_AWB_MODE_WARM_FLUORESCENT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_DAYLIGHT) ?
+            ANDROID_CONTROL_AWB_MODE_DAYLIGHT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_CLOUDY_DAYLIGHT) ?
+            ANDROID_CONTROL_AWB_MODE_CLOUDY_DAYLIGHT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_TWILIGHT) ?
+            ANDROID_CONTROL_AWB_MODE_TWILIGHT :
+        !strcmp(wbMode, CameraParameters::WHITE_BALANCE_SHADE) ?
+            ANDROID_CONTROL_AWB_MODE_SHADE :
+        QTIParameters::wbModeStringToEnum(wbMode);
+}
+
+const char* Parameters::wbModeEnumToString(uint8_t wbMode) {
+    switch (wbMode) {
+        case ANDROID_CONTROL_AWB_MODE_AUTO:
+            return CameraParameters::WHITE_BALANCE_AUTO;
+        case ANDROID_CONTROL_AWB_MODE_INCANDESCENT:
+            return CameraParameters::WHITE_BALANCE_INCANDESCENT;
+        case ANDROID_CONTROL_AWB_MODE_FLUORESCENT:
+            return CameraParameters::WHITE_BALANCE_FLUORESCENT;
+        case ANDROID_CONTROL_AWB_MODE_WARM_FLUORESCENT:
+            return CameraParameters::WHITE_BALANCE_WARM_FLUORESCENT;
+        case ANDROID_CONTROL_AWB_MODE_DAYLIGHT:
+            return CameraParameters::WHITE_BALANCE_DAYLIGHT;
+        case ANDROID_CONTROL_AWB_MODE_CLOUDY_DAYLIGHT:
+            return CameraParameters::WHITE_BALANCE_CLOUDY_DAYLIGHT;
+        case ANDROID_CONTROL_AWB_MODE_TWILIGHT:
+            return CameraParameters::WHITE_BALANCE_TWILIGHT;
+        case ANDROID_CONTROL_AWB_MODE_SHADE:
+            return CameraParameters::WHITE_BALANCE_SHADE;
+        default:
+            return QTIParameters::wbModeEnumToString(wbMode);
+    }
+}
+
+int Parameters::effectModeStringToEnum(const char *effectMode) {
+    return
+        !effectMode ?
+            ANDROID_CONTROL_EFFECT_MODE_OFF :
+        !strcmp(effectMode, CameraParameters::EFFECT_NONE) ?
+            ANDROID_CONTROL_EFFECT_MODE_OFF :
+        !strcmp(effectMode, CameraParameters::EFFECT_MONO) ?
+            ANDROID_CONTROL_EFFECT_MODE_MONO :
+        !strcmp(effectMode, CameraParameters::EFFECT_NEGATIVE) ?
+            ANDROID_CONTROL_EFFECT_MODE_NEGATIVE :
+        !strcmp(effectMode, CameraParameters::EFFECT_SOLARIZE) ?
+            ANDROID_CONTROL_EFFECT_MODE_SOLARIZE :
+        !strcmp(effectMode, CameraParameters::EFFECT_SEPIA) ?
+            ANDROID_CONTROL_EFFECT_MODE_SEPIA :
+        !strcmp(effectMode, CameraParameters::EFFECT_POSTERIZE) ?
+            ANDROID_CONTROL_EFFECT_MODE_POSTERIZE :
+        !strcmp(effectMode, CameraParameters::EFFECT_WHITEBOARD) ?
+            ANDROID_CONTROL_EFFECT_MODE_WHITEBOARD :
+        !strcmp(effectMode, CameraParameters::EFFECT_BLACKBOARD) ?
+            ANDROID_CONTROL_EFFECT_MODE_BLACKBOARD :
+        !strcmp(effectMode, CameraParameters::EFFECT_AQUA) ?
+            ANDROID_CONTROL_EFFECT_MODE_AQUA :
+        -1;
+}
+
+int Parameters::abModeStringToEnum(const char *abMode) {
+    return
+        !abMode ?
+            ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO :
+        !strcmp(abMode, CameraParameters::ANTIBANDING_AUTO) ?
+            ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO :
+        !strcmp(abMode, CameraParameters::ANTIBANDING_OFF) ?
+            ANDROID_CONTROL_AE_ANTIBANDING_MODE_OFF :
+        !strcmp(abMode, CameraParameters::ANTIBANDING_50HZ) ?
+            ANDROID_CONTROL_AE_ANTIBANDING_MODE_50HZ :
+        !strcmp(abMode, CameraParameters::ANTIBANDING_60HZ) ?
+            ANDROID_CONTROL_AE_ANTIBANDING_MODE_60HZ :
+        -1;
+}
+
+int Parameters::sceneModeStringToEnum(const char *sceneMode) {
+    return
+        !sceneMode ?
+            ANDROID_CONTROL_SCENE_MODE_DISABLED :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_AUTO) ?
+            ANDROID_CONTROL_SCENE_MODE_DISABLED :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_ACTION) ?
+            ANDROID_CONTROL_SCENE_MODE_ACTION :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_PORTRAIT) ?
+            ANDROID_CONTROL_SCENE_MODE_PORTRAIT :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_LANDSCAPE) ?
+            ANDROID_CONTROL_SCENE_MODE_LANDSCAPE :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_NIGHT) ?
+            ANDROID_CONTROL_SCENE_MODE_NIGHT :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_NIGHT_PORTRAIT) ?
+            ANDROID_CONTROL_SCENE_MODE_NIGHT_PORTRAIT :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_THEATRE) ?
+            ANDROID_CONTROL_SCENE_MODE_THEATRE :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_BEACH) ?
+            ANDROID_CONTROL_SCENE_MODE_BEACH :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_SNOW) ?
+            ANDROID_CONTROL_SCENE_MODE_SNOW :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_SUNSET) ?
+            ANDROID_CONTROL_SCENE_MODE_SUNSET :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_STEADYPHOTO) ?
+            ANDROID_CONTROL_SCENE_MODE_STEADYPHOTO :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_FIREWORKS) ?
+            ANDROID_CONTROL_SCENE_MODE_FIREWORKS :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_SPORTS) ?
+            ANDROID_CONTROL_SCENE_MODE_SPORTS :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_PARTY) ?
+            ANDROID_CONTROL_SCENE_MODE_PARTY :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_CANDLELIGHT) ?
+            ANDROID_CONTROL_SCENE_MODE_CANDLELIGHT :
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_BARCODE) ?
+            ANDROID_CONTROL_SCENE_MODE_BARCODE:
+        !strcmp(sceneMode, CameraParameters::SCENE_MODE_HDR) ?
+            ANDROID_CONTROL_SCENE_MODE_HDR:
+        -1;
+}
+
+Parameters::Parameters::flashMode_t Parameters::flashModeStringToEnum(
+        const char *flashMode) {
+    return
+        !flashMode ?
+            Parameters::FLASH_MODE_OFF :
+        !strcmp(flashMode, CameraParameters::FLASH_MODE_OFF) ?
+            Parameters::FLASH_MODE_OFF :
+        !strcmp(flashMode, CameraParameters::FLASH_MODE_AUTO) ?
+            Parameters::FLASH_MODE_AUTO :
+        !strcmp(flashMode, CameraParameters::FLASH_MODE_ON) ?
+            Parameters::FLASH_MODE_ON :
+        !strcmp(flashMode, CameraParameters::FLASH_MODE_RED_EYE) ?
+            Parameters::FLASH_MODE_RED_EYE :
+        !strcmp(flashMode, CameraParameters::FLASH_MODE_TORCH) ?
+            Parameters::FLASH_MODE_TORCH :
+        Parameters::FLASH_MODE_INVALID;
+}
+
+const char *Parameters::flashModeEnumToString(flashMode_t flashMode) {
+    switch (flashMode) {
+        case FLASH_MODE_OFF:
+            return CameraParameters::FLASH_MODE_OFF;
+        case FLASH_MODE_AUTO:
+            return CameraParameters::FLASH_MODE_AUTO;
+        case FLASH_MODE_ON:
+            return CameraParameters::FLASH_MODE_ON;
+        case FLASH_MODE_RED_EYE:
+            return CameraParameters::FLASH_MODE_RED_EYE;
+        case FLASH_MODE_TORCH:
+            return CameraParameters::FLASH_MODE_TORCH;
+        default:
+            ALOGE("%s: Unknown flash mode enum %d",
+                    __FUNCTION__, flashMode);
+            return "unknown";
+    }
+}
+
+Parameters::Parameters::focusMode_t Parameters::focusModeStringToEnum(
+        const char *focusMode) {
+    return
+        !focusMode ?
+            Parameters::FOCUS_MODE_INVALID :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_AUTO) ?
+            Parameters::FOCUS_MODE_AUTO :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_INFINITY) ?
+            Parameters::FOCUS_MODE_INFINITY :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_MACRO) ?
+            Parameters::FOCUS_MODE_MACRO :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_FIXED) ?
+            Parameters::FOCUS_MODE_FIXED :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_EDOF) ?
+            Parameters::FOCUS_MODE_EDOF :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_CONTINUOUS_VIDEO) ?
+            Parameters::FOCUS_MODE_CONTINUOUS_VIDEO :
+        !strcmp(focusMode, CameraParameters::FOCUS_MODE_CONTINUOUS_PICTURE) ?
+            Parameters::FOCUS_MODE_CONTINUOUS_PICTURE :
+        Parameters::FOCUS_MODE_INVALID;
+}
+
+const char *Parameters::focusModeEnumToString(focusMode_t focusMode) {
+    switch (focusMode) {
+        case FOCUS_MODE_AUTO:
+            return CameraParameters::FOCUS_MODE_AUTO;
+        case FOCUS_MODE_MACRO:
+            return CameraParameters::FOCUS_MODE_MACRO;
+        case FOCUS_MODE_CONTINUOUS_VIDEO:
+            return CameraParameters::FOCUS_MODE_CONTINUOUS_VIDEO;
+        case FOCUS_MODE_CONTINUOUS_PICTURE:
+            return CameraParameters::FOCUS_MODE_CONTINUOUS_PICTURE;
+        case FOCUS_MODE_EDOF:
+            return CameraParameters::FOCUS_MODE_EDOF;
+        case FOCUS_MODE_INFINITY:
+            return CameraParameters::FOCUS_MODE_INFINITY;
+        case FOCUS_MODE_FIXED:
+            return CameraParameters::FOCUS_MODE_FIXED;
+        default:
+            ALOGE("%s: Unknown focus mode enum: %d",
+                    __FUNCTION__, focusMode);
+            return "unknown";
+    }
+}
+
+status_t Parameters::parseAreas(const char *areasCStr,
+        Vector<Parameters::Area> *areas) {
+    static const size_t NUM_FIELDS = 5;
+    areas->clear();
+    if (areasCStr == NULL) {
+        // If no key exists, use default (0,0,0,0,0)
+        areas->push();
+        return OK;
+    }
+    String8 areasStr(areasCStr);
+    ssize_t areaStart = areasStr.find("(", 0) + 1;
+    while (areaStart != 0) {
+        const char* area = areasStr.string() + areaStart;
+        char *numEnd;
+        int vals[NUM_FIELDS];
+        for (size_t i = 0; i < NUM_FIELDS; i++) {
+            errno = 0;
+            vals[i] = strtol(area, &numEnd, 10);
+            if (errno || numEnd == area) return BAD_VALUE;
+            area = numEnd + 1;
+        }
+        areas->push(Parameters::Area(
+            vals[0], vals[1], vals[2], vals[3], vals[4]) );
+        areaStart = areasStr.find("(", areaStart) + 1;
+    }
+    return OK;
+}
+
+status_t Parameters::validateAreas(const Vector<Parameters::Area> &areas,
+                                      size_t maxRegions,
+                                      AreaKind areaKind) const {
+    // Definition of valid area can be found in
+    // include/camera/CameraParameters.h
+    if (areas.size() == 0) return BAD_VALUE;
+    if (areas.size() == 1) {
+        if (areas[0].left == 0 &&
+                areas[0].top == 0 &&
+                areas[0].right == 0 &&
+                areas[0].bottom == 0 &&
+                areas[0].weight == 0) {
+            // Single (0,0,0,0,0) entry is always valid (== driver decides)
+            return OK;
+        }
+    }
+
+    // fixed focus can only set (0,0,0,0,0) focus area
+    if (areaKind == AREA_KIND_FOCUS && focusMode == FOCUS_MODE_FIXED) {
+        return BAD_VALUE;
+    }
+
+    if (areas.size() > maxRegions) {
+        ALOGE("%s: Too many areas requested: %zu",
+                __FUNCTION__, areas.size());
+        return BAD_VALUE;
+    }
+
+    for (Vector<Parameters::Area>::const_iterator a = areas.begin();
+         a != areas.end(); a++) {
+        if (a->weight < 1 || a->weight > 1000) return BAD_VALUE;
+        if (a->left < -1000 || a->left > 1000) return BAD_VALUE;
+        if (a->top < -1000 || a->top > 1000) return BAD_VALUE;
+        if (a->right < -1000 || a->right > 1000) return BAD_VALUE;
+        if (a->bottom < -1000 || a->bottom > 1000) return BAD_VALUE;
+        if (a->left >= a->right) return BAD_VALUE;
+        if (a->top >= a->bottom) return BAD_VALUE;
+    }
+    return OK;
+}
+
+bool Parameters::boolFromString(const char *boolStr) {
+    return !boolStr ? false :
+        !strcmp(boolStr, CameraParameters::TRUE) ? true :
+        false;
+}
+
+int Parameters::degToTransform(int degrees, bool mirror) {
+    if (!mirror) {
+        if (degrees == 0) return 0;
+        else if (degrees == 90) return HAL_TRANSFORM_ROT_90;
+        else if (degrees == 180) return HAL_TRANSFORM_ROT_180;
+        else if (degrees == 270) return HAL_TRANSFORM_ROT_270;
+    } else {  // Do mirror (horizontal flip)
+        if (degrees == 0) {           // FLIP_H and ROT_0
+            return HAL_TRANSFORM_FLIP_H;
+        } else if (degrees == 90) {   // FLIP_H and ROT_90
+            return HAL_TRANSFORM_FLIP_H | HAL_TRANSFORM_ROT_90;
+        } else if (degrees == 180) {  // FLIP_H and ROT_180
+            return HAL_TRANSFORM_FLIP_V;
+        } else if (degrees == 270) {  // FLIP_H and ROT_270
+            return HAL_TRANSFORM_FLIP_V | HAL_TRANSFORM_ROT_90;
+        }
+    }
+    ALOGE("%s: Bad input: %d", __FUNCTION__, degrees);
+    return -1;
+}
+
+int Parameters::cropXToArray(int x) const {
+    ALOG_ASSERT(x >= 0, "Crop-relative X coordinate = '%d' is out of bounds"
+                         "(lower = 0)", x);
+
+    CropRegion previewCrop = calculateCropRegion(/*previewOnly*/ true);
+    ALOG_ASSERT(x < previewCrop.width, "Crop-relative X coordinate = '%d' "
+                    "is out of bounds (upper = %f)", x, previewCrop.width);
+
+    int ret = x + previewCrop.left;
+
+    ALOG_ASSERT( (ret >= 0 && ret < fastInfo.arrayWidth),
+        "Calculated pixel array value X = '%d' is out of bounds (upper = %d)",
+        ret, fastInfo.arrayWidth);
+    return ret;
+}
+
+int Parameters::cropYToArray(int y) const {
+    ALOG_ASSERT(y >= 0, "Crop-relative Y coordinate = '%d' is out of bounds "
+        "(lower = 0)", y);
+
+    CropRegion previewCrop = calculateCropRegion(/*previewOnly*/ true);
+    ALOG_ASSERT(y < previewCrop.height, "Crop-relative Y coordinate = '%d' is "
+                "out of bounds (upper = %f)", y, previewCrop.height);
+
+    int ret = y + previewCrop.top;
+
+    ALOG_ASSERT( (ret >= 0 && ret < fastInfo.arrayHeight),
+        "Calculated pixel array value Y = '%d' is out of bounds (upper = %d)",
+        ret, fastInfo.arrayHeight);
+
+    return ret;
+
+}
+
+int Parameters::normalizedXToCrop(int x) const {
+    CropRegion previewCrop = calculateCropRegion(/*previewOnly*/ true);
+    return (x + 1000) * (previewCrop.width - 1) / 2000;
+}
+
+int Parameters::normalizedYToCrop(int y) const {
+    CropRegion previewCrop = calculateCropRegion(/*previewOnly*/ true);
+    return (y + 1000) * (previewCrop.height - 1) / 2000;
+}
+
+int Parameters::normalizedXToArray(int x) const {
+
+    // Work-around for HAL pre-scaling the coordinates themselves
+    if (quirks.meteringCropRegion) {
+        return (x + 1000) * (fastInfo.arrayWidth - 1) / 2000;
+    }
+
+    return cropXToArray(normalizedXToCrop(x));
+}
+
+int Parameters::normalizedYToArray(int y) const {
+    // Work-around for HAL pre-scaling the coordinates themselves
+    if (quirks.meteringCropRegion) {
+        return (y + 1000) * (fastInfo.arrayHeight - 1) / 2000;
+    }
+
+    return cropYToArray(normalizedYToCrop(y));
+}
+
+
+Parameters::CropRegion Parameters::calculatePreviewCrop(
+        const CropRegion &scalerCrop) const {
+    float left, top, width, height;
+    float previewAspect = static_cast<float>(previewWidth) / previewHeight;
+    float cropAspect = scalerCrop.width / scalerCrop.height;
+
+    if (previewAspect > cropAspect) {
+        width = scalerCrop.width;
+        height = cropAspect * scalerCrop.height / previewAspect;
+
+        left = scalerCrop.left;
+        top = scalerCrop.top + (scalerCrop.height - height) / 2;
+    } else {
+        width = previewAspect * scalerCrop.width / cropAspect;
+        height = scalerCrop.height;
+
+        left = scalerCrop.left + (scalerCrop.width - width) / 2;
+        top = scalerCrop.top;
+    }
+
+    CropRegion previewCrop = {left, top, width, height};
+
+    return previewCrop;
+}
+
+int Parameters::arrayXToNormalizedWithCrop(int x,
+        const CropRegion &scalerCrop) const {
+    // Work-around for HAL pre-scaling the coordinates themselves
+    if (quirks.meteringCropRegion) {
+        return x * 2000 / (fastInfo.arrayWidth - 1) - 1000;
+    } else {
+        CropRegion previewCrop = calculatePreviewCrop(scalerCrop);
+        return (x - previewCrop.left) * 2000 / (previewCrop.width - 1) - 1000;
+    }
+}
+
+int Parameters::arrayYToNormalizedWithCrop(int y,
+        const CropRegion &scalerCrop) const {
+    // Work-around for HAL pre-scaling the coordinates themselves
+    if (quirks.meteringCropRegion) {
+        return y * 2000 / (fastInfo.arrayHeight - 1) - 1000;
+    } else {
+        CropRegion previewCrop = calculatePreviewCrop(scalerCrop);
+        return (y - previewCrop.top) * 2000 / (previewCrop.height - 1) - 1000;
+    }
+}
+
+status_t Parameters::getFilteredSizes(Size limit, Vector<Size> *sizes) {
+    if (info == NULL) {
+        ALOGE("%s: Static metadata is not initialized", __FUNCTION__);
+        return NO_INIT;
+    }
+    if (sizes == NULL) {
+        ALOGE("%s: Input size is null", __FUNCTION__);
+        return BAD_VALUE;
+    }
+    sizes->clear();
+
+    Vector<StreamConfiguration> scs = getStreamConfigurations();
+    for (size_t i=0; i < scs.size(); i++) {
+        const StreamConfiguration &sc = scs[i];
+        if (sc.isInput == ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_OUTPUT &&
+                sc.format == HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED &&
+                sc.width <= limit.width && sc.height <= limit.height) {
+            Size sz = {sc.width, sc.height};
+            sizes->push(sz);
+        }
+    }
+
+    if (sizes->isEmpty()) {
+        ALOGE("generated preview size list is empty!!");
+        return BAD_VALUE;
+    }
+    return OK;
+}
+
+Parameters::Size Parameters::getMaxSizeForRatio(
+        float ratio, const int32_t* sizeArray, size_t count) {
+    ALOG_ASSERT(sizeArray != NULL, "size array shouldn't be NULL");
+    ALOG_ASSERT(count >= 2 && count % 2 == 0, "count must be a positive even number");
+
+    Size maxSize = {0, 0};
+    for (size_t i = 0; i < count; i += 2) {
+        if (sizeArray[i] > 0 && sizeArray[i+1] > 0) {
+            float curRatio = static_cast<float>(sizeArray[i]) / sizeArray[i+1];
+            if (fabs(curRatio - ratio) < ASPECT_RATIO_TOLERANCE && maxSize.width < sizeArray[i]) {
+                maxSize.width = sizeArray[i];
+                maxSize.height = sizeArray[i+1];
+            }
+        }
+    }
+
+    if (maxSize.width == 0 || maxSize.height == 0) {
+        maxSize.width = sizeArray[0];
+        maxSize.height = sizeArray[1];
+        ALOGW("Unable to find the size to match the given aspect ratio %f."
+                "Fall back to %d x %d", ratio, maxSize.width, maxSize.height);
+    }
+
+    return maxSize;
+}
+
+Parameters::Size Parameters::getMaxSize(const Vector<Parameters::Size> &sizes) {
+    Size maxSize = {-1, -1};
+    for (size_t i = 0; i < sizes.size(); i++) {
+        if (sizes[i].width > maxSize.width ||
+                (sizes[i].width == maxSize.width && sizes[i].height > maxSize.height )) {
+            maxSize = sizes[i];
+        }
+    }
+    return maxSize;
+}
+
+Vector<Parameters::StreamConfiguration> Parameters::getStreamConfigurations() {
+    const int STREAM_CONFIGURATION_SIZE = 4;
+    const int STREAM_FORMAT_OFFSET = 0;
+    const int STREAM_WIDTH_OFFSET = 1;
+    const int STREAM_HEIGHT_OFFSET = 2;
+    const int STREAM_IS_INPUT_OFFSET = 3;
+    Vector<StreamConfiguration> scs;
+
+    camera_metadata_ro_entry_t availableStreamConfigs =
+                staticInfo(ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS);
+    for (size_t i = 0; i < availableStreamConfigs.count; i+= STREAM_CONFIGURATION_SIZE) {
+        int32_t format = availableStreamConfigs.data.i32[i + STREAM_FORMAT_OFFSET];
+        int32_t width = availableStreamConfigs.data.i32[i + STREAM_WIDTH_OFFSET];
+        int32_t height = availableStreamConfigs.data.i32[i + STREAM_HEIGHT_OFFSET];
+        int32_t isInput = availableStreamConfigs.data.i32[i + STREAM_IS_INPUT_OFFSET];
+        StreamConfiguration sc = {format, width, height, isInput};
+        scs.add(sc);
+    }
+    return scs;
+}
+
+int64_t Parameters::getJpegStreamMinFrameDurationNs(Parameters::Size size) {
+    return getMinFrameDurationNs(size, HAL_PIXEL_FORMAT_BLOB);
+}
+
+int64_t Parameters::getMinFrameDurationNs(Parameters::Size size, int fmt) {
+    const int STREAM_DURATION_SIZE = 4;
+    const int STREAM_FORMAT_OFFSET = 0;
+    const int STREAM_WIDTH_OFFSET = 1;
+    const int STREAM_HEIGHT_OFFSET = 2;
+    const int STREAM_DURATION_OFFSET = 3;
+    camera_metadata_ro_entry_t availableStreamMinDurations =
+                staticInfo(ANDROID_SCALER_AVAILABLE_MIN_FRAME_DURATIONS);
+    for (size_t i = 0; i < availableStreamMinDurations.count; i+= STREAM_DURATION_SIZE) {
+        int64_t format = availableStreamMinDurations.data.i64[i + STREAM_FORMAT_OFFSET];
+        int64_t width = availableStreamMinDurations.data.i64[i + STREAM_WIDTH_OFFSET];
+        int64_t height = availableStreamMinDurations.data.i64[i + STREAM_HEIGHT_OFFSET];
+        int64_t duration = availableStreamMinDurations.data.i64[i + STREAM_DURATION_OFFSET];
+        if (format == fmt && width == size.width && height == size.height) {
+            return duration;
+        }
+    }
+
+    return -1;
+}
+
+bool Parameters::isFpsSupported(const Vector<Size> &sizes, int format, int32_t fps) {
+    // Get min frame duration for each size and check if the given fps range can be supported.
+    for (size_t i = 0 ; i < sizes.size(); i++) {
+        int64_t minFrameDuration = getMinFrameDurationNs(sizes[i], format);
+        if (minFrameDuration <= 0) {
+            ALOGE("Min frame duration (%" PRId64") for size (%dx%d) and format 0x%x is wrong!",
+                minFrameDuration, sizes[i].width, sizes[i].height, format);
+            return false;
+        }
+        int32_t maxSupportedFps = 1e9 / minFrameDuration;
+        // Add some margin here for the case where the hal supports 29.xxxfps.
+        maxSupportedFps += FPS_MARGIN;
+        if (fps > maxSupportedFps) {
+            return false;
+        }
+    }
+    return true;
+}
+
+SortedVector<int32_t> Parameters::getAvailableOutputFormats() {
+    SortedVector<int32_t> outputFormats; // Non-duplicated output formats
+    Vector<StreamConfiguration> scs = getStreamConfigurations();
+    for (size_t i = 0; i < scs.size(); i++) {
+        const StreamConfiguration &sc = scs[i];
+        if (sc.isInput == ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_OUTPUT) {
+            outputFormats.add(sc.format);
+        }
+    }
+
+    return outputFormats;
+}
+
+Vector<Parameters::Size> Parameters::getAvailableJpegSizes() {
+    Vector<Parameters::Size> jpegSizes;
+    Vector<StreamConfiguration> scs = getStreamConfigurations();
+    for (size_t i = 0; i < scs.size(); i++) {
+        const StreamConfiguration &sc = scs[i];
+        if (sc.isInput == ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS_OUTPUT &&
+                sc.format == HAL_PIXEL_FORMAT_BLOB) {
+            Size sz = {sc.width, sc.height};
+            jpegSizes.add(sz);
+        }
+    }
+
+    return jpegSizes;
+}
+
+Parameters::CropRegion Parameters::calculateCropRegion(bool previewOnly) const {
+
+    float zoomLeft, zoomTop, zoomWidth, zoomHeight;
+
+    // Need to convert zoom index into a crop rectangle. The rectangle is
+    // chosen to maximize its area on the sensor
+
+    camera_metadata_ro_entry_t maxDigitalZoom =
+            staticInfo(ANDROID_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM);
+    // For each zoom step by how many pixels more do we change the zoom
+    float zoomIncrement = (maxDigitalZoom.data.f[0] - 1) /
+            (NUM_ZOOM_STEPS-1);
+    // The desired activeAreaWidth/cropAreaWidth ratio (or height if h>w)
+    // via interpolating zoom step into a zoom ratio
+    float zoomRatio = 1 + zoomIncrement * zoom;
+    ALOG_ASSERT( (zoomRatio >= 1.f && zoomRatio <= maxDigitalZoom.data.f[0]),
+        "Zoom ratio calculated out of bounds. Expected 1 - %f, actual: %f",
+        maxDigitalZoom.data.f[0], zoomRatio);
+
+    ALOGV("Zoom maxDigital=%f, increment=%f, ratio=%f, previewWidth=%d, "
+          "previewHeight=%d, activeWidth=%d, activeHeight=%d",
+          maxDigitalZoom.data.f[0], zoomIncrement, zoomRatio, previewWidth,
+          previewHeight, fastInfo.arrayWidth, fastInfo.arrayHeight);
+
+    if (previewOnly) {
+        // Calculate a tight crop region for the preview stream only
+        float previewRatio = static_cast<float>(previewWidth) / previewHeight;
+
+        /* Ensure that the width/height never go out of bounds
+         * by scaling across a diffent dimension if an out-of-bounds
+         * possibility exists.
+         *
+         * e.g. if the previewratio < arrayratio and e.g. zoomratio = 1.0, then by
+         * calculating the zoomWidth from zoomHeight we'll actually get a
+         * zoomheight > arrayheight
+         */
+        float arrayRatio = 1.f * fastInfo.arrayWidth / fastInfo.arrayHeight;
+        if (previewRatio >= arrayRatio) {
+            // Adjust the height based on the width
+            zoomWidth =  fastInfo.arrayWidth / zoomRatio;
+            zoomHeight = zoomWidth *
+                    previewHeight / previewWidth;
+
+        } else {
+            // Adjust the width based on the height
+            zoomHeight = fastInfo.arrayHeight / zoomRatio;
+            zoomWidth = zoomHeight *
+                    previewWidth / previewHeight;
+        }
+    } else {
+        // Calculate the global crop region with a shape matching the active
+        // array.
+        zoomWidth = fastInfo.arrayWidth / zoomRatio;
+        zoomHeight = fastInfo.arrayHeight / zoomRatio;
+    }
+
+    // center the zoom area within the active area
+    zoomLeft = (fastInfo.arrayWidth - zoomWidth) / 2;
+    zoomTop = (fastInfo.arrayHeight - zoomHeight) / 2;
+
+    ALOGV("Crop region calculated (x=%d,y=%d,w=%f,h=%f) for zoom=%d",
+        (int32_t)zoomLeft, (int32_t)zoomTop, zoomWidth, zoomHeight, this->zoom);
+
+    CropRegion crop = { zoomLeft, zoomTop, zoomWidth, zoomHeight };
+    return crop;
+}
+
+status_t Parameters::calculatePictureFovs(float *horizFov, float *vertFov)
+        const {
+    camera_metadata_ro_entry_t sensorSize =
+            staticInfo(ANDROID_SENSOR_INFO_PHYSICAL_SIZE, 2, 2);
+    if (!sensorSize.count) return NO_INIT;
+
+    camera_metadata_ro_entry_t pixelArraySize =
+            staticInfo(ANDROID_SENSOR_INFO_PIXEL_ARRAY_SIZE, 2, 2);
+    if (!pixelArraySize.count) return NO_INIT;
+
+    float arrayAspect = static_cast<float>(fastInfo.arrayWidth) /
+            fastInfo.arrayHeight;
+    float stillAspect = static_cast<float>(pictureWidth) / pictureHeight;
+    ALOGV("Array aspect: %f, still aspect: %f", arrayAspect, stillAspect);
+
+    // The crop factors from the full sensor array to the still picture crop
+    // region
+    float horizCropFactor = 1.f;
+    float vertCropFactor = 1.f;
+
+    /**
+     * Need to calculate the still image field of view based on the total pixel
+     * array field of view, and the relative aspect ratios of the pixel array
+     * and output streams.
+     *
+     * Special treatment for quirky definition of crop region and relative
+     * stream cropping.
+     */
+    if (quirks.meteringCropRegion) {
+        // Use max of preview and video as first crop
+        float previewAspect = static_cast<float>(previewWidth) / previewHeight;
+        float videoAspect = static_cast<float>(videoWidth) / videoHeight;
+        if (videoAspect > previewAspect) {
+            previewAspect = videoAspect;
+        }
+        // First crop sensor to preview aspect ratio
+        if (arrayAspect < previewAspect) {
+            vertCropFactor = arrayAspect / previewAspect;
+        } else {
+            horizCropFactor = previewAspect / arrayAspect;
+        }
+        // Second crop to still aspect ratio
+        if (stillAspect < previewAspect) {
+            horizCropFactor *= stillAspect / previewAspect;
+        } else {
+            vertCropFactor *= previewAspect / stillAspect;
+        }
+    } else {
+        /**
+         * Crop are just a function of just the still/array relative aspect
+         * ratios. Since each stream will maximize its area within the crop
+         * region, and for FOV we assume a full-sensor crop region, we only ever
+         * crop the FOV either vertically or horizontally, never both.
+         */
+        horizCropFactor = (arrayAspect > stillAspect) ?
+                (stillAspect / arrayAspect) : 1.f;
+        vertCropFactor = (arrayAspect < stillAspect) ?
+                (arrayAspect / stillAspect) : 1.f;
+    }
+
+    /**
+     * Convert the crop factors w.r.t the active array size to the crop factors
+     * w.r.t the pixel array size.
+     */
+    horizCropFactor *= (static_cast<float>(fastInfo.arrayWidth) /
+                            pixelArraySize.data.i32[0]);
+    vertCropFactor *= (static_cast<float>(fastInfo.arrayHeight) /
+                            pixelArraySize.data.i32[1]);
+
+    ALOGV("Horiz crop factor: %f, vert crop fact: %f",
+            horizCropFactor, vertCropFactor);
+    /**
+     * Basic field of view formula is:
+     *   angle of view = 2 * arctangent ( d / 2f )
+     * where d is the physical sensor dimension of interest, and f is
+     * the focal length. This only applies to rectilinear sensors, for focusing
+     * at distances >> f, etc.
+     */
+    if (horizFov != NULL) {
+        *horizFov = 180 / M_PI * 2 *
+                atanf(horizCropFactor * sensorSize.data.f[0] /
+                        (2 * fastInfo.minFocalLength));
+    }
+    if (vertFov != NULL) {
+        *vertFov = 180 / M_PI * 2 *
+                atanf(vertCropFactor * sensorSize.data.f[1] /
+                        (2 * fastInfo.minFocalLength));
+    }
+    return OK;
+}
+
+int32_t Parameters::fpsFromRange(int32_t /*min*/, int32_t max) const {
+    return max;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/Parameters.h b/services/camera/libcameraservice/api1/qticlient2/Parameters.h
new file mode 100644
index 000000000..9c7a02167
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/Parameters.h
@@ -0,0 +1,480 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2PARAMETERS_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2PARAMETERS_H
+
+#include <system/graphics.h>
+
+#include <utils/Compat.h>
+#include <utils/Errors.h>
+#include <utils/KeyedVector.h>
+#include <utils/Mutex.h>
+#include <utils/String8.h>
+#include <utils/Vector.h>
+
+#include "QTIParameters.h"
+
+#include <camera/CameraParameters.h>
+#include <camera/CameraParameters2.h>
+#include <camera/CameraMetadata.h>
+
+namespace android {
+namespace camera2 {
+
+/**
+ * Current camera state; this is the full state of the Camera under the old
+ * camera API (contents of the CameraParameters2 object in a more-efficient
+ * format, plus other state). The enum values are mostly based off the
+ * corresponding camera2 enums, not the camera1 strings. A few are defined here
+ * if they don't cleanly map to camera2 values.
+ */
+struct Parameters {
+    /**
+     * Parameters and other state
+     */
+    int cameraId;
+    int cameraFacing;
+
+    int previewWidth, previewHeight;
+    int32_t previewFpsRange[2];
+    int previewFormat;
+
+    int previewTransform; // set by CAMERA_CMD_SET_DISPLAY_ORIENTATION
+
+    int pictureWidth, pictureHeight;
+    // Store the picture size before they are overriden by video snapshot
+    int pictureWidthLastSet, pictureHeightLastSet;
+    bool pictureSizeOverriden;
+
+    int32_t jpegThumbSize[2];
+    uint8_t jpegQuality, jpegThumbQuality;
+    int32_t jpegRotation;
+
+    bool gpsEnabled;
+    double gpsCoordinates[3];
+    int64_t gpsTimestamp;
+    String8 gpsProcessingMethod;
+
+    uint8_t wbMode;
+    uint8_t effectMode;
+    uint8_t antibandingMode;
+    uint8_t sceneMode;
+
+    enum flashMode_t {
+        FLASH_MODE_OFF = 0,
+        FLASH_MODE_AUTO,
+        FLASH_MODE_ON,
+        FLASH_MODE_TORCH,
+        FLASH_MODE_RED_EYE = ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE,
+        FLASH_MODE_INVALID = -1
+    } flashMode;
+
+    enum focusMode_t {
+        FOCUS_MODE_AUTO = ANDROID_CONTROL_AF_MODE_AUTO,
+        FOCUS_MODE_MACRO = ANDROID_CONTROL_AF_MODE_MACRO,
+        FOCUS_MODE_CONTINUOUS_VIDEO = ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO,
+        FOCUS_MODE_CONTINUOUS_PICTURE = ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE,
+        FOCUS_MODE_EDOF = ANDROID_CONTROL_AF_MODE_EDOF,
+        FOCUS_MODE_INFINITY,
+        FOCUS_MODE_FIXED,
+        FOCUS_MODE_INVALID = -1
+    } focusMode;
+
+    uint8_t focusState; // Latest focus state from HAL
+
+    // For use with triggerAfWithAuto quirk
+    focusMode_t shadowFocusMode;
+
+    struct Area {
+        int left, top, right, bottom;
+        int weight;
+        Area() {}
+        Area(int left, int top, int right, int bottom, int weight):
+                left(left), top(top), right(right), bottom(bottom),
+                weight(weight) {}
+        bool isEmpty() const {
+            return (left == 0) && (top == 0) && (right == 0) && (bottom == 0);
+        }
+    };
+    Vector<Area> focusingAreas;
+
+    struct Size {
+        int32_t width;
+        int32_t height;
+    };
+
+    struct FpsRange {
+        int32_t low;
+        int32_t high;
+    };
+
+    int32_t exposureCompensation;
+    bool autoExposureLock;
+    bool autoExposureLockAvailable;
+    bool autoWhiteBalanceLock;
+    bool autoWhiteBalanceLockAvailable;
+
+    // 3A region types, for use with ANDROID_CONTROL_MAX_REGIONS
+    enum region_t {
+        REGION_AE = 0,
+        REGION_AWB,
+        REGION_AF,
+        NUM_REGION // Number of region types
+    } region;
+
+    Vector<Area> meteringAreas;
+
+    int zoom;
+    bool zoomAvailable;
+
+    int videoWidth, videoHeight, videoFormat;
+    android_dataspace videoDataSpace;
+
+    bool recordingHint;
+    bool videoStabilization;
+
+    CameraParameters2 params;
+    String8 paramsFlattened;
+
+    // These parameters are also part of the camera API-visible state, but not
+    // directly listed in Camera.Parameters
+    // One of ICamera::VIDEO_BUFFER_MODE_*
+    int32_t videoBufferMode;
+    bool playShutterSound;
+    bool enableFaceDetect;
+
+    bool enableFocusMoveMessages;
+    int afTriggerCounter;
+    int afStateCounter;
+    int currentAfTriggerId;
+    bool afInMotion;
+
+    int precaptureTriggerCounter;
+
+    int takePictureCounter;
+
+    uint32_t previewCallbackFlags;
+    bool previewCallbackOneShot;
+    bool previewCallbackSurface;
+
+    bool allowZslMode;
+    // Whether the jpeg stream is slower than 30FPS and can slow down preview.
+    // When slowJpegMode is true, allowZslMode must be false to avoid slowing down preview.
+    bool slowJpegMode;
+    // Whether ZSL reprocess is supported by the device.
+    bool isZslReprocessPresent;
+
+    // Overall camera state
+    enum State {
+        DISCONNECTED,
+        STOPPED,
+        WAITING_FOR_PREVIEW_WINDOW,
+        PREVIEW,
+        RECORD,
+        STILL_CAPTURE,
+        VIDEO_SNAPSHOT
+    } state;
+
+    // Number of zoom steps to simulate
+    static const unsigned int NUM_ZOOM_STEPS = 100;
+    // Max preview size allowed
+    // This is set to a 1:1 value to allow for any aspect ratio that has
+    // a max long side of 1920 pixels
+    static const unsigned int MAX_PREVIEW_WIDTH = 1920;
+    static const unsigned int MAX_PREVIEW_HEIGHT = 1920;
+    // Initial max preview/recording size bound
+    static const int MAX_INITIAL_PREVIEW_WIDTH = 1920;
+    static const int MAX_INITIAL_PREVIEW_HEIGHT = 1080;
+    // Aspect ratio tolerance
+    static const CONSTEXPR float ASPECT_RATIO_TOLERANCE = 0.001;
+    // Threshold for slow jpeg mode
+    static const int64_t kSlowJpegModeThreshold = 33400000LL; // 33.4 ms
+    // Margin for checking FPS
+    static const int32_t FPS_MARGIN = 1;
+    // Max FPS for default parameters
+    static const int32_t MAX_DEFAULT_FPS = 30;
+
+    // Full static camera info, object owned by someone else, such as
+    // Camera2Device.
+    const CameraMetadata *info;
+
+    // Fast-access static device information; this is a subset of the
+    // information available through the staticInfo() method, used for
+    // frequently-accessed values or values that have to be calculated from the
+    // static information.
+    struct DeviceInfo {
+        int32_t arrayWidth;
+        int32_t arrayHeight;
+        int32_t bestStillCaptureFpsRange[2];
+        uint8_t bestFaceDetectMode;
+        int32_t maxFaces;
+        struct OverrideModes {
+            flashMode_t flashMode;
+            uint8_t     wbMode;
+            focusMode_t focusMode;
+            OverrideModes():
+                    flashMode(FLASH_MODE_INVALID),
+                    wbMode(ANDROID_CONTROL_AWB_MODE_OFF),
+                    focusMode(FOCUS_MODE_INVALID) {
+            }
+        };
+        DefaultKeyedVector<uint8_t, OverrideModes> sceneModeOverrides;
+        float minFocalLength;
+        bool useFlexibleYuv;
+        Size maxJpegSize;
+    } fastInfo;
+
+    // Quirks information; these are short-lived flags to enable workarounds for
+    // incomplete HAL implementations
+    struct Quirks {
+        bool triggerAfWithAuto;
+        bool useZslFormat;
+        bool meteringCropRegion;
+        bool partialResults;
+    } quirks;
+
+    /**
+     * Parameter manipulation and setup methods
+     */
+
+    sp<QTIParameters> qtiParams;
+    Parameters(int cameraId, int cameraFacing);
+    ~Parameters();
+
+    // Sets up default parameters
+    status_t initialize(const CameraMetadata *info, int deviceVersion, sp<CameraProviderManager> manager, sp<CameraDeviceBase> mDevice);
+
+    // Build fast-access device static info from static info
+    status_t buildFastInfo();
+    // Query for quirks from static info
+    status_t buildQuirks();
+
+    // Get entry from camera static characteristics information. min/maxCount
+    // are used for error checking the number of values in the entry. 0 for
+    // max/minCount means to do no bounds check in that direction. In case of
+    // error, the entry data pointer is null and the count is 0.
+    camera_metadata_ro_entry_t staticInfo(uint32_t tag,
+            size_t minCount=0, size_t maxCount=0, bool required=true) const;
+
+    // Validate and update camera parameters based on new settings
+    status_t set(const String8 &paramString);
+
+    // Retrieve the current settings
+    String8 get() const;
+
+    // Update passed-in request for common parameters
+    status_t updateRequest(CameraMetadata *request) const;
+
+    // Add/update JPEG entries in metadata
+    status_t updateRequestJpeg(CameraMetadata *request) const;
+
+    /* Helper functions to override jpeg size for video snapshot */
+    // Override jpeg size by video size. Called during startRecording.
+    status_t overrideJpegSizeByVideoSize();
+    // Recover overridden jpeg size.  Called during stopRecording.
+    status_t recoverOverriddenJpegSize();
+    // if video snapshot size is currently overridden
+    bool isJpegSizeOverridden();
+    // whether zero shutter lag should be used for non-recording operation
+    bool useZeroShutterLag() const;
+
+    // Calculate the crop region rectangle, either tightly about the preview
+    // resolution, or a region just based on the active array; both take
+    // into account the current zoom level.
+    struct CropRegion {
+        float left;
+        float top;
+        float width;
+        float height;
+    };
+    CropRegion calculateCropRegion(bool previewOnly) const;
+
+    // Calculate the field of view of the high-resolution JPEG capture
+    status_t calculatePictureFovs(float *horizFov, float *vertFov) const;
+
+    // Static methods for debugging and converting between camera1 and camera2
+    // parameters
+
+    static const char *getStateName(State state);
+
+    static int formatStringToEnum(const char *format);
+    static const char *formatEnumToString(int format);
+
+    static int wbModeStringToEnum(const char *wbMode);
+    static const char* wbModeEnumToString(uint8_t wbMode);
+    static int effectModeStringToEnum(const char *effectMode);
+    static int abModeStringToEnum(const char *abMode);
+    static int sceneModeStringToEnum(const char *sceneMode);
+    static flashMode_t flashModeStringToEnum(const char *flashMode);
+    static const char* flashModeEnumToString(flashMode_t flashMode);
+    static focusMode_t focusModeStringToEnum(const char *focusMode);
+    static const char* focusModeEnumToString(focusMode_t focusMode);
+
+    static status_t parseAreas(const char *areasCStr,
+            Vector<Area> *areas);
+
+    enum AreaKind
+    {
+        AREA_KIND_FOCUS,
+        AREA_KIND_METERING
+    };
+    status_t validateAreas(const Vector<Area> &areas,
+                                  size_t maxRegions,
+                                  AreaKind areaKind) const;
+    static bool boolFromString(const char *boolStr);
+
+    // Map from camera orientation + facing to gralloc transform enum
+    static int degToTransform(int degrees, bool mirror);
+
+    // API specifies FPS ranges are done in fixed point integer, with LSB = 0.001.
+    // Note that this doesn't apply to the (deprecated) single FPS value.
+    static const int kFpsToApiScale = 1000;
+
+    // Transform from (-1000,-1000)-(1000,1000) normalized coords from camera
+    // API to HAL3 (0,0)-(activePixelArray.width/height) coordinates
+    int normalizedXToArray(int x) const;
+    int normalizedYToArray(int y) const;
+
+    // Transform from HAL3 (0,0)-(activePixelArray.width/height) coordinates to
+    // (-1000,-1000)-(1000,1000) normalized coordinates given a scaler crop
+    // region.
+    int arrayXToNormalizedWithCrop(int x, const CropRegion &scalerCrop) const;
+    int arrayYToNormalizedWithCrop(int y, const CropRegion &scalerCrop) const;
+
+    struct Range {
+        int min;
+        int max;
+    };
+
+    int32_t fpsFromRange(int32_t min, int32_t max) const;
+
+private:
+
+    // Convert from viewfinder crop-region relative array coordinates
+    // to HAL3 sensor array coordinates
+    int cropXToArray(int x) const;
+    int cropYToArray(int y) const;
+
+    // Convert from camera API (-1000,1000)-(1000,1000) normalized coords
+    // to viewfinder crop-region relative array coordinates
+    int normalizedXToCrop(int x) const;
+    int normalizedYToCrop(int y) const;
+
+    // Given a scaler crop region, calculate preview crop region based on
+    // preview aspect ratio.
+    CropRegion calculatePreviewCrop(const CropRegion &scalerCrop) const;
+
+    Vector<Size> availablePreviewSizes;
+    Vector<Size> availableVideoSizes;
+    // Get size list (that are no larger than limit) from static metadata.
+    status_t getFilteredSizes(Size limit, Vector<Size> *sizes);
+    // Get max size (from the size array) that matches the given aspect ratio.
+    Size getMaxSizeForRatio(float ratio, const int32_t* sizeArray, size_t count);
+
+    // Helper function for overriding jpeg size for video snapshot
+    // Check if overridden jpeg size needs to be updated after Parameters::set.
+    // The behavior of this function is tailored to the implementation of Parameters::set.
+    // Do not use this function for other purpose.
+    status_t updateOverriddenJpegSize();
+
+    struct StreamConfiguration {
+        int32_t format;
+        int32_t width;
+        int32_t height;
+        int32_t isInput;
+    };
+
+    // Helper function extract available stream configuration
+    // Only valid since device HAL version 3.2
+    // returns an empty Vector if device HAL version does support it
+    Vector<StreamConfiguration> getStreamConfigurations();
+
+    // Helper function to get minimum frame duration for a jpeg size
+    // return -1 if input jpeg size cannot be found in supported size list
+    int64_t getJpegStreamMinFrameDurationNs(Parameters::Size size);
+
+    // Helper function to get minimum frame duration for a size/format combination
+    // return -1 if input size/format combination cannot be found.
+    int64_t getMinFrameDurationNs(Parameters::Size size, int format);
+
+    // Helper function to check if a given fps is supported by all the sizes with
+    // the same format.
+    // return true if the device doesn't support min frame duration metadata tag.
+    bool isFpsSupported(const Vector<Size> &size, int format, int32_t fps);
+
+    // Helper function to get non-duplicated available output formats
+    SortedVector<int32_t> getAvailableOutputFormats();
+    // Helper function to get available output jpeg sizes
+    Vector<Size> getAvailableJpegSizes();
+    // Helper function to get maximum size in input Size vector.
+    // The maximum size is defined by comparing width first, when width ties comparing height.
+    Size getMaxSize(const Vector<Size>& sizes);
+
+    int mDeviceVersion;
+};
+
+// This class encapsulates the Parameters class so that it can only be accessed
+// by constructing a Lock object, which locks the SharedParameter's mutex.
+class SharedParameters {
+  public:
+    SharedParameters(int cameraId, int cameraFacing):
+            mParameters(cameraId, cameraFacing) {
+    }
+
+    template<typename S, typename P>
+    class BaseLock {
+      public:
+        explicit BaseLock(S &p):
+                mParameters(p.mParameters),
+                mSharedParameters(p) {
+            mSharedParameters.mLock.lock();
+        }
+
+        ~BaseLock() {
+            mSharedParameters.mLock.unlock();
+        }
+        P &mParameters;
+      private:
+        // Disallow copying, default construction
+        BaseLock();
+        BaseLock(const BaseLock &);
+        BaseLock &operator=(const BaseLock &);
+        S &mSharedParameters;
+    };
+    typedef BaseLock<SharedParameters, Parameters> Lock;
+    typedef BaseLock<const SharedParameters, const Parameters> ReadLock;
+
+    // Access static info, read-only and immutable, so no lock needed
+    camera_metadata_ro_entry_t staticInfo(uint32_t tag,
+            size_t minCount=0, size_t maxCount=0) const {
+        return mParameters.staticInfo(tag, minCount, maxCount);
+    }
+
+    // Only use for dumping or other debugging
+    const Parameters &unsafeAccess() {
+        return mParameters;
+    }
+  private:
+    Parameters mParameters;
+    mutable Mutex mLock;
+};
+
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/QTIParameters.cpp b/services/camera/libcameraservice/api1/qticlient2/QTIParameters.cpp
new file mode 100644
index 000000000..4c86f6061
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/QTIParameters.cpp
@@ -0,0 +1,746 @@
+/* Copyright (c) 2017, The Linux Foundation. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *     * Neither the name of The Linux Foundation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+#define LOG_TAG "Camera2-QTIParameters"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+#define LOG_NDEBUG 0
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <utils/Vector.h>
+#include <utils/SortedVector.h>
+
+#include <math.h>
+#include <stdlib.h>
+#include <cutils/properties.h>
+
+#include "QTIParameters.h"
+#include "Parameters.h"
+#include "system/camera.h"
+#include "hardware/camera_common.h"
+#include <android/hardware/ICamera.h>
+#include <media/MediaProfiles.h>
+#include <media/mediarecorder.h>
+#include "api1/Camera2Client.h"
+
+namespace android {
+namespace camera2 {
+
+//Sharpness
+const char KEY_QTI_VENDOR_SHARPNESS_RANGE[] = "org.codeaurora.qcamera3.sharpness.range";
+const char KEY_QTI_VENDOR_SHARPNESS_STRENGTH[] = "org.codeaurora.qcamera3.sharpness.strength";
+const char KEY_QTI_MAX_SHARPNESS[] = "max-sharpness";
+const char KEY_QTI_SHARPNESS[] = "sharpness";
+
+//saturation
+const char KEY_QTI_VENDOR_SATURATION_RANGE[] = "org.codeaurora.qcamera3.saturation.range";
+const char KEY_QTI_VENDOR_SATURATION[] = "org.codeaurora.qcamera3.saturation.use_saturation";
+const char KEY_QTI_MAX_SATURATION[] = "max-saturation";
+const char KEY_QTI_SATURATION[] = "saturation";
+
+//instant aec
+const char KEY_QTI_VENDOR_INSTANT_MODE[] = "org.codeaurora.qcamera3.instant_aec.instant_aec_mode";
+const char KEY_QTI_VENDOR_INSTANT_MODES[] =
+        "org.codeaurora.qcamera3.instant_aec.instant_aec_available_modes";
+const char KEY_QTI_INSTANT_AEC_SUPPORTED_MODES[] = "instant-aec-values";
+const char KEY_QTI_INSTANT_AEC[] = "instant-aec";
+// Values for instant AEC modes
+const char KEY_QTI_INSTANT_AEC_DISABLE[] = "0";
+const char KEY_QTI_INSTANT_AEC_AGGRESSIVE_AEC[] = "1";
+const char KEY_QTI_INSTANT_AEC_FAST_AEC[] = "2";
+
+//exposure metering
+const char KEY_QTI_VENDOR_EXPOSURE_METER_MODES[] =
+        "org.codeaurora.qcamera3.exposure_metering.available_modes";
+const char KEY_QTI_VENDOR_EXPOSURE_METER[] =
+        "org.codeaurora.qcamera3.exposure_metering.exposure_metering_mode";
+const char KEY_QTI_AUTO_EXPOSURE_VALUES[] = "auto-exposure-values";
+const char KEY_QTI_AUTO_EXPOSURE[] = "auto-exposure";
+//values for exposure metering
+const char AUTO_EXPOSURE_FRAME_AVG[] = "frame-average";
+const char AUTO_EXPOSURE_CENTER_WEIGHTED[] = "center-weighted";
+const char AUTO_EXPOSURE_SPOT_METERING[] = "spot-metering";
+const char AUTO_EXPOSURE_SMART_METERING[] = "smart-metering";
+const char AUTO_EXPOSURE_USER_METERING[] = "user-metering";
+const char AUTO_EXPOSURE_SPOT_METERING_ADV[] = "spot-metering-adv";
+const char AUTO_EXPOSURE_CENTER_WEIGHTED_ADV[] = "center-weighted-adv";
+
+//iso-exp priority
+const char KEY_QTI_VENDOR_ISO_EXP_SELECT_PRIORITY[]  =
+        "org.codeaurora.qcamera3.iso_exp_priority.select_priority";
+const char KEY_QTI_VENDOR_ISO_EXP_USE_VALUE[]  =
+        "org.codeaurora.qcamera3.iso_exp_priority.use_iso_exp_priority";
+//Manual Exposure
+const char KEY_QTI_SUPPORTED_MANUAL_EXPOSURE_MODES[] = "manual-exposure-modes";
+const char KEY_QTI_EXP_TIME_PRIORITY[] = "exp-time-priority";
+const char KEY_QTI_MIN_EXPOSURE_TIME[] = "min-exposure-time";
+const char KEY_QTI_MAX_EXPOSURE_TIME[] = "max-exposure-time";
+const char KEY_QTI_EXPOSURE_TIME[] = "exposure-time";
+const char KEY_QTI_USER_SETTING[] = "user-setting";
+const char KEY_QTI_MIN_ISO[] = "min-iso";
+const char KEY_QTI_MAX_ISO[] = "max-iso";
+const char KEY_QTI_ISO_PRIORITY[] = "iso-priority";
+const char KEY_QTI_SUPPORTED_ISO_MODES[] = "iso-values";
+const char KEY_QTI_ISO_MODE[] = "iso";
+const char ISO_MANUAL[] = "manual";
+const char KEY_QTI_CONTINUOUS_ISO[] = "continuous-iso";
+// Values for ISO Settings
+const char ISO_AUTO[] = "auto";
+const char ISO_100[] = "ISO100";
+const char ISO_200[] = "ISO200";
+const char ISO_400[] = "ISO400";
+const char ISO_800[] = "ISO800";
+const char ISO_1600[] = "ISO1600";
+const char ISO_3200[] = "ISO3200";
+const char VALUE_OFF[] = "off";
+const char VALUE_ON[] = "on";
+
+//Manual White Balance
+const char KEY_QTI_WB_CCT_MODE[] = "color-temperature";
+const char KEY_QTI_WB_GAIN_MODE[] = "rbgb-gains";
+const char KEY_QTI_MIN_WB_CCT[] = "min-wb-cct";
+const char KEY_QTI_MAX_WB_CCT[] = "max-wb-cct";
+const char KEY_QTI_MIN_WB_GAIN[] = "min-wb-gain";
+const char KEY_QTI_MAX_WB_GAIN[] = "max-wb-gain";
+const char KEY_QTI_SUPPORTED_MANUAL_WB_MODES[] = "manual-wb-modes";
+const char KEY_WHITE_BALANCE[] = "whitebalance";
+const char WHITE_BALANCE_MANUAL[] = "manual";
+const char KEY_QTI_MANUAL_WB_TYPE[] = "manual-wb-type";
+const char KEY_QTI_MANUAL_WB_VALUE[] = "manual-wb-value";
+const char KEY_QTI_MANUAL_WB_GAINS[] = "manual-wb-gains";
+
+//redeye-reduction
+const char KEY_QTI_REDEYE_REDUCTION[] = "redeye-reduction";
+//face-detection
+const char  KEY_QTI_FACE_DETECTION_MODES[] = "face-detection-values";
+
+camera_metadata_ro_entry_t g_availableSensitivityRange;
+double minExposureTime;
+double maxExposureTime;
+const char minWbGain[] = "1.0";
+const char maxWbGain[] = "4.0";
+
+
+status_t QTIParameters::initialize(void *parametersParent,
+        sp<CameraDeviceBase> device, sp<CameraProviderManager> manager) {
+    status_t res = OK;
+
+    Parameters* ParentParams = (Parameters*)parametersParent;
+    mVendorTagId = manager->getProviderTagIdLocked(device->getId().string());
+    sp<VendorTagDescriptor> vTags =
+        VendorTagDescriptor::getGlobalVendorTagDescriptor();
+    if ((nullptr == vTags.get()) || (0 >= vTags->getTagCount())) {
+        sp<VendorTagDescriptorCache> cache =
+                VendorTagDescriptorCache::getGlobalVendorTagCache();
+        if (cache.get()) {
+            cache->getVendorTagDescriptor(mVendorTagId, &vTags);
+        }
+    }
+    uint32_t tag = 0;
+    isoValue = -1;
+    exposureTime = -1;
+
+    // Temp Initialize
+    ParentParams->params.set("max-contrast", 10);
+
+    ParentParams->params.set("redeye-reduction-values",
+            "disable,enable");
+
+    ParentParams->params.set(KEY_QTI_REDEYE_REDUCTION,
+            "disable");
+
+    ParentParams->params.set("num-snaps-per-shutter", 1);
+
+    ParentParams->params.set("ae-bracket-hdr-values","Off,AE-Bracket");
+    ParentParams->params.set("ae-bracket-hdr","Off");
+
+    // ISO
+    // Get the supported sensitivity range from device3 static info
+    camera_metadata_ro_entry_t availableSensitivityRange =
+        ParentParams->staticInfo(ANDROID_SENSOR_INFO_SENSITIVITY_RANGE);
+    if (availableSensitivityRange.count == 2) {
+        int32_t isoMin = availableSensitivityRange.data.i32[0];
+        int32_t isoMax = availableSensitivityRange.data.i32[1];
+        g_availableSensitivityRange = availableSensitivityRange;
+
+        String8 supportedIsoModes;
+        supportedIsoModes += ISO_AUTO;
+        if (100 > isoMin && 100 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_100;
+        }
+        if (200 > isoMin && 200 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_200;
+        }
+        if (400 > isoMin && 400 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_400;
+        }
+        if (800 > isoMin && 800 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_800;
+        }
+        if (1600 > isoMin && 1600 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_1600;
+        }
+        if (3200 > isoMin && 3200 <= isoMax) {
+            supportedIsoModes += ",";
+            supportedIsoModes += ISO_3200;
+        }
+        ParentParams->params.set(KEY_QTI_SUPPORTED_ISO_MODES,
+                supportedIsoModes);
+        // Set default value
+        ParentParams->params.set(KEY_QTI_ISO_MODE,
+                ISO_AUTO);
+    }
+
+    //Sharpness
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_SHARPNESS_RANGE, vTags.get(), &tag);
+    camera_metadata_ro_entry_t availableSharpnessRange = ParentParams->staticInfo(tag);
+    if (availableSharpnessRange.count == 2) {
+        ParentParams->params.set(KEY_QTI_MAX_SHARPNESS,availableSharpnessRange.data.i32[1]);
+        //Default value
+        ParentParams->params.set(KEY_QTI_SHARPNESS,availableSharpnessRange.data.i32[1]);
+    }
+
+    //Saturation
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_SATURATION_RANGE, vTags.get(), &tag);
+    camera_metadata_ro_entry_t availableSaturationRange =
+            ParentParams->staticInfo(tag);
+    if (availableSaturationRange.count == 4) {
+        ParentParams->params.set(KEY_QTI_MAX_SATURATION,availableSaturationRange.data.i32[1]);
+        //Default value
+        ParentParams->params.set(KEY_QTI_SATURATION,availableSaturationRange.data.i32[2]);
+    }
+
+    //Exposure Metering
+    tag=0;
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_EXPOSURE_METER_MODES, vTags.get(), &tag);
+    camera_metadata_ro_entry_t availableMeteringModes =
+            ParentParams->staticInfo(tag);
+
+    String8 MeteringModes;
+    for(int meterModes=0;meterModes<(int)availableMeteringModes.count;meterModes++) {
+        if((availableMeteringModes.data.i32[meterModes] < 0) ||
+                (availableMeteringModes.data.i32[meterModes] > 6))
+            continue;
+
+        if(meterModes != 0) {
+            MeteringModes += ",";
+        }
+
+        if(availableMeteringModes.data.i32[meterModes] == 0 ) {
+            MeteringModes += AUTO_EXPOSURE_FRAME_AVG;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 1 ) {
+            MeteringModes += AUTO_EXPOSURE_CENTER_WEIGHTED;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 2 ) {
+            MeteringModes += AUTO_EXPOSURE_SPOT_METERING;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 3 ) {
+            MeteringModes += AUTO_EXPOSURE_SMART_METERING;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 4 ) {
+            MeteringModes += AUTO_EXPOSURE_USER_METERING;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 5 ) {
+            MeteringModes += AUTO_EXPOSURE_SPOT_METERING_ADV;
+        }
+        else if(availableMeteringModes.data.i32[meterModes] == 6 ) {
+            MeteringModes += AUTO_EXPOSURE_CENTER_WEIGHTED_ADV;
+        }
+    }
+
+    ParentParams->params.set(KEY_QTI_AUTO_EXPOSURE_VALUES,
+                    MeteringModes);
+
+    ParentParams->params.set(KEY_QTI_AUTO_EXPOSURE,
+                    AUTO_EXPOSURE_FRAME_AVG);
+
+    //Instant AEC
+    tag=0;
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_INSTANT_MODES, vTags.get(), &tag);
+    camera_metadata_ro_entry_t availableInstantAecModes =
+            ParentParams->staticInfo(tag);
+    String8 instantAecModes;
+    for(int aecModes=0;aecModes<(int)availableInstantAecModes.count;aecModes++) {
+        if((availableInstantAecModes.data.i32[aecModes] < 0) ||
+                (availableInstantAecModes.data.i32[aecModes] > 2))
+            continue;
+
+        if(aecModes != 0) {
+            instantAecModes += ",";
+        }
+
+        if(availableInstantAecModes.data.i32[aecModes] == 0) {
+            instantAecModes += KEY_QTI_INSTANT_AEC_DISABLE;
+        } else if(availableInstantAecModes.data.i32[aecModes] == 1) {
+            instantAecModes += KEY_QTI_INSTANT_AEC_AGGRESSIVE_AEC;
+        } else if(availableInstantAecModes.data.i32[aecModes] == 2) {
+            instantAecModes += KEY_QTI_INSTANT_AEC_FAST_AEC;
+        }
+    }
+    if (availableInstantAecModes.count > 0) {
+        ParentParams->params.set(KEY_QTI_INSTANT_AEC_SUPPORTED_MODES, instantAecModes);
+        //default Instance AEC
+        ParentParams->params.set(KEY_QTI_INSTANT_AEC, KEY_QTI_INSTANT_AEC_DISABLE);
+    }
+
+    //Manual Exposure
+    String8 manualExpModes(VALUE_OFF);
+    manualExpModes += ",";
+    manualExpModes += KEY_QTI_EXP_TIME_PRIORITY;
+    manualExpModes += ",";
+    manualExpModes += KEY_QTI_ISO_PRIORITY;
+    manualExpModes += ",";
+    manualExpModes += KEY_QTI_USER_SETTING;
+
+    if (availableSensitivityRange.count == 2) {
+        ParentParams->params.set(KEY_QTI_MIN_ISO,availableSensitivityRange.data.i32[0]);
+        ParentParams->params.set(KEY_QTI_MAX_ISO,availableSensitivityRange.data.i32[1]);
+    }
+
+    tag=0;
+    camera_metadata_ro_entry_t availableExposureTimeRange =
+            ParentParams->staticInfo(ANDROID_SENSOR_INFO_EXPOSURE_TIME_RANGE);
+    if (availableExposureTimeRange.count == 2) {
+        char expTimeStr[30];
+        //values are in nano sec, convert to milli sec for upper layers
+        minExposureTime = (double) availableExposureTimeRange.data.i64[0] / 1000000.0;
+        maxExposureTime = (double) availableExposureTimeRange.data.i64[1] / 1000000.0;
+        snprintf(expTimeStr, sizeof(expTimeStr), "%f", minExposureTime);
+        ParentParams->params.set(KEY_QTI_MIN_EXPOSURE_TIME,expTimeStr);
+        snprintf(expTimeStr, sizeof(expTimeStr), "%f", maxExposureTime);
+        ParentParams->params.set(KEY_QTI_MAX_EXPOSURE_TIME,expTimeStr);
+        ParentParams->params.set(KEY_QTI_SUPPORTED_MANUAL_EXPOSURE_MODES,manualExpModes.string());
+    }
+
+    //Manual White Balance
+    String8 supportedWbModes;
+    const char *awbModes= ParentParams->params.get(CameraParameters::KEY_SUPPORTED_WHITE_BALANCE);
+    supportedWbModes += WHITE_BALANCE_MANUAL;
+    supportedWbModes += ",";
+    supportedWbModes += awbModes;
+    ParentParams->params.set(CameraParameters::KEY_SUPPORTED_WHITE_BALANCE,
+            supportedWbModes.string());
+
+    String8 manualWbModes(VALUE_OFF);
+    manualWbModes += ",";
+    manualWbModes += KEY_QTI_WB_CCT_MODE;
+    manualWbModes += ",";
+    manualWbModes += KEY_QTI_WB_GAIN_MODE;
+    ParentParams->params.set(KEY_QTI_MIN_WB_CCT,"2000");
+    ParentParams->params.set(KEY_QTI_MAX_WB_CCT,"8000");
+    ParentParams->params.set(KEY_QTI_MIN_WB_GAIN,minWbGain);
+    ParentParams->params.set(KEY_QTI_MAX_WB_GAIN,maxWbGain);
+    ParentParams->params.set(KEY_QTI_SUPPORTED_MANUAL_WB_MODES, manualWbModes.string());
+
+    //Face detection
+    String8 faceDetectionModes(VALUE_OFF);
+    faceDetectionModes += ",";
+    faceDetectionModes += VALUE_ON;
+    ParentParams->params.set(KEY_QTI_FACE_DETECTION_MODES,faceDetectionModes.string());
+
+    return res;
+}
+
+status_t QTIParameters::set(CameraParameters2& newParams) {
+    status_t res = OK;
+    char prop[PROPERTY_VALUE_MAX];
+
+    // ISO
+    const char *isoMode = newParams.get(KEY_QTI_ISO_MODE);
+    if (isoMode) {
+        if (!strcmp(isoMode, ISO_MANUAL)) {
+            const char *str = newParams.get(KEY_QTI_CONTINUOUS_ISO);
+            if (str != NULL) {
+                res = setContinuousISO(str,newParams);
+                if(res !=OK){
+                    return res;
+                }
+            }
+        } else if (!strcmp(isoMode, ISO_100)) {
+            isoValue = 100;
+        } else if (!strcmp(isoMode, ISO_200)) {
+            isoValue = 200;
+        } else if (!strcmp(isoMode, ISO_400)) {
+            isoValue = 400;
+        } else if (!strcmp(isoMode, ISO_800)) {
+            isoValue = 800;
+        } else if (!strcmp(isoMode, ISO_1600)) {
+            isoValue = 1600;
+        } else if (!strcmp(isoMode, ISO_3200)) {
+            isoValue = 3200;
+        } else {
+            isoValue = 0;
+        }
+    }
+
+    //exposure time
+    const char *str = newParams.get(KEY_QTI_EXPOSURE_TIME);
+
+    if (str != NULL) {
+        res = setExposureTime(str,newParams);
+        if(res !=OK){
+            return res;
+        }
+    }
+
+    //Sharpness value
+    const char *sharpness=newParams.get(KEY_QTI_SHARPNESS);
+    if(sharpness != NULL) {
+        sharpnessValue= atoi(sharpness);
+    }
+
+    //Saturation
+    const char *saturation=newParams.get(KEY_QTI_SATURATION);
+    if(saturation != NULL) {
+        saturationValue= atoi(saturation);
+    }
+
+    //Exposure Metering
+    const char *exmeter=newParams.get(KEY_QTI_AUTO_EXPOSURE);
+    if(!strcmp(exmeter,AUTO_EXPOSURE_FRAME_AVG)) {
+        exposureMetering = 0;
+    } else if (!strcmp(exmeter,AUTO_EXPOSURE_CENTER_WEIGHTED)) {
+        exposureMetering = 1;
+    } else if(!strcmp(exmeter,AUTO_EXPOSURE_SPOT_METERING)) {
+        exposureMetering = 2;
+    } else if(!strcmp(exmeter,AUTO_EXPOSURE_SMART_METERING)) {
+        exposureMetering = 3;
+    } else if(!strcmp(exmeter,AUTO_EXPOSURE_USER_METERING)) {
+        exposureMetering = 4;
+    } else if(!strcmp(exmeter,AUTO_EXPOSURE_SPOT_METERING_ADV)) {
+        exposureMetering = 5;
+    } else if(!strcmp(exmeter,AUTO_EXPOSURE_CENTER_WEIGHTED_ADV)) {
+        exposureMetering = 6;
+    }
+
+    //Instant AEC
+    const char *instantAec=newParams.get(KEY_QTI_INSTANT_AEC);
+    if(instantAec != NULL) {
+        instantAecValue= atoi(instantAec);
+    } else {
+        memset(prop, 0, sizeof(prop));
+        property_get("persist.camera.instant.aec", prop, "0");
+        instantAecValue= (int32_t)atoi(prop);
+    }
+
+    //Manual White Balance
+    const char *whiteBalance = newParams.get(KEY_WHITE_BALANCE);
+    if(whiteBalance) {
+        if (!strcmp(whiteBalance, WHITE_BALANCE_MANUAL)) {
+            const char *value = newParams.get(KEY_QTI_MANUAL_WB_VALUE);
+            const char *type = newParams.get(KEY_QTI_MANUAL_WB_TYPE);
+            if ((value != NULL) && (type != NULL)) {
+                newParams.set(KEY_QTI_MANUAL_WB_TYPE, type);
+                newParams.set(KEY_QTI_MANUAL_WB_VALUE, value);
+                int32_t wbType = atoi(type);
+
+                if (wbType == CAM_MANUAL_WB_MODE_GAIN) {
+                    res = setManualWBGains(value,newParams);
+                    if(res != OK) {
+                        return res;
+                    }
+                } else {
+                    res = BAD_VALUE;
+                }
+            }
+        }
+    }
+
+    //redeye-reduction
+    if(!strcmp(newParams.get(KEY_QTI_REDEYE_REDUCTION),"enable")) {
+        flashMode = (flashMode_t)Parameters::FLASH_MODE_RED_EYE;
+        newParams.set(CameraParameters::KEY_FLASH_MODE,flashModeEnumToString(flashMode));
+    }
+    else {
+        flashMode = (flashMode_t)Parameters::FLASH_MODE_INVALID;
+    }
+
+    return res;
+}
+
+const char *QTIParameters::flashModeEnumToString(flashMode_t flashMode) {
+    switch (flashMode) {
+        case FLASH_MODE_RED_EYE:
+            return CameraParameters::FLASH_MODE_RED_EYE;
+        default:
+            ALOGE("%s: Unknown flash mode enum %d",
+                    __FUNCTION__, flashMode);
+            return "unknown";
+    }
+}
+
+
+int QTIParameters::wbModeStringToEnum(const char *wbMode) {
+    return
+        !strcmp(wbMode, WHITE_BALANCE_MANUAL) ?
+            ANDROID_CONTROL_AWB_MODE_OFF :
+        -1;
+}
+
+const char* QTIParameters::wbModeEnumToString(uint8_t wbMode) {
+    switch (wbMode) {
+        case ANDROID_CONTROL_AWB_MODE_OFF:
+            return WHITE_BALANCE_MANUAL;
+        default:
+            ALOGE("%s: Unknown wb mode enum %d",
+                    __FUNCTION__, wbMode);
+            return "unknown";
+    }
+}
+
+status_t QTIParameters::updateRequest(CameraMetadata *request) const {
+    status_t res = OK;
+    uint32_t tag = 0;
+    int64_t isoVal;
+    sp<VendorTagDescriptor> vTags =
+        VendorTagDescriptor::getGlobalVendorTagDescriptor();
+    if ((nullptr == vTags.get()) || (0 >= vTags->getTagCount())) {
+        sp<VendorTagDescriptorCache> cache =
+                VendorTagDescriptorCache::getGlobalVendorTagCache();
+        if (cache.get()) {
+            cache->getVendorTagDescriptor(mVendorTagId, &vTags);
+        }
+    }
+
+    if (!request) {
+       return BAD_VALUE;
+    }
+
+    if (isoValue != -1) {
+        int32_t selectPriority = 0; // 0 for iso, 1 for exp.
+        isoVal = isoValue;
+
+        res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_ISO_EXP_SELECT_PRIORITY,
+                vTags.get(), &tag);
+        res = request->update(tag, &selectPriority, 1);
+        res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_ISO_EXP_USE_VALUE, vTags.get(), &tag);
+        res = request->update(tag, &(isoVal),  1);
+        if (res != OK) {
+            return res;
+        }
+
+        //erase the default value of construct_default_setting.
+        res = request->erase(ANDROID_SENSOR_SENSITIVITY);
+        if (res != OK) {
+            return res;
+        }
+        res = request->erase(ANDROID_SENSOR_EXPOSURE_TIME);
+        if (res != OK) {
+            return res;
+        }
+
+    }
+
+    if (exposureTime > 0) {
+        int32_t selectPriority = 1; // 0 for iso, 1 for exp.
+        res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_ISO_EXP_SELECT_PRIORITY,
+                vTags.get(), &tag);
+        res = request->update(tag, &selectPriority, 1);
+        res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_ISO_EXP_USE_VALUE, vTags.get(), &tag);
+        res = request->update(tag, &(exposureTime),  1);
+        if (res != OK) {
+            return res;
+        }
+
+        //erase the default value of construct_default_setting.
+        res = request->erase(ANDROID_SENSOR_SENSITIVITY);
+        if (res != OK) {
+            return res;
+        }
+        res = request->erase(ANDROID_SENSOR_EXPOSURE_TIME);
+        if (res != OK) {
+            return res;
+        }
+    }
+
+    //Sharpness value
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_SHARPNESS_STRENGTH, vTags.get(), &tag);
+    res = request->update(tag,&sharpnessValue, 1);
+    if (res != OK) {
+        return res;
+    }
+
+    //Saturation value
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_SATURATION, vTags.get(), &tag);
+    res = request->update(tag,&saturationValue, 1);
+    if (res != OK) {
+        return res;
+    }
+
+    //Exposure Metering
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_EXPOSURE_METER, vTags.get(), &tag);
+    res = request->update(tag,&exposureMetering, 1);
+    if (res != OK) {
+        return res;
+    }
+
+    //Instant AEC
+    res = CameraMetadata::getTagFromName(KEY_QTI_VENDOR_INSTANT_MODE, vTags.get(), &tag);
+    res = request->update(tag,&instantAecValue, 1);
+    if (res != OK) {
+        return res;
+    }
+
+    //Color Correction gains
+    res = request->update(ANDROID_COLOR_CORRECTION_GAINS,(float *)&(manualWb.gains),4);
+    if (res != OK) {
+        return res;
+    }
+
+    //redeye-reduction
+    if(flashMode == (flashMode_t)Parameters::FLASH_MODE_RED_EYE) {
+        uint8_t reqFlashMode = ANDROID_FLASH_MODE_OFF;
+        uint8_t reqAeMode = flashMode;
+
+        res = request->update(ANDROID_FLASH_MODE, &reqFlashMode, 1);
+        if (res != OK) return res;
+        res = request->update(ANDROID_CONTROL_AE_MODE, &reqAeMode, 1);
+        if (res != OK) return res;
+    }
+
+    return res;
+}
+
+int32_t QTIParameters::setManualWBGains(const char *gainStr, CameraParameters2& newParams)
+{
+    int32_t res = OK;
+    if (gainStr != NULL) {
+        double rGain,gGain,bGain;
+        res = parseGains(gainStr, rGain, gGain, bGain);
+        if (res != OK) {
+            return res;
+        }
+
+        double minGain = atof(minWbGain);
+        double maxGain = atof(maxWbGain);
+
+        if (rGain >= minGain && rGain <= maxGain &&
+                gGain >= minGain && gGain <= maxGain &&
+                bGain >= minGain && bGain <= maxGain) {
+            newParams.set(KEY_QTI_MANUAL_WB_GAINS, gainStr);
+
+            manualWb.type = CAM_MANUAL_WB_MODE_GAIN;
+            manualWb.gains.rGain = rGain;
+            manualWb.gains.gEvenGain = gGain;
+            manualWb.gains.gOddGain = gGain;
+            manualWb.gains.bGain = bGain;
+            return res;
+        }
+        return BAD_VALUE;
+    }
+    return BAD_VALUE;
+}
+
+int32_t QTIParameters::parseGains(const char *gainStr, double &rGain,
+                                          double &gGain, double &bGain)
+{
+    int32_t res = OK;
+    char *saveptr = NULL;
+    size_t gainsSize = strlen(gainStr) + 1;
+    char* gains = (char*) calloc(1, gainsSize);
+    if (NULL == gains) {
+        ALOGE("No memory for gains");
+        return NO_MEMORY;
+    }
+    strlcpy(gains, gainStr, gainsSize);
+    char *token = strtok_r(gains, ",", &saveptr);
+
+    if (NULL != token) {
+        rGain = (float) atof(token);
+        token = strtok_r(NULL, ",", &saveptr);
+    }
+
+    if (NULL != token) {
+        gGain = (float) atof(token);
+        token = strtok_r(NULL, ",", &saveptr);
+    }
+
+    if (NULL != token) {
+        bGain = (float) atof(token);
+    } else {
+        ALOGE("Malformed string for gains");
+        res = BAD_VALUE;
+    }
+
+    free(gains);
+    return res;
+}
+
+
+int32_t  QTIParameters::setExposureTime(const char *expTimeStr, CameraParameters2& newParams)
+{
+    double expTimeMs = atof(expTimeStr);
+    //input is in milli seconds. Convert to nano sec
+    int64_t expTimeNs = (int64_t)(expTimeMs*1000000L);
+
+    // expTime == 0 means not to use manual exposure time.
+    if ((0 <= expTimeMs) &&
+            ((expTimeMs == 0) ||
+            ((expTimeMs >= (int64_t) minExposureTime) &&
+            (expTimeMs <= (int64_t) maxExposureTime)))) {
+        newParams.set(KEY_QTI_EXPOSURE_TIME, expTimeStr);
+        exposureTime = expTimeNs;
+
+        return OK;
+    }
+    return BAD_VALUE;
+}
+
+int32_t  QTIParameters::setContinuousISO(const char *isoVal, CameraParameters2& newParams)
+{
+    char iso[PROPERTY_VALUE_MAX];
+    int32_t continousIso = 0;
+
+    // Check if continuous ISO is set through setproperty
+    property_get("persist.camera.continuous.iso", iso, "");
+    if (strlen(iso) > 0) {
+        continousIso = atoi(iso);
+    } else {
+        continousIso = atoi(isoVal);
+    }
+
+    if ((continousIso >= 0) &&
+            (continousIso <= g_availableSensitivityRange.data.i32[1])) {
+        newParams.set(KEY_QTI_CONTINUOUS_ISO, isoVal);
+        isoValue = continousIso;
+        return OK;
+    }
+    ALOGE("Invalid iso value: %d", continousIso);
+    return BAD_VALUE;
+}
+
+}; // namespace camera2
+}; // namespace android
+
diff --git a/services/camera/libcameraservice/api1/qticlient2/QTIParameters.h b/services/camera/libcameraservice/api1/qticlient2/QTIParameters.h
new file mode 100644
index 000000000..d8e1f5c55
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/QTIParameters.h
@@ -0,0 +1,120 @@
+/* Copyright (c) 2017, The Linux Foundation. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above
+ *       copyright notice, this list of conditions and the following
+ *       disclaimer in the documentation and/or other materials provided
+ *       with the distribution.
+ *     * Neither the name of The Linux Foundation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
+ * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
+ * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_QTICAMERA2PARAMETERS_H
+#define ANDROID_SERVERS_CAMERA_QTICAMERA2PARAMETERS_H
+
+#include <system/graphics.h>
+#include <utils/RefBase.h>
+
+#include <utils/Compat.h>
+#include <utils/Errors.h>
+#include <utils/KeyedVector.h>
+#include <utils/Mutex.h>
+#include <utils/String8.h>
+#include <utils/Vector.h>
+
+#include <camera/CameraParameters.h>
+#include <camera/CameraParameters2.h>
+#include <camera/CameraMetadata.h>
+#include <camera/VendorTagDescriptor.h>
+#include <CameraService.h>
+
+namespace android {
+
+class CameraDeviceBase;
+
+namespace camera2 {
+
+#define QTIAMERA_MAX_EXP_TIME_LEVEL1      100
+
+typedef enum {
+    CAM_MANUAL_WB_MODE_CCT,
+    CAM_MANUAL_WB_MODE_GAIN,
+    CAM_MANUAL_WB_MODE_MAX
+}cam_manual_wb_mode_type;
+
+typedef struct {
+    float rGain;
+    float gEvenGain;
+    float gOddGain;
+    float bGain;
+} cam_awb_gain_t;
+
+typedef struct {
+    cam_manual_wb_mode_type type;
+    union{
+        int32_t cct;
+        cam_awb_gain_t gains;
+    };
+} cam_manual_wb_parm_t;
+
+struct Parameters;
+
+class QTIParameters: public virtual RefBase{
+    /**
+     * QTI specific parameters and other info
+     */
+private:
+    int32_t isoValue;
+    int32_t sharpnessValue;
+    int32_t saturationValue;
+    int32_t exposureMetering;
+    int32_t instantAecValue;
+    int64_t exposureTime;
+    cam_manual_wb_parm_t manualWb;
+    enum flashMode_t {
+        FLASH_MODE_RED_EYE = ANDROID_CONTROL_AE_MODE_ON_AUTO_FLASH_REDEYE,
+        FLASH_MODE_INVALID = -1
+    } flashMode;
+    metadata_vendor_id_t mVendorTagId;
+
+public:
+    // Sets up default QTI parameters
+    status_t initialize(void *parametersParent, sp<CameraDeviceBase> device, sp<CameraProviderManager> manager);
+    // Validate and update camera parameters based on new settings
+    status_t set(CameraParameters2& newParams);
+    // Update passed-in request for common parameters
+    status_t updateRequest(CameraMetadata *request) const;
+    static const char* wbModeEnumToString(uint8_t wbMode);
+    static int wbModeStringToEnum(const char *wbMode);
+
+private:
+    int32_t setContinuousISO(const char *isoValue, CameraParameters2& newParams);
+    int32_t setExposureTime(const char *expTimeStr, CameraParameters2& newParams);
+    int32_t setManualWBGains(const char *gainStr, CameraParameters2& newParams);
+    int32_t parseGains(const char *gainStr, double &r_gain,
+            double &g_gain, double &b_gain);
+    const char *flashModeEnumToString(flashMode_t flashMode);
+};
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.cpp b/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.cpp
new file mode 100644
index 000000000..a449f1ac0
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.cpp
@@ -0,0 +1,623 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-StreamingProcessor"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+//#define LOG_NNDEBUG 0 // Per-frame verbose logging
+
+#ifdef LOG_NNDEBUG
+#define ALOGVV(...) ALOGV(__VA_ARGS__)
+#else
+#define ALOGVV(...) ((void)0)
+#endif
+
+#include <cutils/properties.h>
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <gui/BufferItem.h>
+#include <gui/Surface.h>
+#include <media/hardware/HardwareAPI.h>
+
+#include "common/CameraDeviceBase.h"
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/StreamingProcessor.h"
+#include "api1/qticlient2/Camera2Heap.h"
+
+namespace android {
+namespace camera2 {
+
+StreamingProcessor::StreamingProcessor(sp<Camera2Client> client):
+        mClient(client),
+        mDevice(client->getCameraDevice()),
+        mId(client->getCameraId()),
+        mActiveRequest(NONE),
+        mPaused(false),
+        mPreviewRequestId(Camera2Client::kPreviewRequestIdStart),
+        mPreviewStreamId(NO_STREAM),
+        mRecordingRequestId(Camera2Client::kRecordingRequestIdStart),
+        mRecordingStreamId(NO_STREAM)
+{
+}
+
+StreamingProcessor::~StreamingProcessor() {
+    deletePreviewStream();
+    deleteRecordingStream();
+}
+
+status_t StreamingProcessor::setPreviewWindow(const sp<Surface>& window) {
+    ATRACE_CALL();
+    status_t res;
+
+    res = deletePreviewStream();
+    if (res != OK) return res;
+
+    Mutex::Autolock m(mMutex);
+
+    mPreviewWindow = window;
+
+    return OK;
+}
+
+status_t StreamingProcessor::setRecordingWindow(const sp<Surface>& window) {
+    ATRACE_CALL();
+    status_t res;
+
+    res = deleteRecordingStream();
+    if (res != OK) return res;
+
+    Mutex::Autolock m(mMutex);
+
+    mRecordingWindow = window;
+
+    return OK;
+}
+
+bool StreamingProcessor::haveValidPreviewWindow() const {
+    Mutex::Autolock m(mMutex);
+    return mPreviewWindow != 0;
+}
+
+bool StreamingProcessor::haveValidRecordingWindow() const {
+    Mutex::Autolock m(mMutex);
+    return mRecordingWindow != nullptr;
+}
+
+status_t StreamingProcessor::updatePreviewRequest(const Parameters &params) {
+    ATRACE_CALL();
+    status_t res;
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    Mutex::Autolock m(mMutex);
+    if (mPreviewRequest.entryCount() == 0) {
+        sp<Camera2Client> client = mClient.promote();
+        if (client == 0) {
+            ALOGE("%s: Camera %d: Client does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        // Use CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG for ZSL streaming case.
+        if (params.useZeroShutterLag() && !params.recordingHint) {
+            res = device->createDefaultRequest(
+                    CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG, &mPreviewRequest);
+        } else {
+            res = device->createDefaultRequest(CAMERA3_TEMPLATE_PREVIEW,
+                    &mPreviewRequest);
+        }
+
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to create default preview request: "
+                    "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+    }
+
+    res = params.updateRequest(&mPreviewRequest);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update common entries of preview "
+                "request: %s (%d)", __FUNCTION__, mId,
+                strerror(-res), res);
+        return res;
+    }
+
+    res = mPreviewRequest.update(ANDROID_REQUEST_ID,
+            &mPreviewRequestId, 1);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update request id for preview: %s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    return OK;
+}
+
+status_t StreamingProcessor::updatePreviewStream(const Parameters &params) {
+    ATRACE_CALL();
+    Mutex::Autolock m(mMutex);
+
+    status_t res;
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if (mPreviewStreamId != NO_STREAM) {
+        // Check if stream parameters have to change
+        uint32_t currentWidth, currentHeight;
+        res = device->getStreamInfo(mPreviewStreamId,
+                &currentWidth, &currentHeight, 0, 0);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Error querying preview stream info: "
+                    "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+        if (currentWidth != (uint32_t)params.previewWidth ||
+                currentHeight != (uint32_t)params.previewHeight) {
+            ALOGV("%s: Camera %d: Preview size switch: %d x %d -> %d x %d",
+                    __FUNCTION__, mId, currentWidth, currentHeight,
+                    params.previewWidth, params.previewHeight);
+            res = device->waitUntilDrained();
+            if (res != OK) {
+                ALOGE("%s: Camera %d: Error waiting for preview to drain: "
+                        "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+                return res;
+            }
+            res = device->deleteStream(mPreviewStreamId);
+            if (res != OK) {
+                ALOGE("%s: Camera %d: Unable to delete old output stream "
+                        "for preview: %s (%d)", __FUNCTION__, mId,
+                        strerror(-res), res);
+                return res;
+            }
+            mPreviewStreamId = NO_STREAM;
+        }
+    }
+
+    if (mPreviewStreamId == NO_STREAM) {
+        res = device->createStream(mPreviewWindow,
+                params.previewWidth, params.previewHeight,
+                CAMERA2_HAL_PIXEL_FORMAT_OPAQUE, HAL_DATASPACE_UNKNOWN,
+                CAMERA3_STREAM_ROTATION_0, &mPreviewStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to create preview stream: %s (%d)",
+                    __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+    }
+
+    res = device->setStreamTransform(mPreviewStreamId,
+            params.previewTransform);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to set preview stream transform: "
+                "%s (%d)", __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    return OK;
+}
+
+status_t StreamingProcessor::deletePreviewStream() {
+    ATRACE_CALL();
+    status_t res;
+
+    Mutex::Autolock m(mMutex);
+
+    if (mPreviewStreamId != NO_STREAM) {
+        sp<CameraDeviceBase> device = mDevice.promote();
+        if (device == 0) {
+            ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        ALOGV("%s: for cameraId %d on streamId %d",
+            __FUNCTION__, mId, mPreviewStreamId);
+
+        res = device->waitUntilDrained();
+        if (res != OK) {
+            ALOGE("%s: Error waiting for preview to drain: %s (%d)",
+                    __FUNCTION__, strerror(-res), res);
+            return res;
+        }
+        res = device->deleteStream(mPreviewStreamId);
+        if (res != OK) {
+            ALOGE("%s: Unable to delete old preview stream: %s (%d)",
+                    __FUNCTION__, strerror(-res), res);
+            return res;
+        }
+        mPreviewStreamId = NO_STREAM;
+    }
+    return OK;
+}
+
+int StreamingProcessor::getPreviewStreamId() const {
+    Mutex::Autolock m(mMutex);
+    return mPreviewStreamId;
+}
+
+status_t StreamingProcessor::updateRecordingRequest(const Parameters &params) {
+    ATRACE_CALL();
+    status_t res;
+    Mutex::Autolock m(mMutex);
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if (mRecordingRequest.entryCount() == 0) {
+        res = device->createDefaultRequest(CAMERA2_TEMPLATE_VIDEO_RECORD,
+                &mRecordingRequest);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to create default recording request:"
+                    " %s (%d)", __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+    }
+
+    res = params.updateRequest(&mRecordingRequest);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update common entries of recording "
+                "request: %s (%d)", __FUNCTION__, mId,
+                strerror(-res), res);
+        return res;
+    }
+
+    res = mRecordingRequest.update(ANDROID_REQUEST_ID,
+            &mRecordingRequestId, 1);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to update request id for request: %s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    return OK;
+}
+
+status_t StreamingProcessor::recordingStreamNeedsUpdate(
+        const Parameters &params, bool *needsUpdate) {
+    status_t res;
+
+    if (needsUpdate == 0) {
+        ALOGE("%s: Camera %d: invalid argument", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if (mRecordingStreamId == NO_STREAM) {
+        *needsUpdate = true;
+        return OK;
+    }
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    uint32_t currentWidth, currentHeight, currentFormat;
+    android_dataspace currentDataSpace;
+    res = device->getStreamInfo(mRecordingStreamId,
+            &currentWidth, &currentHeight, &currentFormat, &currentDataSpace);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Error querying recording output stream info: "
+                "%s (%d)", __FUNCTION__, mId,
+                strerror(-res), res);
+        return res;
+    }
+
+    if (mRecordingWindow == nullptr ||
+            currentWidth != (uint32_t)params.videoWidth ||
+            currentHeight != (uint32_t)params.videoHeight ||
+            currentFormat != (uint32_t)params.videoFormat ||
+            currentDataSpace != params.videoDataSpace) {
+        *needsUpdate = true;
+        return res;
+    }
+    *needsUpdate = false;
+    return res;
+}
+
+status_t StreamingProcessor::updateRecordingStream(const Parameters &params) {
+    ATRACE_CALL();
+    status_t res;
+    Mutex::Autolock m(mMutex);
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if (mRecordingStreamId != NO_STREAM) {
+        // Check if stream parameters have to change
+        uint32_t currentWidth, currentHeight;
+        uint32_t currentFormat;
+        android_dataspace currentDataSpace;
+        res = device->getStreamInfo(mRecordingStreamId,
+                &currentWidth, &currentHeight,
+                &currentFormat, &currentDataSpace);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Error querying recording output stream info: "
+                    "%s (%d)", __FUNCTION__, mId,
+                    strerror(-res), res);
+            return res;
+        }
+        if (currentWidth != (uint32_t)params.videoWidth ||
+                currentHeight != (uint32_t)params.videoHeight ||
+                currentFormat != (uint32_t)params.videoFormat ||
+                currentDataSpace != params.videoDataSpace) {
+            // TODO: Should wait to be sure previous recording has finished
+            res = device->deleteStream(mRecordingStreamId);
+
+            if (res == -EBUSY) {
+                ALOGV("%s: Camera %d: Device is busy, call "
+                      "updateRecordingStream after it becomes idle",
+                      __FUNCTION__, mId);
+                return res;
+            } else if (res != OK) {
+                ALOGE("%s: Camera %d: Unable to delete old output stream "
+                        "for recording: %s (%d)", __FUNCTION__,
+                        mId, strerror(-res), res);
+                return res;
+            }
+            mRecordingStreamId = NO_STREAM;
+        }
+    }
+
+    if (mRecordingStreamId == NO_STREAM) {
+        res = device->createStream(mRecordingWindow,
+                params.videoWidth, params.videoHeight,
+                params.videoFormat, params.videoDataSpace,
+                CAMERA3_STREAM_ROTATION_0, &mRecordingStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't create output stream for recording: "
+                    "%s (%d)", __FUNCTION__, mId,
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    return OK;
+}
+
+status_t StreamingProcessor::deleteRecordingStream() {
+    ATRACE_CALL();
+    status_t res;
+
+    Mutex::Autolock m(mMutex);
+
+    if (mRecordingStreamId != NO_STREAM) {
+        sp<CameraDeviceBase> device = mDevice.promote();
+        if (device == 0) {
+            ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        res = device->waitUntilDrained();
+        if (res != OK) {
+            ALOGE("%s: Error waiting for HAL to drain: %s (%d)",
+                    __FUNCTION__, strerror(-res), res);
+            return res;
+        }
+        res = device->deleteStream(mRecordingStreamId);
+        if (res != OK) {
+            ALOGE("%s: Unable to delete recording stream: %s (%d)",
+                    __FUNCTION__, strerror(-res), res);
+            return res;
+        }
+        mRecordingStreamId = NO_STREAM;
+    }
+    return OK;
+}
+
+int StreamingProcessor::getRecordingStreamId() const {
+    return mRecordingStreamId;
+}
+
+status_t StreamingProcessor::startStream(StreamType type,
+        const Vector<int32_t> &outputStreams) {
+    ATRACE_CALL();
+    status_t res;
+
+    if (type == NONE) return INVALID_OPERATION;
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    ALOGV("%s: Camera %d: type = %d", __FUNCTION__, mId, type);
+
+    Mutex::Autolock m(mMutex);
+
+    CameraMetadata &request = (type == PREVIEW) ?
+            mPreviewRequest : mRecordingRequest;
+
+    res = request.update(
+        ANDROID_REQUEST_OUTPUT_STREAMS,
+        outputStreams);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to set up preview request: %s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    res = request.sort();
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Error sorting preview request: %s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    res = device->setStreamingRequest(request);
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Unable to set preview request to start preview: "
+                "%s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+    mActiveRequest = type;
+    mPaused = false;
+    mActiveStreamIds = outputStreams;
+    return OK;
+}
+
+status_t StreamingProcessor::togglePauseStream(bool pause) {
+    ATRACE_CALL();
+    status_t res;
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    ALOGV("%s: Camera %d: toggling pause to %d", __FUNCTION__, mId, pause);
+
+    Mutex::Autolock m(mMutex);
+
+    if (mActiveRequest == NONE) {
+        ALOGE("%s: Camera %d: Can't toggle pause, streaming was not started",
+              __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if (mPaused == pause) {
+        return OK;
+    }
+
+    if (pause) {
+        res = device->clearStreamingRequest();
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't clear stream request: %s (%d)",
+                    __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+    } else {
+        CameraMetadata &request =
+                (mActiveRequest == PREVIEW) ? mPreviewRequest
+                                            : mRecordingRequest;
+        res = device->setStreamingRequest(request);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to set preview request to resume: "
+                    "%s (%d)",
+                    __FUNCTION__, mId, strerror(-res), res);
+            return res;
+        }
+    }
+
+    mPaused = pause;
+    return OK;
+}
+
+status_t StreamingProcessor::stopStream() {
+    ATRACE_CALL();
+    status_t res;
+
+    Mutex::Autolock m(mMutex);
+
+    sp<CameraDeviceBase> device = mDevice.promote();
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    res = device->clearStreamingRequest();
+    if (res != OK) {
+        ALOGE("%s: Camera %d: Can't clear stream request: %s (%d)",
+                __FUNCTION__, mId, strerror(-res), res);
+        return res;
+    }
+
+    mActiveRequest = NONE;
+    mActiveStreamIds.clear();
+    mPaused = false;
+
+    return OK;
+}
+
+int32_t StreamingProcessor::getActiveRequestId() const {
+    Mutex::Autolock m(mMutex);
+    switch (mActiveRequest) {
+        case NONE:
+            return 0;
+        case PREVIEW:
+            return mPreviewRequestId;
+        case RECORD:
+            return mRecordingRequestId;
+        default:
+            ALOGE("%s: Unexpected mode %d", __FUNCTION__, mActiveRequest);
+            return 0;
+    }
+}
+
+status_t StreamingProcessor::incrementStreamingIds() {
+    ATRACE_CALL();
+    Mutex::Autolock m(mMutex);
+
+    mPreviewRequestId++;
+    if (mPreviewRequestId >= Camera2Client::kPreviewRequestIdEnd) {
+        mPreviewRequestId = Camera2Client::kPreviewRequestIdStart;
+    }
+    mRecordingRequestId++;
+    if (mRecordingRequestId >= Camera2Client::kRecordingRequestIdEnd) {
+        mRecordingRequestId = Camera2Client::kRecordingRequestIdStart;
+    }
+    return OK;
+}
+
+status_t StreamingProcessor::dump(int fd, const Vector<String16>& /*args*/) {
+    String8 result;
+
+    result.append("  Current requests:\n");
+    if (mPreviewRequest.entryCount() != 0) {
+        result.append("    Preview request:\n");
+        write(fd, result.string(), result.size());
+        mPreviewRequest.dump(fd, 2, 6);
+        result.clear();
+    } else {
+        result.append("    Preview request: undefined\n");
+    }
+
+    if (mRecordingRequest.entryCount() != 0) {
+        result = "    Recording request:\n";
+        write(fd, result.string(), result.size());
+        mRecordingRequest.dump(fd, 2, 6);
+        result.clear();
+    } else {
+        result = "    Recording request: undefined\n";
+    }
+
+    const char* streamTypeString[] = {
+        "none", "preview", "record"
+    };
+    result.append(String8::format("   Active request: %s (paused: %s)\n",
+                                  streamTypeString[mActiveRequest],
+                                  mPaused ? "yes" : "no"));
+
+    write(fd, result.string(), result.size());
+
+    return OK;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.h b/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.h
new file mode 100644
index 000000000..22231d557
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/StreamingProcessor.h
@@ -0,0 +1,116 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_STREAMINGPROCESSOR_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_STREAMINGPROCESSOR_H
+
+#include <utils/Mutex.h>
+#include <utils/String16.h>
+#include <gui/BufferItemConsumer.h>
+
+#include "camera/CameraMetadata.h"
+
+namespace android {
+
+class Camera2Client;
+class CameraDeviceBase;
+class IMemory;
+
+namespace camera2 {
+
+struct Parameters;
+class Camera2Heap;
+
+/**
+ * Management and processing for preview and recording streams
+ */
+class StreamingProcessor : public virtual VirtualLightRefBase {
+  public:
+    explicit StreamingProcessor(sp<Camera2Client> client);
+    ~StreamingProcessor();
+
+    status_t setPreviewWindow(const sp<Surface>& window);
+    status_t setRecordingWindow(const sp<Surface>& window);
+
+    bool haveValidPreviewWindow() const;
+    bool haveValidRecordingWindow() const;
+
+    status_t updatePreviewRequest(const Parameters &params);
+    status_t updatePreviewStream(const Parameters &params);
+    status_t deletePreviewStream();
+    int getPreviewStreamId() const;
+
+    status_t updateRecordingRequest(const Parameters &params);
+    // If needsUpdate is set to true, a updateRecordingStream call with params will recreate
+    // recording stream
+    status_t recordingStreamNeedsUpdate(const Parameters &params, bool *needsUpdate);
+    status_t updateRecordingStream(const Parameters &params);
+    status_t deleteRecordingStream();
+    int getRecordingStreamId() const;
+
+    enum StreamType {
+        NONE,
+        PREVIEW,
+        RECORD
+    };
+    status_t startStream(StreamType type,
+            const Vector<int32_t> &outputStreams);
+
+    // Toggle between paused and unpaused. Stream must be started first.
+    status_t togglePauseStream(bool pause);
+
+    status_t stopStream();
+
+    // Returns the request ID for the currently streaming request
+    // Returns 0 if there is no active request.
+    status_t getActiveRequestId() const;
+    status_t incrementStreamingIds();
+
+    status_t dump(int fd, const Vector<String16>& args);
+
+  private:
+    mutable Mutex mMutex;
+
+    enum {
+        NO_STREAM = -1
+    };
+
+    wp<Camera2Client> mClient;
+    wp<CameraDeviceBase> mDevice;
+    int mId;
+
+    StreamType mActiveRequest;
+    bool mPaused;
+
+    Vector<int32_t> mActiveStreamIds;
+
+    // Preview-related members
+    int32_t mPreviewRequestId;
+    int mPreviewStreamId;
+    CameraMetadata mPreviewRequest;
+    sp<Surface> mPreviewWindow;
+
+    int32_t mRecordingRequestId;
+    int mRecordingStreamId;
+    sp<Surface>  mRecordingWindow;
+    CameraMetadata mRecordingRequest;
+};
+
+
+}; // namespace camera2
+}; // namespace android
+
+#endif
diff --git a/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.cpp b/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.cpp
new file mode 100644
index 000000000..0a234232f
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.cpp
@@ -0,0 +1,905 @@
+/*
+ * Copyright (C) 2013 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera2-ZslProcessor"
+#define ATRACE_TAG ATRACE_TAG_CAMERA
+//#define LOG_NDEBUG 0
+//#define LOG_NNDEBUG 0
+
+#ifdef LOG_NNDEBUG
+#define ALOGVV(...) ALOGV(__VA_ARGS__)
+#else
+#define ALOGVV(...) if (0) ALOGV(__VA_ARGS__)
+#endif
+
+#include <inttypes.h>
+
+#include <utils/Log.h>
+#include <utils/Trace.h>
+#include <gui/Surface.h>
+
+#include "common/CameraDeviceBase.h"
+#include "api1/Camera2Client.h"
+#include "api1/qticlient2/CaptureSequencer.h"
+#include "api1/qticlient2/ZslProcessor.h"
+#include "device3/Camera3Device.h"
+
+typedef android::RingBufferConsumer::PinnedBufferItem PinnedBufferItem;
+
+namespace android {
+namespace camera2 {
+
+namespace {
+struct TimestampFinder : public RingBufferConsumer::RingBufferComparator {
+    typedef RingBufferConsumer::BufferInfo BufferInfo;
+
+    enum {
+        SELECT_I1 = -1,
+        SELECT_I2 = 1,
+        SELECT_NEITHER = 0,
+    };
+
+    explicit TimestampFinder(nsecs_t timestamp) : mTimestamp(timestamp) {}
+    ~TimestampFinder() {}
+
+    template <typename T>
+    static void swap(T& a, T& b) {
+        T tmp = a;
+        a = b;
+        b = tmp;
+    }
+
+    /**
+     * Try to find the best candidate for a ZSL buffer.
+     * Match priority from best to worst:
+     *  1) Timestamps match.
+     *  2) Timestamp is closest to the needle (and lower).
+     *  3) Timestamp is closest to the needle (and higher).
+     *
+     */
+    virtual int compare(const BufferInfo *i1,
+                        const BufferInfo *i2) const {
+        // Try to select non-null object first.
+        if (i1 == NULL) {
+            return SELECT_I2;
+        } else if (i2 == NULL) {
+            return SELECT_I1;
+        }
+
+        // Best result: timestamp is identical
+        if (i1->mTimestamp == mTimestamp) {
+            return SELECT_I1;
+        } else if (i2->mTimestamp == mTimestamp) {
+            return SELECT_I2;
+        }
+
+        const BufferInfo* infoPtrs[2] = {
+            i1,
+            i2
+        };
+        int infoSelectors[2] = {
+            SELECT_I1,
+            SELECT_I2
+        };
+
+        // Order i1,i2 so that always i1.timestamp < i2.timestamp
+        if (i1->mTimestamp > i2->mTimestamp) {
+            swap(infoPtrs[0], infoPtrs[1]);
+            swap(infoSelectors[0], infoSelectors[1]);
+        }
+
+        // Second best: closest (lower) timestamp
+        if (infoPtrs[1]->mTimestamp < mTimestamp) {
+            return infoSelectors[1];
+        } else if (infoPtrs[0]->mTimestamp < mTimestamp) {
+            return infoSelectors[0];
+        }
+
+        // Worst: closest (higher) timestamp
+        return infoSelectors[0];
+
+        /**
+         * The above cases should cover all the possibilities,
+         * and we get an 'empty' result only if the ring buffer
+         * was empty itself
+         */
+    }
+
+    const nsecs_t mTimestamp;
+}; // struct TimestampFinder
+} // namespace anonymous
+
+ZslProcessor::ZslProcessor(
+    sp<Camera2Client> client,
+    wp<CaptureSequencer> sequencer):
+        Thread(false),
+        mLatestClearedBufferTimestamp(0),
+        mState(RUNNING),
+        mClient(client),
+        mSequencer(sequencer),
+        mId(client->getCameraId()),
+        mZslStreamId(NO_STREAM),
+        mInputStreamId(NO_STREAM),
+        mFrameListHead(0),
+        mHasFocuser(false),
+        mInputBuffer(nullptr),
+        mProducer(nullptr),
+        mInputProducer(nullptr),
+        mInputProducerSlot(-1) {
+    // Initialize buffer queue and frame list based on pipeline max depth.
+    size_t pipelineMaxDepth = kDefaultMaxPipelineDepth;
+    if (client != 0) {
+        sp<Camera3Device> device =
+        static_cast<Camera3Device*>(client->getCameraDevice().get());
+        if (device != 0) {
+            camera_metadata_ro_entry_t entry =
+                device->info().find(ANDROID_REQUEST_PIPELINE_MAX_DEPTH);
+            if (entry.count == 1) {
+                pipelineMaxDepth = entry.data.u8[0];
+            } else {
+                ALOGW("%s: Unable to find the android.request.pipelineMaxDepth,"
+                        " use default pipeline max depth %d", __FUNCTION__,
+                        kDefaultMaxPipelineDepth);
+            }
+
+            entry = device->info().find(ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE);
+            if (entry.count > 0 && entry.data.f[0] != 0.) {
+                mHasFocuser = true;
+            }
+        }
+    }
+
+    ALOGV("%s: Initialize buffer queue and frame list depth based on max pipeline depth (%zu)",
+          __FUNCTION__, pipelineMaxDepth);
+    // Need to keep buffer queue longer than metadata queue because sometimes buffer arrives
+    // earlier than metadata which causes the buffer corresponding to oldest metadata being
+    // removed.
+    mFrameListDepth = pipelineMaxDepth;
+    mBufferQueueDepth = mFrameListDepth + 1;
+
+    mZslQueue.insertAt(0, mBufferQueueDepth);
+    mFrameList.insertAt(0, mFrameListDepth);
+    sp<CaptureSequencer> captureSequencer = mSequencer.promote();
+    if (captureSequencer != 0) captureSequencer->setZslProcessor(this);
+}
+
+ZslProcessor::~ZslProcessor() {
+    ALOGV("%s: Exit", __FUNCTION__);
+    deleteStream();
+}
+
+void ZslProcessor::onResultAvailable(const CaptureResult &result) {
+    ATRACE_CALL();
+    ALOGV("%s:", __FUNCTION__);
+    Mutex::Autolock l(mInputMutex);
+    camera_metadata_ro_entry_t entry;
+    entry = result.mMetadata.find(ANDROID_SENSOR_TIMESTAMP);
+    nsecs_t timestamp = entry.data.i64[0];
+    if (entry.count == 0) {
+        ALOGE("%s: metadata doesn't have timestamp, skip this result", __FUNCTION__);
+        return;
+    }
+
+    entry = result.mMetadata.find(ANDROID_REQUEST_FRAME_COUNT);
+    if (entry.count == 0) {
+        ALOGE("%s: metadata doesn't have frame number, skip this result", __FUNCTION__);
+        return;
+    }
+    int32_t frameNumber = entry.data.i32[0];
+
+    ALOGVV("Got preview metadata for frame %d with timestamp %" PRId64, frameNumber, timestamp);
+
+    if (mState != RUNNING) return;
+
+    // Corresponding buffer has been cleared. No need to push into mFrameList
+    if (timestamp <= mLatestClearedBufferTimestamp) return;
+
+    mFrameList.editItemAt(mFrameListHead) = result.mMetadata;
+    mFrameListHead = (mFrameListHead + 1) % mFrameListDepth;
+}
+
+status_t ZslProcessor::updateStream(const Parameters &params) {
+    ATRACE_CALL();
+    ALOGV("%s: Configuring ZSL streams", __FUNCTION__);
+    status_t res;
+
+    Mutex::Autolock l(mInputMutex);
+
+    sp<Camera2Client> client = mClient.promote();
+    if (client == 0) {
+        ALOGE("%s: Camera %d: Client does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+    sp<Camera3Device> device =
+        static_cast<Camera3Device*>(client->getCameraDevice().get());
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    if ((mZslStreamId != NO_STREAM) || (mInputStreamId != NO_STREAM)) {
+        // Check if stream parameters have to change
+        uint32_t currentWidth, currentHeight;
+        res = device->getStreamInfo(mZslStreamId,
+                &currentWidth, &currentHeight, 0, 0);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Error querying capture output stream info: "
+                    "%s (%d)", __FUNCTION__,
+                    client->getCameraId(), strerror(-res), res);
+            return res;
+        }
+        if (currentWidth != (uint32_t)params.fastInfo.arrayWidth ||
+                currentHeight != (uint32_t)params.fastInfo.arrayHeight) {
+            if (mZslStreamId != NO_STREAM) {
+                ALOGV("%s: Camera %d: Deleting stream %d since the buffer "
+                      "dimensions changed",
+                    __FUNCTION__, client->getCameraId(), mZslStreamId);
+                res = device->deleteStream(mZslStreamId);
+                if (res == -EBUSY) {
+                    ALOGV("%s: Camera %d: Device is busy, call updateStream again "
+                          " after it becomes idle", __FUNCTION__, mId);
+                    return res;
+                } else if(res != OK) {
+                    ALOGE("%s: Camera %d: Unable to delete old output stream "
+                            "for ZSL: %s (%d)", __FUNCTION__,
+                            client->getCameraId(), strerror(-res), res);
+                    return res;
+                }
+                mZslStreamId = NO_STREAM;
+            }
+
+            if (mInputStreamId != NO_STREAM) {
+                ALOGV("%s: Camera %d: Deleting stream %d since the buffer "
+                      "dimensions changed",
+                    __FUNCTION__, client->getCameraId(), mInputStreamId);
+                res = device->deleteStream(mInputStreamId);
+                if (res == -EBUSY) {
+                    ALOGV("%s: Camera %d: Device is busy, call updateStream again "
+                          " after it becomes idle", __FUNCTION__, mId);
+                    return res;
+                } else if(res != OK) {
+                    ALOGE("%s: Camera %d: Unable to delete old output stream "
+                            "for ZSL: %s (%d)", __FUNCTION__,
+                            client->getCameraId(), strerror(-res), res);
+                    return res;
+                }
+                mInputStreamId = NO_STREAM;
+            }
+            if (nullptr != mInputProducer.get()) {
+                mInputProducer->disconnect(NATIVE_WINDOW_API_CPU);
+                mInputProducer.clear();
+            }
+        }
+    }
+
+    if (mInputStreamId == NO_STREAM) {
+        res = device->createInputStream(params.fastInfo.arrayWidth,
+            params.fastInfo.arrayHeight, HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED,
+            &mInputStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't create input stream: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    if (mZslStreamId == NO_STREAM) {
+        // Create stream for HAL production
+        // TODO: Sort out better way to select resolution for ZSL
+
+        sp<IGraphicBufferProducer> producer;
+        sp<IGraphicBufferConsumer> consumer;
+        BufferQueue::createBufferQueue(&producer, &consumer);
+        mProducer = new RingBufferConsumer(consumer, GRALLOC_USAGE_HW_CAMERA_ZSL,
+            mBufferQueueDepth);
+        mProducer->setName(String8("Camera2-ZslRingBufferConsumer"));
+        sp<Surface> outSurface = new Surface(producer);
+
+        res = device->createStream(outSurface, params.fastInfo.arrayWidth,
+            params.fastInfo.arrayHeight, HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED,
+            HAL_DATASPACE_UNKNOWN, CAMERA3_STREAM_ROTATION_0, &mZslStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Can't create ZSL stream: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    client->registerFrameListener(Camera2Client::kPreviewRequestIdStart,
+            Camera2Client::kPreviewRequestIdEnd,
+            this,
+            /*sendPartials*/false);
+
+    return OK;
+}
+
+status_t ZslProcessor::deleteStream() {
+    ATRACE_CALL();
+    status_t res;
+    sp<Camera3Device> device = nullptr;
+    sp<Camera2Client> client = nullptr;
+
+    Mutex::Autolock l(mInputMutex);
+
+    if ((mZslStreamId != NO_STREAM) || (mInputStreamId != NO_STREAM)) {
+        client = mClient.promote();
+        if (client == 0) {
+            ALOGE("%s: Camera %d: Client does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+
+        device =
+            reinterpret_cast<Camera3Device*>(client->getCameraDevice().get());
+        if (device == 0) {
+            ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+            return INVALID_OPERATION;
+        }
+    }
+
+    if (mZslStreamId != NO_STREAM) {
+        res = device->deleteStream(mZslStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Cannot delete ZSL output stream %d: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    mZslStreamId, strerror(-res), res);
+            return res;
+        }
+
+        mZslStreamId = NO_STREAM;
+    }
+    if (mInputStreamId != NO_STREAM) {
+        res = device->deleteStream(mInputStreamId);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Cannot delete input stream %d: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    mInputStreamId, strerror(-res), res);
+            return res;
+        }
+
+        mInputStreamId = NO_STREAM;
+    }
+
+    if (nullptr != mInputProducer.get()) {
+        mInputProducer->disconnect(NATIVE_WINDOW_API_CPU);
+        mInputProducer.clear();
+    }
+
+    return OK;
+}
+
+int ZslProcessor::getStreamId() const {
+    Mutex::Autolock l(mInputMutex);
+    return mZslStreamId;
+}
+
+status_t ZslProcessor::updateRequestWithDefaultStillRequest(CameraMetadata &request) const {
+    sp<Camera2Client> client = mClient.promote();
+    if (client == 0) {
+        ALOGE("%s: Camera %d: Client does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+    sp<Camera3Device> device =
+        static_cast<Camera3Device*>(client->getCameraDevice().get());
+    if (device == 0) {
+        ALOGE("%s: Camera %d: Device does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    CameraMetadata stillTemplate;
+    device->createDefaultRequest(CAMERA3_TEMPLATE_STILL_CAPTURE, &stillTemplate);
+
+    // Find some of the post-processing tags, and assign the value from template to the request.
+    // Only check the aberration mode and noise reduction mode for now, as they are very important
+    // for image quality.
+    uint32_t postProcessingTags[] = {
+            ANDROID_NOISE_REDUCTION_MODE,
+            ANDROID_COLOR_CORRECTION_ABERRATION_MODE,
+            ANDROID_COLOR_CORRECTION_MODE,
+            ANDROID_TONEMAP_MODE,
+            ANDROID_SHADING_MODE,
+            ANDROID_HOT_PIXEL_MODE,
+            ANDROID_EDGE_MODE
+    };
+
+    camera_metadata_entry_t entry;
+    for (size_t i = 0; i < sizeof(postProcessingTags) / sizeof(uint32_t); i++) {
+        entry = stillTemplate.find(postProcessingTags[i]);
+        if (entry.count > 0) {
+            request.update(postProcessingTags[i], entry.data.u8, 1);
+        }
+    }
+
+    return OK;
+}
+
+void ZslProcessor::notifyInputReleased() {
+    Mutex::Autolock l(mInputMutex);
+
+    assert(nullptr != mInputBuffer.get());
+    assert(nullptr != mInputProducer.get());
+
+    sp<GraphicBuffer> gb;
+    sp<Fence> fence;
+    auto rc = mInputProducer->detachNextBuffer(&gb, &fence);
+    if (NO_ERROR != rc) {
+        ALOGE("%s: Failed to detach buffer from input producer: %d",
+            __FUNCTION__, rc);
+        return;
+    }
+
+    BufferItem &item = mInputBuffer->getBufferItem();
+    sp<GraphicBuffer> inputBuffer = item.mGraphicBuffer;
+    if (gb->handle != inputBuffer->handle) {
+        ALOGE("%s: Input mismatch, expected buffer %p received %p", __FUNCTION__,
+            inputBuffer->handle, gb->handle);
+        return;
+    }
+
+    mInputBuffer.clear();
+    ALOGV("%s: Memory optimization, clearing ZSL queue",
+          __FUNCTION__);
+    clearZslResultQueueLocked();
+
+    // Required so we accept more ZSL requests
+    mState = RUNNING;
+}
+
+void ZslProcessor::InputProducerListener::onBufferReleased() {
+    sp<ZslProcessor> parent = mParent.promote();
+    if (nullptr != parent.get()) {
+        parent->notifyInputReleased();
+    }
+}
+
+status_t ZslProcessor::pushToReprocess(int32_t requestId) {
+    ALOGV("%s: Send in reprocess request with id %d",
+            __FUNCTION__, requestId);
+    Mutex::Autolock l(mInputMutex);
+    status_t res;
+    sp<Camera2Client> client = mClient.promote();
+
+    if (client == 0) {
+        ALOGE("%s: Camera %d: Client does not exist", __FUNCTION__, mId);
+        return INVALID_OPERATION;
+    }
+
+    IF_ALOGV() {
+        dumpZslQueue(-1);
+    }
+
+    size_t metadataIdx;
+    nsecs_t candidateTimestamp = getCandidateTimestampLocked(&metadataIdx);
+
+    if (candidateTimestamp == -1) {
+        ALOGV("%s: Could not find good candidate for ZSL reprocessing",
+              __FUNCTION__);
+        return NOT_ENOUGH_DATA;
+    } else {
+        ALOGV("%s: Found good ZSL candidate idx: %u",
+            __FUNCTION__, (unsigned int) metadataIdx);
+    }
+
+    if (nullptr == mInputProducer.get()) {
+        res = client->getCameraDevice()->getInputBufferProducer(
+            &mInputProducer);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to retrieve input producer: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    strerror(-res), res);
+            return res;
+        }
+
+        IGraphicBufferProducer::QueueBufferOutput output;
+        res = mInputProducer->connect(new InputProducerListener(this),
+            NATIVE_WINDOW_API_CPU, false, &output);
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to connect to input producer: "
+                    "%s (%d)", __FUNCTION__, client->getCameraId(),
+                    strerror(-res), res);
+            return res;
+        }
+    }
+
+    res = enqueueInputBufferByTimestamp(candidateTimestamp,
+        /*actualTimestamp*/NULL);
+    if (res == NO_BUFFER_AVAILABLE) {
+        ALOGV("%s: No ZSL buffers yet", __FUNCTION__);
+        return NOT_ENOUGH_DATA;
+    } else if (res != OK) {
+        ALOGE("%s: Unable to push buffer for reprocessing: %s (%d)",
+                __FUNCTION__, strerror(-res), res);
+        return res;
+    }
+
+    {
+        CameraMetadata request = mFrameList[metadataIdx];
+
+        // Verify that the frame is reasonable for reprocessing
+
+        camera_metadata_entry_t entry;
+        entry = request.find(ANDROID_CONTROL_AE_STATE);
+        if (entry.count == 0) {
+            ALOGE("%s: ZSL queue frame has no AE state field!",
+                    __FUNCTION__);
+            return BAD_VALUE;
+        }
+        if (entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_CONVERGED &&
+                entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_LOCKED) {
+            ALOGV("%s: ZSL queue frame AE state is %d, need full capture",
+                    __FUNCTION__, entry.data.u8[0]);
+            return NOT_ENOUGH_DATA;
+        }
+
+        uint8_t requestType = ANDROID_REQUEST_TYPE_REPROCESS;
+        res = request.update(ANDROID_REQUEST_TYPE,
+                &requestType, 1);
+        if (res != OK) {
+            ALOGE("%s: Unable to update request type",
+                  __FUNCTION__);
+            return INVALID_OPERATION;
+        }
+
+        int32_t inputStreams[1] =
+                { mInputStreamId };
+        res = request.update(ANDROID_REQUEST_INPUT_STREAMS,
+                inputStreams, 1);
+        if (res != OK) {
+            ALOGE("%s: Unable to update request input streams",
+                  __FUNCTION__);
+            return INVALID_OPERATION;
+        }
+
+        uint8_t captureIntent =
+                static_cast<uint8_t>(ANDROID_CONTROL_CAPTURE_INTENT_STILL_CAPTURE);
+        res = request.update(ANDROID_CONTROL_CAPTURE_INTENT,
+                &captureIntent, 1);
+        if (res != OK ) {
+            ALOGE("%s: Unable to update request capture intent",
+                  __FUNCTION__);
+            return INVALID_OPERATION;
+        }
+
+        // TODO: Shouldn't we also update the latest preview frame?
+        int32_t outputStreams[1] =
+                { client->getCaptureStreamId() };
+        res = request.update(ANDROID_REQUEST_OUTPUT_STREAMS,
+                outputStreams, 1);
+        if (res != OK) {
+            ALOGE("%s: Unable to update request output streams",
+                  __FUNCTION__);
+            return INVALID_OPERATION;
+        }
+
+        res = request.update(ANDROID_REQUEST_ID,
+                &requestId, 1);
+        if (res != OK ) {
+            ALOGE("%s: Unable to update frame to a reprocess request",
+                  __FUNCTION__);
+            return INVALID_OPERATION;
+        }
+
+        res = client->stopStream();
+        if (res != OK) {
+            ALOGE("%s: Camera %d: Unable to stop preview for ZSL capture: "
+                "%s (%d)",
+                __FUNCTION__, client->getCameraId(), strerror(-res), res);
+            return INVALID_OPERATION;
+        }
+
+        // Update JPEG settings
+        {
+            SharedParameters::Lock l(client->getParameters());
+            res = l.mParameters.updateRequestJpeg(&request);
+            if (res != OK) {
+                ALOGE("%s: Camera %d: Unable to update JPEG entries of ZSL "
+                        "capture request: %s (%d)", __FUNCTION__,
+                        client->getCameraId(),
+                        strerror(-res), res);
+                return res;
+            }
+        }
+
+        // Update post-processing settings
+        res = updateRequestWithDefaultStillRequest(request);
+        if (res != OK) {
+            ALOGW("%s: Unable to update post-processing tags, the reprocessed image quality "
+                    "may be compromised", __FUNCTION__);
+        }
+
+        mLatestCapturedRequest = request;
+        res = client->getCameraDevice()->capture(request);
+        if (res != OK ) {
+            ALOGE("%s: Unable to send ZSL reprocess request to capture: %s"
+                  " (%d)", __FUNCTION__, strerror(-res), res);
+            return res;
+        }
+
+        mState = LOCKED;
+    }
+
+    return OK;
+}
+
+status_t ZslProcessor::enqueueInputBufferByTimestamp(
+        nsecs_t timestamp,
+        nsecs_t* actualTimestamp) {
+
+    TimestampFinder timestampFinder = TimestampFinder(timestamp);
+
+    mInputBuffer = mProducer->pinSelectedBuffer(timestampFinder,
+        /*waitForFence*/false);
+
+    if (nullptr == mInputBuffer.get()) {
+        ALOGE("%s: No ZSL buffers were available yet", __FUNCTION__);
+        return NO_BUFFER_AVAILABLE;
+    }
+
+    nsecs_t actual = mInputBuffer->getBufferItem().mTimestamp;
+
+    if (actual != timestamp) {
+        // TODO: This is problematic, the metadata queue timestamp should
+        //       usually have a corresponding ZSL buffer with the same timestamp.
+        //       If this is not the case, then it is possible that we will use
+        //       a ZSL buffer from a different request, which can result in
+        //       side effects during the reprocess pass.
+        ALOGW("%s: ZSL buffer candidate search didn't find an exact match --"
+              " requested timestamp = %" PRId64 ", actual timestamp = %" PRId64,
+              __FUNCTION__, timestamp, actual);
+    }
+
+    if (nullptr != actualTimestamp) {
+        *actualTimestamp = actual;
+    }
+
+    BufferItem &item = mInputBuffer->getBufferItem();
+    auto rc = mInputProducer->attachBuffer(&mInputProducerSlot,
+        item.mGraphicBuffer);
+    if (OK != rc) {
+        ALOGE("%s: Failed to attach input ZSL buffer to producer: %d",
+            __FUNCTION__, rc);
+        return rc;
+    }
+
+    IGraphicBufferProducer::QueueBufferOutput output;
+    IGraphicBufferProducer::QueueBufferInput input(item.mTimestamp,
+            item.mIsAutoTimestamp, item.mDataSpace, item.mCrop,
+            item.mScalingMode, item.mTransform, item.mFence);
+    rc = mInputProducer->queueBuffer(mInputProducerSlot, input, &output);
+    if (OK != rc) {
+        ALOGE("%s: Failed to queue ZSL buffer to producer: %d",
+            __FUNCTION__, rc);
+        return rc;
+    }
+
+    return rc;
+}
+
+status_t ZslProcessor::clearInputRingBufferLocked(nsecs_t* latestTimestamp) {
+
+    if (nullptr != latestTimestamp) {
+        *latestTimestamp = mProducer->getLatestTimestamp();
+    }
+    mInputBuffer.clear();
+
+    return mProducer->clear();
+}
+
+status_t ZslProcessor::clearZslQueue() {
+    Mutex::Autolock l(mInputMutex);
+    // If in middle of capture, can't clear out queue
+    if (mState == LOCKED) return OK;
+
+    return clearZslQueueLocked();
+}
+
+status_t ZslProcessor::clearZslQueueLocked() {
+    if (NO_STREAM != mZslStreamId) {
+        // clear result metadata list first.
+        clearZslResultQueueLocked();
+        return clearInputRingBufferLocked(&mLatestClearedBufferTimestamp);
+    }
+    return OK;
+}
+
+void ZslProcessor::clearZslResultQueueLocked() {
+    mFrameList.clear();
+    mFrameListHead = 0;
+    mFrameList.insertAt(0, mFrameListDepth);
+}
+
+void ZslProcessor::dump(int fd, const Vector<String16>& /*args*/) const {
+    Mutex::Autolock l(mInputMutex);
+    if (!mLatestCapturedRequest.isEmpty()) {
+        String8 result("    Latest ZSL capture request:\n");
+        write(fd, result.string(), result.size());
+        mLatestCapturedRequest.dump(fd, 2, 6);
+    } else {
+        String8 result("    Latest ZSL capture request: none yet\n");
+        write(fd, result.string(), result.size());
+    }
+    dumpZslQueue(fd);
+}
+
+bool ZslProcessor::threadLoop() {
+    // TODO: remove dependency on thread. For now, shut thread down right
+    // away.
+    return false;
+}
+
+void ZslProcessor::dumpZslQueue(int fd) const {
+    String8 header("ZSL queue contents:");
+    String8 indent("    ");
+    ALOGV("%s", header.string());
+    if (fd != -1) {
+        header = indent + header + "\n";
+        write(fd, header.string(), header.size());
+    }
+    for (size_t i = 0; i < mZslQueue.size(); i++) {
+        const ZslPair &queueEntry = mZslQueue[i];
+        nsecs_t bufferTimestamp = queueEntry.buffer.mTimestamp;
+        camera_metadata_ro_entry_t entry;
+        nsecs_t frameTimestamp = 0;
+        int frameAeState = -1;
+        if (!queueEntry.frame.isEmpty()) {
+            entry = queueEntry.frame.find(ANDROID_SENSOR_TIMESTAMP);
+            if (entry.count > 0) frameTimestamp = entry.data.i64[0];
+            entry = queueEntry.frame.find(ANDROID_CONTROL_AE_STATE);
+            if (entry.count > 0) frameAeState = entry.data.u8[0];
+        }
+        String8 result =
+                String8::format("   %zu: b: %" PRId64 "\tf: %" PRId64 ", AE state: %d", i,
+                        bufferTimestamp, frameTimestamp, frameAeState);
+        ALOGV("%s", result.string());
+        if (fd != -1) {
+            result = indent + result + "\n";
+            write(fd, result.string(), result.size());
+        }
+
+    }
+}
+
+bool ZslProcessor::isFixedFocusMode(uint8_t afMode) const {
+    switch (afMode) {
+        case ANDROID_CONTROL_AF_MODE_AUTO:
+        case ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO:
+        case ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE:
+        case ANDROID_CONTROL_AF_MODE_MACRO:
+            return false;
+            break;
+        case ANDROID_CONTROL_AF_MODE_OFF:
+        case ANDROID_CONTROL_AF_MODE_EDOF:
+            return true;
+        default:
+            ALOGE("%s: unknown focus mode %d", __FUNCTION__, afMode);
+            return false;
+    }
+}
+
+nsecs_t ZslProcessor::getCandidateTimestampLocked(size_t* metadataIdx) const {
+    /**
+     * Find the smallest timestamp we know about so far
+     * - ensure that aeState is either converged or locked
+     */
+
+    size_t idx = 0;
+    nsecs_t minTimestamp = -1;
+
+    size_t emptyCount = mFrameList.size();
+
+    for (size_t j = 0; j < mFrameList.size(); j++) {
+        const CameraMetadata &frame = mFrameList[j];
+        if (!frame.isEmpty()) {
+
+            emptyCount--;
+
+            camera_metadata_ro_entry_t entry;
+            entry = frame.find(ANDROID_SENSOR_TIMESTAMP);
+            if (entry.count == 0) {
+                ALOGE("%s: Can't find timestamp in frame!",
+                        __FUNCTION__);
+                continue;
+            }
+            nsecs_t frameTimestamp = entry.data.i64[0];
+            if (minTimestamp > frameTimestamp || minTimestamp == -1) {
+
+                entry = frame.find(ANDROID_CONTROL_AE_STATE);
+
+                if (entry.count == 0) {
+                    /**
+                     * This is most likely a HAL bug. The aeState field is
+                     * mandatory, so it should always be in a metadata packet.
+                     */
+                    ALOGW("%s: ZSL queue frame has no AE state field!",
+                            __FUNCTION__);
+                    continue;
+                }
+                if (entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_CONVERGED &&
+                        entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_LOCKED) {
+                    ALOGVV("%s: ZSL queue frame AE state is %d, need "
+                           "full capture",  __FUNCTION__, entry.data.u8[0]);
+                    continue;
+                }
+
+                entry = frame.find(ANDROID_CONTROL_AF_MODE);
+                if (entry.count == 0) {
+                    ALOGW("%s: ZSL queue frame has no AF mode field!",
+                            __FUNCTION__);
+                    continue;
+                }
+                uint8_t afMode = entry.data.u8[0];
+                if (afMode == ANDROID_CONTROL_AF_MODE_OFF) {
+                    // Skip all the ZSL buffer for manual AF mode, as we don't really
+                    // know the af state.
+                    continue;
+                }
+
+                // Check AF state if device has focuser and focus mode isn't fixed
+                if (mHasFocuser && !isFixedFocusMode(afMode)) {
+                    // Make sure the candidate frame has good focus.
+                    entry = frame.find(ANDROID_CONTROL_AF_STATE);
+                    if (entry.count == 0) {
+                        ALOGW("%s: ZSL queue frame has no AF state field!",
+                                __FUNCTION__);
+                        continue;
+                    }
+                    uint8_t afState = entry.data.u8[0];
+                    if (afState != ANDROID_CONTROL_AF_STATE_PASSIVE_FOCUSED &&
+                            afState != ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED &&
+                            afState != ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED) {
+                        ALOGVV("%s: ZSL queue frame AF state is %d is not good for capture, skip it",
+                                __FUNCTION__, afState);
+                        continue;
+                    }
+                }
+
+                minTimestamp = frameTimestamp;
+                idx = j;
+            }
+
+            ALOGVV("%s: Saw timestamp %" PRId64, __FUNCTION__, frameTimestamp);
+        }
+    }
+
+    if (emptyCount == mFrameList.size()) {
+        /**
+         * This could be mildly bad and means our ZSL was triggered before
+         * there were any frames yet received by the camera framework.
+         *
+         * This is a fairly corner case which can happen under:
+         * + a user presses the shutter button real fast when the camera starts
+         *     (startPreview followed immediately by takePicture).
+         * + burst capture case (hitting shutter button as fast possible)
+         *
+         * If this happens in steady case (preview running for a while, call
+         *     a single takePicture) then this might be a fwk bug.
+         */
+        ALOGW("%s: ZSL queue has no metadata frames", __FUNCTION__);
+    }
+
+    ALOGV("%s: Candidate timestamp %" PRId64 " (idx %zu), empty frames: %zu",
+          __FUNCTION__, minTimestamp, idx, emptyCount);
+
+    if (metadataIdx) {
+        *metadataIdx = idx;
+    }
+
+    return minTimestamp;
+}
+
+}; // namespace camera2
+}; // namespace android
diff --git a/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.h b/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.h
new file mode 100644
index 000000000..3ae009487
--- /dev/null
+++ b/services/camera/libcameraservice/api1/qticlient2/ZslProcessor.h
@@ -0,0 +1,170 @@
+/*
+ * Copyright (C) 2013 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef ANDROID_SERVERS_CAMERA_CAMERA2_ZSLPROCESSOR_H
+#define ANDROID_SERVERS_CAMERA_CAMERA2_ZSLPROCESSOR_H
+
+#include <utils/Thread.h>
+#include <utils/String16.h>
+#include <utils/Vector.h>
+#include <utils/Mutex.h>
+#include <utils/Condition.h>
+#include <gui/BufferItem.h>
+#include <gui/BufferItemConsumer.h>
+#include <gui/RingBufferConsumer.h>
+#include <gui/IProducerListener.h>
+#include <camera/CameraMetadata.h>
+
+#include "api1/qticlient2/FrameProcessor.h"
+
+namespace android {
+
+class Camera2Client;
+
+namespace camera2 {
+
+class CaptureSequencer;
+struct Parameters;
+
+/***
+ * ZSL queue processing for HALv3.0 or newer
+ */
+class ZslProcessor :
+            virtual public Thread,
+            virtual public FrameProcessor::FilteredListener {
+  public:
+    ZslProcessor(sp<Camera2Client> client, wp<CaptureSequencer> sequencer);
+    ~ZslProcessor();
+
+    // From FrameProcessor::FilteredListener
+    virtual void onResultAvailable(const CaptureResult &result);
+
+    /**
+     ****************************************
+     * ZslProcessorInterface implementation *
+     ****************************************
+     */
+
+    // Update the streams by recreating them if the size/format has changed
+    status_t updateStream(const Parameters &params);
+
+    // Delete the underlying CameraDevice streams
+    status_t deleteStream();
+
+    // Get ID for use with android.request.outputStreams / inputStreams
+    int getStreamId() const;
+
+    /**
+     * Submits a ZSL capture request (id = requestId)
+     *
+     * An appropriate ZSL buffer is selected by the closest timestamp,
+     * then we push that buffer to be reprocessed by the HAL.
+     * A capture request is created and submitted on behalf of the client.
+     */
+    status_t pushToReprocess(int32_t requestId);
+
+    // Flush the ZSL buffer queue, freeing up all the buffers
+    status_t clearZslQueue();
+
+    void dump(int fd, const Vector<String16>& args) const;
+
+  private:
+
+    class InputProducerListener : public BnProducerListener {
+    public:
+        InputProducerListener(wp<ZslProcessor> parent) : mParent(parent) {}
+        virtual void onBufferReleased();
+        virtual bool needsReleaseNotify() { return true; }
+
+    private:
+        wp<ZslProcessor> mParent;
+    };
+
+    static const nsecs_t kWaitDuration = 10000000; // 10 ms
+    nsecs_t mLatestClearedBufferTimestamp;
+
+    enum {
+        RUNNING,
+        LOCKED
+    } mState;
+
+    enum { NO_BUFFER_AVAILABLE = BufferQueue::NO_BUFFER_AVAILABLE };
+
+    wp<Camera2Client> mClient;
+    wp<CaptureSequencer> mSequencer;
+
+    const int mId;
+
+    mutable Mutex mInputMutex;
+
+    enum {
+        NO_STREAM = -1
+    };
+
+    int mZslStreamId;
+    int mInputStreamId;
+
+    struct ZslPair {
+        BufferItem buffer;
+        CameraMetadata frame;
+    };
+
+    static const int32_t kDefaultMaxPipelineDepth = 4;
+    size_t mBufferQueueDepth;
+    size_t mFrameListDepth;
+    Vector<CameraMetadata> mFrameList;
+    size_t mFrameListHead;
+
+    ZslPair mNextPair;
+
+    Vector<ZslPair> mZslQueue;
+
+    CameraMetadata mLatestCapturedRequest;
+
+    bool mHasFocuser;
+
+    // Input buffer queued into HAL
+    sp<RingBufferConsumer::PinnedBufferItem> mInputBuffer;
+    sp<RingBufferConsumer>                   mProducer;
+    sp<IGraphicBufferProducer>               mInputProducer;
+    int                                      mInputProducerSlot;
+
+    virtual bool threadLoop();
+
+    status_t clearZslQueueLocked();
+
+    void clearZslResultQueueLocked();
+
+    void dumpZslQueue(int id) const;
+
+    nsecs_t getCandidateTimestampLocked(size_t* metadataIdx) const;
+
+    status_t enqueueInputBufferByTimestamp( nsecs_t timestamp,
+        nsecs_t* actualTimestamp);
+    status_t clearInputRingBufferLocked(nsecs_t* latestTimestamp);
+    void notifyInputReleased();
+
+    bool isFixedFocusMode(uint8_t afMode) const;
+
+    // Update the post-processing metadata with the default still capture request template
+    status_t updateRequestWithDefaultStillRequest(CameraMetadata &request) const;
+};
+
+
+}; //namespace camera2
+}; //namespace android
+
+#endif
-- 
2.17.0

